<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CV on Fengyuan Dai</title><link>https://SuperCarryDFY.github.io/Blog/categories/cv/</link><description>Recent content in CV on Fengyuan Dai</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 04 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/Blog/categories/cv/index.xml" rel="self" type="application/rss+xml"/><item><title>SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title><link>https://SuperCarryDFY.github.io/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/</link><pubDate>Tue, 04 Jul 2023 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/</guid><description>&lt;img src="https://SuperCarryDFY.github.io/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled.png" alt="Featured image of post SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS" />&lt;h1 id="score-based-generative-modeling-through-stochastic-differential-equations">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS&lt;/h1>
&lt;h2 id="score-based-generative-modeling-with-sdes">Score based generative modeling with SDEs&lt;/h2>
&lt;p>diffusion process可以被表述为以下形式&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled.png"
width="281"
height="57"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_hu4f7bf0bd396bfed18d49a99097cb1c52_4513_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_hu4f7bf0bd396bfed18d49a99097cb1c52_4513_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="492"
data-flex-basis="1183px"
>&lt;/p>
&lt;p>reverse-time SDE可以被表述为以下形式（可以看到需要知道分布分数s_theta）&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_1.png"
width="976"
height="229"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_1_hua3ed86c62624c4cba4e8d9d54c2b0d7c_89539_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_1_hua3ed86c62624c4cba4e8d9d54c2b0d7c_89539_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="426"
data-flex-basis="1022px"
>&lt;/p>
&lt;h3 id="estimating-scores-for-the-sde">estimating scores for the SDE&lt;/h3>
&lt;p>和SMLD那篇文章一样，用denoising score matching的方式训练：&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_2.png"
width="914"
height="65"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_2_hudd96ee5f145d26d6c68ab53bca4675cf_15620_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_2_hudd96ee5f145d26d6c68ab53bca4675cf_15620_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="1406"
data-flex-basis="3374px"
>&lt;/p>
&lt;h3 id="vevp-sdes-and-beyond">VE,VP SDEs and Beyond&lt;/h3>
&lt;p>这里讲了SMLD,DDPM和SDE的关系（SMLD和DDPM可以看作离散的SDEs的两种不同模式）。&lt;/p>
&lt;p>对SMLD（Variance Exploding SDE）：&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_3.png"
width="989"
height="341"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_3_hu7ae01872c3ed3f307770bcde38d56f20_91767_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_3_hu7ae01872c3ed3f307770bcde38d56f20_91767_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="696px"
>&lt;/p>
&lt;p>对DDPM（Variance Preserving SDE）：&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_4.png"
width="973"
height="195"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_4_hu7722c1a9c439561a83babe6e8303f77b_38092_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_4_hu7722c1a9c439561a83babe6e8303f77b_38092_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="498"
data-flex-basis="1197px"
>&lt;/p>
&lt;h2 id="solving-the-reverse-sde">Solving the reverse SDE&lt;/h2>
&lt;p>作者提出了三种采样方式&lt;/p>
&lt;h3 id="general-purpose-numerical-sde-solversreverse-diffusion-samplers">general-purpose numerical SDE solvers（reverse diffusion samplers）&lt;/h3>
&lt;p>DDPM的采样方法&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_5.png"
width="735"
height="85"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_5_hu7dad25f652a31a580e87e3e6ebd2aea2_10121_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_5_hu7dad25f652a31a580e87e3e6ebd2aea2_10121_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="864"
data-flex-basis="2075px"
>&lt;/p>
&lt;p>被称之为祖先采样（ancestral sampling），而作者提出了reverse diffusion samplers&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_6.png"
width="717"
height="59"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_6_hu1bcb226548027da4c37fb15c44de6149_7931_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_6_hu1bcb226548027da4c37fb15c44de6149_7931_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="1215"
data-flex-basis="2916px"
>&lt;/p>
&lt;p>可以证明，ancestral sampling，当beta_i趋近于0的时候，可以转化为reverse diffusion samplers的形式&lt;/p>
&lt;h3 id="predictor-corrector-samplers">Predictor-corrector samplers&lt;/h3>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_7.png"
width="984"
height="307"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_7_hu5bdbffab8c853e0b7ed63fc8f6afa3a9_87907_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_7_hu5bdbffab8c853e0b7ed63fc8f6afa3a9_87907_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="320"
data-flex-basis="769px"
>&lt;/p>
&lt;h3 id="probability-flow">probability flow&lt;/h3>
&lt;p>对于每个SDE，存在一个确定性的diffusion过程：ODE&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_8.png"
width="418"
height="57"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_8_hubbc5c18afe1f54b2516dc9e0cb9eeba6_6782_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_8_hubbc5c18afe1f54b2516dc9e0cb9eeba6_6782_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Untitled"
class="gallery-image"
data-flex-grow="733"
data-flex-basis="1760px"
>&lt;/p>
&lt;p>ODE速度更快但是生成的质量较差。&lt;/p>
&lt;h2 id="controallable-generation">controallable generation&lt;/h2>
&lt;p>懒得看了&lt;/p></description></item><item><title>On the Integration of Self-Attention and Convolution</title><link>https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/</link><pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/</guid><description>&lt;img src="https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png" alt="Featured image of post On the Integration of Self-Attention and Convolution" />&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/q6tB2b4KcskzQZn.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;strong>CNN&lt;/strong> The intrinsic characteristics impose crucial inductive biases for image processing. (平移不变性等)&lt;/p>
&lt;p>&lt;strong>Self-Attention&lt;/strong> The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )&lt;/p>
&lt;blockquote>
&lt;p>Generally, we owed self-attention based model&amp;rsquo;s great performance to the global receptive field it have.&lt;/p>
&lt;/blockquote>
&lt;p>Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules.&lt;/p>
&lt;p>&lt;strong>Contribution:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>A strong underlying relation between self-attention and convolution is revealed&lt;/li>
&lt;li>An elegant integration of the self-attention and convolution module, which enjoys the benefits of both worlds&lt;/li>
&lt;/ul>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/4cCOJaNnGZPq56Q.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="conv">Conv&lt;/h3>
&lt;p>( This can be better illustrated by image a )&lt;/p>
&lt;p>They decompose the Conv with K*K kernel to two stages. On the first stage, K*K kernel can be seen as many 1*1 kernel. On the second stage, we can gain finally feature map by shift each feature map and sum them.&lt;/p>
&lt;h3 id="self-attn">Self-attn&lt;/h3>
&lt;p>( This can be better illustrated by image b )&lt;/p>
&lt;p>Like Conv, they treat self-attn with two steps. Firstly, 1*1 kernel conv ( euqal to Fully Connected Network) can be used to generate query, key and value. Then, they use query and key to gain weight and put it on the value.&lt;/p>
&lt;h3 id="acmix">ACmix&lt;/h3>
&lt;p>The first stage of Conv and Self-attn can be shared. In the second stage, there are two paths which corresponds to each paradigm. Then, two learnable parameters $\alpha$ and $\beta$ is set for reweighting each feature map for sum up.&lt;/p>
&lt;h4 id="shift">Shift&lt;/h4>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/hKvFbsuZg6Qkt7w.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>In the Conv path, they add a small trick compared to normal Conv.&lt;/p>
&lt;blockquote>
&lt;p>Shifting tensors towards various directions practically breaks the data locality and is difficult to achieve vectorized implementation. This may greatly impair the actual efficiency of our module at the inference time. ( This part I don&amp;rsquo;t understand perfectly)&lt;/p>
&lt;/blockquote>
&lt;p>As a remedy, they turn to apply depthwise convolution with fixed kernels. Here comes to a question. Now that you use conv kernel to shift feature map, why not make it learnable? So they use learnable kernels and initialized as shift kernels.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>The idea is quite simple. So They need strong experiments result to support them. They use their module in many models and many task( classification, detection and segmentation ). All seems good.&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/TXMgFrQBpkd6Wcs.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/qn4bxQupHwck21A.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="alpha-and-beta">alpha and beta&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/VJ5gh2LsanSkfIH.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>At last, they check $\alpha$ and $\beta$ in each layer. This leads to a interesting conclusion. We can see that at early stage $\beta$ take dominate position. This means Conv is a good feature extractor. But at the last stage, self-attention shows superiority over convolution.&lt;/p>
&lt;blockquote>
&lt;p>This is also consistent with the design patterns in the previous works where self-attention is mostly adopted in the last stages to replace the original 3×3 convolution, and convolutions at early stages are proved to be more effective for vision transformers&lt;/p>
&lt;/blockquote></description></item><item><title>Dilated Convolution &amp; Deconvolution</title><link>https://SuperCarryDFY.github.io/Blog/p/dilated-convolution-deconvolution/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/dilated-convolution-deconvolution/</guid><description>&lt;h2 id="dilated-convolution">Dilated Convolution&lt;/h2>
&lt;p>Common Conv&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/50/v2-d552433faa8363df84c53b905443a556_720w.webp?source=1940ef5c"
loading="lazy"
alt="动图"
>&lt;/p>
&lt;p>Dilated Conv&lt;/p>
&lt;p>&lt;img src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif?raw=true"
loading="lazy"
alt="image-20221023221652075"
>&lt;/p>
&lt;p>空洞卷积在一定程度上能增大卷积神经网络的感受野，但是利用其设计语义分割网络则会存在如下两个问题。&lt;/p>
&lt;p>&lt;strong>1. The Gridding Effect&lt;/strong>&lt;/p>
&lt;p>如果和之前的操作一样，仅仅只是反复叠加3*3的kernal的话，那么在过程中就会存在一定的信息损失。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/v2-478a6b82e1508a147712af63d6472d9a_r.jpg?source=1940ef5c"
loading="lazy"
alt="img"
>&lt;/p>
&lt;p>&lt;strong>2. 对小物体的分割&lt;/strong>&lt;/p>
&lt;p>增大感受野对小物体的分割似乎没有好处&lt;/p>
&lt;p>因此图森组提出了较好的解决方法：Hybrid Dilated Convolution (HDC)&lt;/p>
&lt;ol>
&lt;li>叠加卷积的 dilation rate 不能有大于1的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。我们将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构。&lt;/li>
&lt;li>需要满足$$M_i = max[M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$$， 其中$r_i$，$M_i$分别代表第i层的dilation rate和最大dilation rate。&lt;/li>
&lt;/ol>
&lt;p>来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>如何理解空洞卷积（dilated convolution）？&lt;/a>， 作者@&lt;a class="link" href="https://www.zhihu.com/people/lorenmt" target="_blank" rel="noopener"
>刘诗昆&lt;/a>&lt;/p>
&lt;h2 id="deconvolution">Deconvolution&lt;/h2>
&lt;p>deconv大致可以分成如下三个方面&lt;/p>
&lt;ul>
&lt;li>unsupervised learning&lt;/li>
&lt;li>CNN可视化&lt;/li>
&lt;li>upsampling&lt;/li>
&lt;/ul>
&lt;p>呃。。一般来说上采样+卷积的性能比反卷积要好，况且反卷积存在棋盘格效应。&lt;/p>
&lt;p>来自&lt;a class="link" href="https://www.zhihu.com/question/43609045/answer/132235276" target="_blank" rel="noopener"
>如何理解深度学习中的deconvolution networks？&lt;/a>，作者@&lt;a class="link" href="https://www.zhihu.com/people/xutan" target="_blank" rel="noopener"
>谭旭&lt;/a>&lt;/p></description></item><item><title>Few-shot Object Detection via Feature Reweighting</title><link>https://SuperCarryDFY.github.io/Blog/p/few-shot-object-detection-via-feature-reweighting/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/few-shot-object-detection-via-feature-reweighting/</guid><description>&lt;img src="https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png" alt="Featured image of post Few-shot Object Detection via Feature Reweighting" />&lt;p>&lt;img src="https://s2.loli.net/2022/09/13/oZqPrCX2h7ixfje.png"
loading="lazy"
alt="image-20220913110345915"
>&lt;/p>
&lt;p>This paper is from ICCV 2019. It addressed detection tasks R-CNN based network. However, it simply uses a shared classifier( mlp, I think) and bbox regressor, which is put forward in R-CNN though, to get predictions. Also, it tries a new form loss function out and gets a relatively large promotion.&lt;/p>
&lt;h2 id="abstruct--introduction">ABSTRUCT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>The model is mainly consist of two modules, i.e.,&lt;/p>
&lt;ul>
&lt;li>a meta feature learner.&lt;/li>
&lt;li>a light-weight feature reweighting module.&lt;/li>
&lt;/ul>
&lt;p>The training process is corresponding to two-phase learning scheme,&lt;/p>
&lt;ul>
&lt;li>first learn meta features and good reweighting module from base classes.&lt;/li>
&lt;li>fine-tune the detection model to adapt to novel classes.&lt;/li>
&lt;/ul>
&lt;p>Though it contains two-phase training, it&amp;rsquo;s an end-to-end method.&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png"
loading="lazy"
alt="image-20220913110439796"
>&lt;/p>
&lt;p>&lt;strong>Reweighting Module&lt;/strong>&lt;/p>
&lt;p>This module taking the support examples as input learns to embed support information into reweighting vectors and adjusts the contribution of each meta feature of the query image accordingly for the following detection prediction module.
It is like what is used in BAM. However, in BAM it just uses global max pooling ( or global average pooling, whatever), which is not learnable, to get the prototype vectors. This paper uses a learnable layer to get prototype vectors.
After that, they apply prototype vectors to obtain the class-specific feature Fi for novel class i by F_i = F \times w_i, where \times means channel-wise multiplication.&lt;/p>
&lt;p>&lt;strong>Shared Classifier &amp;amp; BBox Regressor&lt;/strong>&lt;/p>
&lt;p>There aren&amp;rsquo;t any details about Shared Classifier &amp;amp; BBox Regressor module. I simply think this module just being MLP combines BBox Regressor.&lt;/p>
&lt;p>&lt;strong>Learning Scheme&lt;/strong>&lt;/p>
&lt;p>In the first stage, they just feed the model with abundant base images with annotations. In this way, the model can learn to coordinate the two modules in the desired way.&lt;/p>
&lt;p>In the second stage, they fine-tune the model on both base and novel classes. The training procedure is the same as the first phase, except that it takes significantly fewer iterations for the model to converge.&lt;/p>
&lt;p>After two training phases, the model can do a test without a novel class as input (it should not be deemed as a novel class, for it has seen the classes in the second stage) and reweighting module, because it remembers the prototype vectors of all class and just do inference in novel images.&lt;/p>
&lt;h2 id="loss-function">LOSS FUNCTION&lt;/h2>
&lt;p>It is intuitive to use binary cross-entropy as a detection loss function, regressing 1 if the object is the target class and 0 otherwise. However, binary cross-entropy strives to produce balanced positive and negative predictions and could not remove such false predictions.
Instead, they adopt a softmax layer to calibrate the classification scores among different classes. It can be denoted as \hat c_i = \frac{e^{c_i}}{\sum_{j=1}^Ne^{c_j}}. Then, the loss function is below：
$$
L_c= -\sum_{i=1}^{N}1(·,i)log(\hat c_i)
$$&lt;/p>
&lt;p>where 1(·, i) is an indicator function for whether the current anchor box really belongs to class i or not.
Finally, the overall loss function is
$$
L_{det} = L_c + L_bbx + L_{obj}
$$&lt;/p></description></item><item><title>Learning What Not to Segment A New Perspective on Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png" alt="Featured image of post Learning What Not to Segment A New Perspective on Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>I read the paper in the first month when I came in MiLab. However, at that time I could not recognize the quality of this method (I usually don&amp;rsquo;t read the experiment carefully). It appeared when I was reading another paper called &amp;ldquo;Holistic Prototype Activation for Few-Shot Segmentation&amp;rdquo;. The HPA performs well but can not beat BAM (although HPA has less parameter to train, obviously). So I decided to read this paper again.&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>Previous problem:&lt;/p>
&lt;ul>
&lt;li>the trained models are biased towards the seen classes instead of being ideally class-agnostic&lt;/li>
&lt;/ul>
&lt;p>Contribution&lt;/p>
&lt;ul>
&lt;li>
&lt;p>BAM, i.e., base and the meta has two branches, allowing model to learn what not to segment and what to segmentation, respectively.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Design a special loss in order to train two branches suitably.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extend the proposed approach to a more challenging setting, which simultaneously identifies the targets of base and novel classes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In the following image, (a) is a classical method to address FSS task; (b) is BAM approach; (c) is the extension of BAM&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/hOT72tJIqimkDKL.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>They adopt two stage training method, which means they train base learner and meta learner separately.&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/4KBGJQmrc71nMu2.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="base-learner">BASE LEARNER&lt;/h3>
&lt;p>Query image goes through four ResNet blocks and becomes intermediate feature maps $f_b^q$. Then the decoder network $D_b$ yields the prediction result. $N_b$ represents the number of base categories.
$$
P_b = softmax(D_b(f_b^q)) \in R^{(1+N_b)\times H\times W}
$$
Loss can be defined as
$$
L_{base} = \frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}CE(P_{b;i},m^q_{b;i})
$$
It is worth noting that they do not employ the general FSS learning paradigm (update the parameter in each episode). And they train the base learner independently. They explain as follow:&lt;/p>
&lt;blockquote>
&lt;p>It is unrealistic to additionally build such a large network on the basis of the original few-shot model, which will introduce too many parameters and slow down the inference speed.&lt;/p>
&lt;p>It is unknown whether the base learner can be trained well with the episodic learning paradigm, so a two stage training strategy is eventually adopted.&lt;/p>
&lt;/blockquote>
&lt;p>In the ablation study, it shows that with two stage train, the model can perform better.&lt;/p>
&lt;h3 id="meta-learner">META LEARNER&lt;/h3>
&lt;p>This part is highly resemble similar to CANet, employing &amp;ldquo;expand &amp;amp; concatenate&amp;rdquo; operations. The loss can be described as
$$
L_{meta} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_{m;i},m_i^q)
$$
, when $n_e$ denotes the number of training episodes in each batch&lt;/p>
&lt;h3 id="ensemble">Ensemble&lt;/h3>
&lt;p>This part is designed to leverage the low-level feature to adjust the coarse predictions which is derived from meta learner.&lt;/p>
&lt;p>Firstly, we calculate the overall indicator $\psi$ for guiding the adjustment process:
$$
A_s = F_{reshape}(f_{low}^s) \in R^{C_1\times N},\
G^s = A_sA^T \in R^{C_1\times C_1}
$$
$G^s$ should be denoted as Gram matrix, BTW.&lt;/p>
&lt;blockquote>
&lt;p>Gram matrix can be regarded as eccentric covariance matrix between features. Every number in gram matrix describe the relation between every two feature, about which two features appear simultaneously, which two features just offset from each other, etc.&lt;/p>
&lt;/blockquote>
&lt;p>$$
\psi =||G^s - G^q||_F
$$&lt;/p>
&lt;p>$||\ ||_F$ denotes the Frobenius norm of the input metirx.&lt;/p>
&lt;p>After that, the final segmentation pridections $P_f$ can be described as follow:
$$
p_f^0 = F_{ensemble}(F_\psi(p_m^0),p_b^f), \
p_f=p_f^0(+)F_\psi(p_m^1)
$$
where $p_m$,$p_b$ denote the predictions of the meta learner and base learner respectively. The superscript &amp;ldquo;0&amp;rdquo; and &amp;ldquo;1&amp;rdquo; represent the background and foreground respectively. Both $F_ψ$ and $F_{ensemble}$ are 1×1 convolution operations with specific initial parameters.&lt;/p>
&lt;h3 id="loss">LOSS&lt;/h3>
&lt;p>$$
L=L_{final} + \lambda L_{meta} \
L_{final} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_i^q,m_i^q)
$$&lt;/p>
&lt;p>where $L_{meta}$ is the loss function of the meta learner defined in the meta learner stage.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/1E4h7VSepL2wHUG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/JuMlP1aGcUwxI94.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>Affordance Transfer Learning for Human-Object Interaction Detection</title><link>https://SuperCarryDFY.github.io/Blog/p/affordance-transfer-learning-for-human-object-interaction-detection/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/affordance-transfer-learning-for-human-object-interaction-detection/</guid><description>&lt;img src="https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png" alt="Featured image of post Affordance Transfer Learning for Human-Object Interaction Detection" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/avbpq9mtxy5IdCF.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>这篇实质是完成一个分类任务，并且能在unseen objects上也能辨认出其affordance，应该比较容易拿过来做few-shot任务。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/UltqHo4P2MOs9Lf.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>train用这张图，test用下面object affordance recognition那张图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="affordance-transfer-learning">AFFORDANCE TRANSFER LEARNING&lt;/h3>
&lt;p>&lt;strong>Efficient HOI Composition&lt;/strong>&lt;/p>
&lt;p>To compose a new HOI by the object $\hat{l_o}$ and verb $l_v$, we assign the label to the composite HOI as follows,&lt;/p>
&lt;p>$$
\hat{y} = (\hat{l_o}A_o) \and (l_vA_v)
$$&lt;/p>
&lt;p>，其中$A_o$和$A_v$是分别关于object和verb的同现矩阵co-occurrence matrix（？）&lt;/p>
&lt;p>&lt;strong>Invalid HOI Elimination&lt;/strong>&lt;/p>
&lt;p>有些HOI是无效的（比如ride orange），所以把无效HOI在上式矩阵的对应位置清零了&lt;/p>
&lt;h3 id="object-affordance-recognition">OBJECT AFFORDANCE RECOGNITION&lt;/h3>
&lt;p>这里主要解释如何在test phase做推理&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/CaRPrfTUmpjAqtX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对每个affordance，我们随机抽取M（这里M=100）个instances，抽取完特征之后作为affordance feature bank&lt;/p>
&lt;p>对一个输入的object feature，我们把它和bank所有的affordance一一结合起来，把所有的HOI predictions都转换成affordance prediction（这有啥区别），然后就得到了有许多重复元素的affordance lists。一个元素重复得越多说明有这个affordance的可能习惯越大。&lt;/p>
&lt;h3 id="optimization-and-inference">OPTIMIZATION AND INFERENCE&lt;/h3>
&lt;p>loss就比较常规
$$
L = L_{hoi_sp}+\lambda_1L_{hoi}+\lambda_2L_{ATL}
$$
，其中$\lambda_1$,$\lambda_2$都是超参数。&lt;/p></description></item><item><title>Learning Transferable Human-Object Interaction Detector with Natural Language Supervision</title><link>https://SuperCarryDFY.github.io/Blog/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/</guid><description>&lt;img src="https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png" alt="Featured image of post Learning Transferable Human-Object Interaction Detector with Natural Language Supervision" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/gWoakEicOMpR68j.png"
loading="lazy"
alt="image-20220822150720449"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>这篇文章的工作是，输入一张图片，输出HOI特征，并且用HOI短语作为监督训练（基于CLIP）。&lt;/p>
&lt;p>他与其他HOI transfer工作不同的点在于，之前的工作对unseen object都是用discrete label作为输出，得到HOI。但是这就要求label中对应的HOI（就是每个动作）都预训练过，很难去识别interaction that out of the predefined list。这篇的思路主要是对文字和图像同时encode，然后寻找最近的匹配对，所以不存在这个问题。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/IGi64rQnLkNBma3.png"
loading="lazy"
alt="image-20220822152554566"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png"
loading="lazy"
alt="image-20220822152833390"
>&lt;/p>
&lt;p>他们定义了HOI为 ${(b_p,b_o,a,o)}$（有些文章定义为triplet，其实都差不多，最重要的是verb），其中$b_p$和$b_o$是人和物的bounding box。$a$和$o$分别是human action和object category。&lt;/p>
&lt;h3 id="preliminary">PRELIMINARY&lt;/h3>
&lt;p>在preliminary中，他们简单设想了一些步骤，用Faster RCNN得到bounding box然后输入到CLIP，就能在Unseen数据集上得到SOTA，even without tuning.（废话，人家设计的时候也没管unseen HOI啊）&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/36FEm9MqlODH4wT.png"
loading="lazy"
alt="image-20220822155533112"
>&lt;/p>
&lt;h3 id="proposed-method">PROPOSED METHOD&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>visual input: ${(h,c,b_p,b_o)}$,&lt;/p>
&lt;ul>
&lt;li>h is feature representation for interactions&lt;/li>
&lt;li>$b_p,b_o$ is bounding box&lt;/li>
&lt;li>c is the confidence score for bounding box prediction&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>text encoder: raw text of interactions&lt;/p>
&lt;/li>
&lt;li>
&lt;p>similarity $h^Ts$, where h, s denote the output of visual encoder and text encoder. They are semantic features in the same dimension.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ViT-based Visual Encoder&lt;/strong>&lt;/p>
&lt;p>前半部分输入跟ViT一样，对图像分patch之后加position embedding，在首位置插入CLS后直接作为输入放到ViT中。其中，关于CLS的理解如下&lt;/p>
&lt;blockquote>
&lt;p>CLS的特点&lt;/p>
&lt;ul>
&lt;li>不基于图像内容&lt;/li>
&lt;li>位置编码固定&lt;/li>
&lt;/ul>
&lt;p>好处&lt;/p>
&lt;ul>
&lt;li>该token随机初始化，能够编码整个序列的统计特性&lt;/li>
&lt;li>本身不基于图像内容，避免对某个特定的token产生偏向性&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>但是和ViT不同的是，这里期望能识别出多种不同的HOI（ViT最开始是做分类任务的）。所以在原本的序列后面另外加入了M个CLS（留了M个空让网络自己去学）。&lt;/p>
&lt;p>然而，输入的patch（以下称为X）和作为HOI的CLS（以下称为H）在ViT中作计算时亦有不同。&lt;/p>
&lt;p>X就是正常经过Transformer block，不管H。其中MHA是多层注意力机制，LN是layer norm，MLP是2层感知机。
$$
X_l^{&amp;rsquo;} = MHA(X_{l-1})+X_{l-1} \
X_l = MLP(LN(X_l^{&amp;rsquo;}))+X_l^{&amp;rsquo;}
$$
H在做多层注意力机制时，需要聚合来自X的信息（但是不需要位置0的CLS，他们说如果不mask的话，HOI会直接copy位置0的信息）
$$
H_l^{&amp;rsquo;}=MHA(H_{l-1},X_{l-1}^{[1:]}) + H_{l-1} \
H_l = MLP(LN(H_l^{&amp;rsquo;}))+H_l^{&amp;rsquo;}
$$
&lt;strong>HOI Sequence Parser&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/X8iwQBC4c9SYFyI.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从transformer block出来的HOI序列有一个问题，没办法区分开来。这其实也好理解，因为输入的时候并没有带位置信息，实际上H之间都是等效的。&lt;/p>
&lt;p>所以作者故意在这个模块中使用Sequence manner 而不是 in parallel的方式。&lt;/p>
&lt;p>&lt;strong>Project Head and Bounding Box Regressor&lt;/strong>&lt;/p>
&lt;p>Project Head其实就是一个线性层，把从transformer block出来的X映射到text encoder的输出的维度，方便做相似度计算，从而找到最近的tensor。&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>LOSS部分没有很复杂，由两部分组成，分别是box head输出和project head的loss
$$
L_m(i,\phi) = L_b(\hat{b_p^i},b_p^{\phi_i})+L_b(\hat{b_o^i},b_o^{\phi_i})+L_h(\hat{h_i},s_{\phi_i})
$$
其中$L_b$表示Bounding box 的loss，$L_h$代表CLIP那边的loss，包括text-to-visual和visual-to-text&lt;/p></description></item><item><title>CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</title><link>https://SuperCarryDFY.github.io/Blog/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png" alt="Featured image of post CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/AiIQyMX2cZBlStK.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从TPAMI20那篇过来的，主要想看一下哪里说的middle-level feature&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>consists of&lt;/p>
&lt;ul>
&lt;li>a two-branch dense comparison module
&lt;ul>
&lt;li>performs multi-level feature comparison between the support image and the query image&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>an iterative optimization module
&lt;ul>
&lt;li>iteratively refines the predicted results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>有趣的地方&lt;/p>
&lt;ul>
&lt;li>先前的工作，从1-shot拓展到k-shot时，都是用non-learnable fusion，这篇文章中用的是attention mechanism。&lt;/li>
&lt;li>做test的时候，不再输入support image mask了，而是输入support image bounding box.&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/Ea6tJYghbZvXCiW.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="dense-comparison-module">DENSE COMPARISON MODULE&lt;/h3>
&lt;p>在CNN中&lt;/p>
&lt;ul>
&lt;li>feature in low layers often relate to low-level cues 比如颜色，边缘。个人感觉是因为感受野不够大&lt;/li>
&lt;li>feature in higher layers relate to object-level concepts 比如物品种类&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>因为他们要求模型训练完之后具备一定的generalization，而middle-level fature有可能会包含来自没见过的物体的part。比如说训练的时候见过小轿车，那么在测试中，我就比较容易根据middle-level中的轮胎来segment公交车。（make sense）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>dialated convolution&lt;/strong>空洞卷积&lt;/p>
&lt;p>以下来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>知乎&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。&lt;/p>
&lt;p>传统卷积核的本质问题：pool减小图片尺寸，upsampling增大图片尺寸，在这个过程中肯定有信息丢失掉了。因此空洞卷积就是不通过pooling也能获得较大感受野的方法&lt;/p>
&lt;p>dialated convolution的优点：内部数据结构的保留和避免使用 down-sampling 。&lt;/p>
&lt;/blockquote>
&lt;p>具体的，他们把Resnet分成4个block，只用block2和block3的输出，将其concat到一起。后面就是support feature与support feaure相乘来把背景像素清0，avg pool之后repeat到之前的维度和query feature concat到一起，比较常规。&lt;/p>
&lt;h3 id="inerative-optimization-module">INERATIVE OPTIMIZATION MODULE&lt;/h3>
&lt;p>每一次迭代中，首先进行如下运算。其中$M$是output of the residual blocks，$x$是DCM模块的输出，$y_{t-1}$是上一个迭代块的输出，$F$是concat之后再经过两层卷积层。
$$
M_t = x + F(x,y_{t-1})
$$
对$M_t$，再经过两层vanilla residual blocks。然后再经过&lt;a class="link" href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener"
>ASSP&lt;/a>模块输出。&lt;/p>
&lt;p>就差不多这么迭代n次&lt;/p>
&lt;h3 id="attention-mechanism-for-k-shot-segmentation">ATTENTION MECHANISM FOR K-SHOT SEGMENTATION&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/ZEuUP7zm2i9IKks.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;strong>这也算attention嘛? query, key, value分别是什么？&lt;/strong>&lt;/p>
&lt;p>在attention模块中，经过两层卷积再softmax得到$\hat\lambda_k$，然后把$\hat\lambda_k$和经过卷积的support sample n相乘。&lt;/p>
&lt;h2 id="ablation-study">ABLATION STUDY&lt;/h2>
&lt;p>&lt;strong>Feature for Comparison&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/8ZJEQznfqDu9sTj.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里说单个的话block2效果是最好的，整体上block2+block3的效果最好。嗯，比较可信。&lt;/p>
&lt;blockquote>
&lt;p>其中提了一嘴While block4 corresponds to high-level features, e.g., categories, and incorporates a great number of parameters (2048 channels), which makes it hard to optimize under the fewshot setting.&lt;/p>
&lt;p>深有同感！&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Attention vs. Feature Fusion vs. Mask Fusion&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/layPuvoMD6Xrgkc.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从这里可以看出Attention效果是最好的。但是相比来说的话还是多了两层卷积，增加了参数。Feature-Avg表现不错，感觉跟attention也差不多了（主要是没有另外加参数）。Mask-Avg这么奇怪的想法竟然也有效，能+0.5。还有Mask-OR你要笑死我嘛，怎么还没1-shot高啊，怎么回事啊小老弟，纯纯的帮倒忙。&lt;/p></description></item><item><title>Holistic Prototype Activation for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/holistic-prototype-activation-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/holistic-prototype-activation-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png" alt="Featured image of post Holistic Prototype Activation for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/B18VowS6RfMEJKX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）&lt;/p>
&lt;p>Motivations-&amp;gt;现在的FSS主要有俩问题&lt;/p>
&lt;ul>
&lt;li>会把不属于本类的物体也分割进来&lt;/li>
&lt;li>分不清边界&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/OJjBCcq7IUnFfey.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体在train的时候见过，那么这个物体肯定就不是要分割的物体了(好像有点transductive了哈，但是严格说又不是)&lt;/p>
&lt;blockquote>
&lt;p>The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)&lt;/p>
&lt;/blockquote>
&lt;p>对于第二个问题，他们说关键就是要找出support和query中的本质联系（0.0），他们提出了基于DeepLabv3的CRD模型来解决这个问题。&lt;/p>
&lt;blockquote>
&lt;p>We argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images&lt;/p>
&lt;/blockquote>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="prototype-acquisition">PROTOTYPE ACQUISITION&lt;/h3>
&lt;p>在这个模块生成base class的prototype
$$
P_b^c = \frac{1}{N}\sum_{i=1}^{N_c}\frac{1}{|\hat{L_c^i}|}\sum_{j=1}^{HW}F_c^{ij}\hat{L_c^{ij}}
$$
其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。&lt;/p>
&lt;h3 id="prototype-activation-module">PROTOTYPE ACTIVATION MODULE&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/SeOERlPDLBuTasG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。&lt;/p>
&lt;p>对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\phi$是1&lt;em>1的卷积再加激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间
$$
A_k^{ij}=g_\phi(F_q^{ij}+P^k_h)
$$
然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别）
$$
k^&lt;/em> = argmax_k(A_k^{ij}) \
M^{ij}=A_{k^&lt;em>}^{ij} ,\ if \ k^&lt;/em>=1 \ otherwise \ 0
$$&lt;/p>
&lt;p>下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为前面不是已经做了匹配度计算了么，所以直接在对应位置做expand就好了，具体可见下图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/HumfepLVWd6byYa.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="cross-reference-decoder">CROSS-REFERENCE DECODER&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/uwkmMHxE4P8q15Y.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/y95dGIcosCltvVN.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>One-Shot Affordance Detection</title><link>https://SuperCarryDFY.github.io/Blog/p/one-shot-affordance-detection/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/one-shot-affordance-detection/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png" alt="Featured image of post One-Shot Affordance Detection" />&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2106.14747.pdf" target="_blank" rel="noopener"
>One-shot Affordance Detetion 2106.14747.pdf (arxiv.org)&lt;/a>&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;ul>
&lt;li>可供性检测就是通过一张图片识别物体潜在的动作。&lt;/li>
&lt;li>OS-AD网络可以在所有候选图片中帮助发现普遍的可供性，并且学会适应感知未发现的可供性。&lt;/li>
&lt;li>他们建立了一个数据集PAD ：4k Image；31 affordance；72个物体类别&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>挑战OS-AD 给一张图片，告知其图片上的物体的行为，则可以察觉所有物体普遍的可供性&lt;/li>
&lt;li>问题：现实生活中一个物体可能有多个affordance（例如沙发可以躺也可以睡），而具体用什么affordance取决于人在这个场景中的目的。抛去目的的指引，直接从一张图片中学习affordance会导致忽略了其他视觉上的对此时的任务有效的affordance
&lt;ul>
&lt;li>从行为中找暗示&lt;/li>
&lt;li>采用&lt;strong>collaboration learning&lt;/strong>去捕捉不同物体间的潜在关系，抵消物体不同的appearance，增加泛化性；OS-AD PLM，PTM，CEM&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可供性检测应该能适用于各种环境： PAD 目标驱动可供性数据集&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;ul>
&lt;li>Affordance Detection&lt;/li>
&lt;li>One-Shot Learn
&lt;ul>
&lt;li>based on metric learning using the siamese neural network 度量学习；孪生神经网络&lt;/li>
&lt;li>meta-learning and generation models 元学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="methods">Methods&lt;/h2>
&lt;h3 id="framework">Framework&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png"
loading="lazy"
alt="framework2.png"
>&lt;/p>
&lt;ul>
&lt;li>input: query images, human-object interactions&lt;/li>
&lt;li>ResNet50 -&amp;gt; 获得图像表现 $X$ and $ X_{sup} $&lt;/li>
&lt;li>输入$X_{sup}$和 人和物体的边界矩阵到PLM -&amp;gt; 提取human-object interaction信息，对action-purpose编码，发现人想要旋转的原因&lt;/li>
&lt;li>输入feature representation和$X$到PTM里面 -&amp;gt; 让网络学会处理带affordance的信息&lt;/li>
&lt;li>输入encoded feature 到CEM， 输出affordance&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-learning-module">Purpose Learning Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/HUW6kjnPJ29qX83.png"
loading="lazy"
alt="plm.png"
>&lt;/p>
&lt;p>:star:&lt;a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_On_Exploring_Undetermined_Relationships_for_Visual_Relationship_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener"
>On Exploring Undetermined Relationships for Visual Relationship Detection&lt;/a>受到了这篇文章的启发，说instance（人或物）的特征可以指导网络哪里应该focus。&lt;/p>
&lt;p>先得到$M_O$和$M_H$ &lt;strong>（这两者分别代表什么？作者说是为了让模型去分别focus on物体和个人，引入了注意力机制，其中GMP的作用是得到最显著的特征）&lt;/strong> 其中⊗ 代表element-wise product，元素对应位置相乘，$f_O$和$f_H$是$X_O$和$X_H$进行 global maximum pooling（GMP）后的值
$$
M_O = Softmax(f_O⊗X_{sup})⊗X_{sup} \
M_H = Softmax(f_H⊗X_{sup})⊗X_{sup}
$$
作者说他们用$f_O$去指导网络应该focus on人物交互$M_{HO}$
$$
M_{HO}=Conv(f_O⊗X_H)
$$
最后得到encoding of the action purpose $F_{sup}$，其中&amp;quot; ·&amp;ldquo;代表position-wise dot product.
$$
F_{sup} = MaxPooling((M_{HO}·M_H)+(M_{HO}·M_O))
$$&lt;/p>
&lt;ul>
&lt;li>输入：$X_{sup}$以及人和物体的边界框&lt;/li>
&lt;li>输出：动作目的编码 $F_{sup}$&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-transfer-module">Purpose Transfer Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/qARrZbuCIm4BipT.png"
loading="lazy"
alt="ptm.png"
>&lt;/p>
&lt;p>通过attention机制，将action purpose传递到query image中，加强相关features
$$
X_{T_i} = X_i + Softmax(X_i⊗F_{sup})⊗X_i,\ where\ i \ in\ [1,n]
$$&lt;/p>
&lt;h3 id="collaboration-enhancement-module">Collaboration Enhancement Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/lx5Jb4jkPhIuSN3.png"
loading="lazy"
alt="cem.png"
>&lt;/p>
&lt;p>交替使用E-step和M-step，得到一个紧凑的基集，重建query image的特征图。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从PTM输入的$X_T = {X_{T_1},&amp;hellip;,X_{T_n}}$经过卷积得到$F={F_1,&amp;hellip;F_n}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化基集$\mu$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>E-step估计隐变量$Z={Z_1,&amp;hellip;Z_n}$&lt;/p>
&lt;ul>
&lt;li>第k个basis 第j个像素 第i个图片&lt;/li>
&lt;li>$Z_{ijk} = \frac{\kappa(f_{ij},\mu_k)}{\sum_{l=1}^{K}\kappa(f_{ij},\mu_l)}$&lt;/li>
&lt;li>$f_{ij}$第i个图像的第j个位置的特征&lt;/li>
&lt;li>$\kappa$是指数核函数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>M-step更新基集$\mu$，并把$\mu$作为$F$的加权平均&lt;/p>
&lt;ul>
&lt;li>$\mu_k = \frac{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}f_{ij}}{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>经过E-M的迭代后，我们用$\mu$和$Z$去重建$X$并得到$F$&lt;/p>
&lt;ul>
&lt;li>$F_i=Z_i\mu$&lt;/li>
&lt;li>$\tilde X_i=X_i+Conv(F_i)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="前景知识">前景知识&lt;/h4>
&lt;p>Expectation-Maximization (E-M)&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/67120173" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/67120173&lt;/a>.&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>初始化参数&lt;/li>
&lt;li>根据初始化的参数，划分类别&lt;/li>
&lt;li>根据最大似然估计重新计算参数&lt;/li>
&lt;li>重复步骤1-3，迭代n次，参数收敛&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>期望最大化注意力机制&lt;/strong>&lt;/p>
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener"
>EMANet&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.jianshu.com/p/6bb799d256b1" target="_blank" rel="noopener"
>https://www.jianshu.com/p/6bb799d256b1&lt;/a>&lt;/li>
&lt;li>作者写的知乎专栏：https://zhuanlan.zhihu.com/p/78018142&lt;/li>
&lt;/ul>
&lt;p>分为$A_E,A_M,A_R$三部分组成，前两者是EM算法的E步和M步&lt;/p>
&lt;ul>
&lt;li>假定输入的特征图为$X\in R^{N\times C}$，基初始值为$\mu\in R^{K\times C}$&lt;/li>
&lt;li>$A_E$步估计隐变量$Z\in R^{N\times K}$，则第k个基对第n个像素的权责可以计算为
&lt;ul>
&lt;li>$z_{nk}=\frac{\kappa(x_n,\mu_k)}{\sum_{j=1}^{K}\kappa(x_n,\mu_j)}$&lt;/li>
&lt;li>实现时可以用公式 $Z=softmax(\lambda X(\mu^T))$，其中$\lambda$作为超参数控制$Z$的分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_M$步更新基。$\mu$被计算为$X$的加权平均。第k个基被个更新为
&lt;ul>
&lt;li>$\mu_k=\frac{\sum_{n=1}^Nz_{nk}X_n}{\sum_{n=1}^Nz_{nk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_E$和$A_M$交替执行T步后，$\mu$和$Z$近似收敛，可以用来对X重新评估
&lt;ul>
&lt;li>$\tilde X=Z\mu$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="decoder">Decoder&lt;/h3>
&lt;p>$$
P^m_i=Conv(Unsample(Conv(X^m_i)+P^{m+1}_i)),\ where\ m \ in \ [1,4]
$$&lt;/p>
&lt;ul>
&lt;li>其中m是第m层，i表示&lt;/li>
&lt;/ul>
&lt;p>把检测结果在与原图相同的特征维度还原出来&lt;/p>
&lt;p>用交叉熵Cross-entropy来作为损失函数&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;ul>
&lt;li>k-fold evaluation protocol 将数据集分成三部分，其中之二作为训练集，剩下作为测试集&lt;/li>
&lt;/ul>
&lt;h3 id="benchmark-setting">Benchmark Setting&lt;/h3>
&lt;ul>
&lt;li>IoU metric
&lt;ul>
&lt;li>for segmentation task 切割任务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mean Absolute Error (MAE)
&lt;ul>
&lt;li>measure the absolute error between the prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>E-measure&lt;/strong> 相关文章 &lt;a class="link" href="https://github.com/DengPingFan/E-measure" target="_blank" rel="noopener"
>E-measure: Enhanced-alignment Measure for Binary Foreground Map Evaluation&lt;/a>
&lt;ul>
&lt;li>a metric that combines local pixels and image-level average values to jointly capture image-level statistics and local pixel matching information.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Pearson Correlation Coefficient (CC)&lt;/strong>
&lt;ul>
&lt;li>皮尔逊相关系数 两个变量之间的协方差和标准差的商 $$ p_{X,Y}=\frac{cov(X,Y)}{\sigma_x\sigma_y} $$&lt;/li>
&lt;li>measure the correlation between prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>其他训练参数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Adam optimizer&lt;/li>
&lt;li>resnet50&lt;/li>
&lt;li>The input is randomly clipped from 360×360 to 320×320 with random horizontal flipping. 随机裁剪+水平翻转&lt;/li>
&lt;li>40 epochs on 1080ti&lt;/li>
&lt;li>learning rate 1e-4&lt;/li>
&lt;li>the number of bases in CEM is $K=256$&lt;/li>
&lt;li>E-M 迭代次数 3&lt;/li>
&lt;/ul>
&lt;h3 id="quantitative-and-qualitative-comparisons">Quantitative and Qualitative Comparisons&lt;/h3>
&lt;p>对比下来就是我们的模型很好很好&lt;/p></description></item><item><title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png" alt="Featured image of post Prior Guided Feature Enrichment Network for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/cs5Rm6YUpuX3QFf.png"
loading="lazy"
alt="title1.png"
>&lt;/p>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>主要解决了两个问题：&lt;/p>
&lt;ul>
&lt;li>Generalization Reduction &amp;amp; High-Level Features.
&lt;ul>
&lt;li>[CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱）&lt;/li>
&lt;li>他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Spatial Inconsistency.
&lt;ul>
&lt;li>因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">RELATED WORK&lt;/h2>
&lt;p>&lt;strong>Few-Shot Learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>meta-learning
&lt;ul>
&lt;li>跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>metric-learning
&lt;ul>
&lt;li>Prototypical network&lt;/li>
&lt;li>这篇文章比较偏向于metric-learning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png"
loading="lazy"
alt="framework1.png"
>&lt;/p>
&lt;h3 id="prior-for-few-shot-segmentation">Prior for Few-Shot Segmentation&lt;/h3>
&lt;p>CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。&lt;/p>
&lt;p>Prior Generation的具体做法&lt;/p>
&lt;ul>
&lt;li>
&lt;p>先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask
$$
X_Q=F(I_Q), \ X_S = F(I_S)\times M_S
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity
$$
cos(x_q,x_s)=\frac{x_q^Tx_s}{|x_q||x_s|},\ \ \ \ q,s\in{1,2,&amp;hellip;,hw}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每一个$x_q \in X_Q$来说，取其中最大的值作为correspondence value
$$
c_q = max_{s\in {1,2&amp;hellip;,hw}}(cos(x_q,x_s))
$$&lt;/p>
&lt;p>$$
C_Q = [c_1,c_2,&amp;hellip;,c_hw] \in R^{hw\times1}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization
$$
Y_Q = \frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\epsilon}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="feature-enrichment-module">Feature Enrichment Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/rhadcOZ1iPuQ7nH.png"
loading="lazy"
alt="module1.png"
>&lt;/p>
&lt;p>将support image和query image关联起来的方法&lt;/p>
&lt;ul>
&lt;li>对support image做global average pooling
&lt;ul>
&lt;li>不用说都感觉效果一般&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>multi-level spatial information
&lt;ul>
&lt;li>说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>作者提出的FEM可以很好的解决问题。其中M的具体操作如下&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/P9ux8joOMAlyv3t.png"
loading="lazy"
alt="module2.png"
>&lt;/p>
&lt;h3 id="loss-function">Loss Function&lt;/h3>
&lt;p>$$
L = \frac{\sigma}{n}\sum_{i=1}^{n}{L_1^i+L_2}
$$&lt;/p>
&lt;p>主要选用交叉熵作为损失函数。&lt;/p>
&lt;ul>
&lt;li>$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成&lt;/li>
&lt;li>具体来说，这个X应该是FEM模块中，每一层的feature在information concentration之前，interpolate后做交叉熵的值&lt;/li>
&lt;li>$L_2$ 最后prediction和label的交叉熵。&lt;/li>
&lt;/ul></description></item></channel></rss>