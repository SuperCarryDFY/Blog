[{"content":"Most of the contents can be found from E(n) Equivariant Graph Neural Networks\nDefine Equivariance Then the Translation, Rotation, Permutation equivariance can be defined as followes:\nDefine Invariance Similar to equivariance, invariance meas after a function, the output remains invariance when the input performs translation, rotation or permutation.\n","date":"2023-07-10T00:00:00Z","permalink":"https://SuperCarryDFY.github.io/Blog/p/about-equivariant-and-invariant/","title":"About Equivariant and Invariant"},{"content":"Score based generative modeling with SDEs diffusion process可以被表述为以下形式\nreverse-time SDE可以被表述为以下形式（可以看到需要知道分布分数s_theta）\nestimating scores for the SDE 和SMLD那篇文章一样，用denoising score matching的方式训练：\nVE,VP SDEs and Beyond 这里讲了SMLD,DDPM和SDE的关系（SMLD和DDPM可以看作离散的SDEs的两种不同模式）。\n对SMLD（Variance Exploding SDE）：\n对DDPM（Variance Preserving SDE）：\nSolving the reverse SDE 作者提出了三种采样方式\ngeneral-purpose numerical SDE solvers（reverse diffusion samplers） DDPM的采样方法\n被称之为祖先采样（ancestral sampling），而作者提出了reverse diffusion samplers\n可以证明，ancestral sampling，当beta_i趋近于0的时候，可以转化为reverse diffusion samplers的形式\nPredictor-corrector samplers probability flow 对于每个SDE，存在一个确定性的diffusion过程：ODE\nODE速度更快但是生成的质量较差。\ncontroallable generation 懒得看了\n","date":"2023-07-04T00:00:00Z","image":"https://SuperCarryDFY.github.io/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/Untitled_hu4f7bf0bd396bfed18d49a99097cb1c52_4513_120x120_fill_box_smart1_3.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/score-based-generative-modeling-through-stochastic-differential-equations/","title":"SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS"},{"content":"\nIntroduction CNN The intrinsic characteristics impose crucial inductive biases for image processing. (平移不变性等)\nSelf-Attention The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )\nGenerally, we owed self-attention based model\u0026rsquo;s great performance to the global receptive field it have.\nConsidering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules.\nContribution:\nA strong underlying relation between self-attention and convolution is revealed An elegant integration of the self-attention and convolution module, which enjoys the benefits of both worlds Method Conv ( This can be better illustrated by image a )\nThey decompose the Conv with K*K kernel to two stages. On the first stage, K*K kernel can be seen as many 1*1 kernel. On the second stage, we can gain finally feature map by shift each feature map and sum them.\nSelf-attn ( This can be better illustrated by image b )\nLike Conv, they treat self-attn with two steps. Firstly, 1*1 kernel conv ( euqal to Fully Connected Network) can be used to generate query, key and value. Then, they use query and key to gain weight and put it on the value.\nACmix The first stage of Conv and Self-attn can be shared. In the second stage, there are two paths which corresponds to each paradigm. Then, two learnable parameters $\\alpha$ and $\\beta$ is set for reweighting each feature map for sum up.\nShift In the Conv path, they add a small trick compared to normal Conv.\nShifting tensors towards various directions practically breaks the data locality and is difficult to achieve vectorized implementation. This may greatly impair the actual efficiency of our module at the inference time. ( This part I don\u0026rsquo;t understand perfectly)\nAs a remedy, they turn to apply depthwise convolution with fixed kernels. Here comes to a question. Now that you use conv kernel to shift feature map, why not make it learnable? So they use learnable kernels and initialized as shift kernels.\nExperiments The idea is quite simple. So They need strong experiments result to support them. They use their module in many models and many task( classification, detection and segmentation ). All seems good.\nalpha and beta At last, they check $\\alpha$ and $\\beta$ in each layer. This leads to a interesting conclusion. We can see that at early stage $\\beta$ take dominate position. This means Conv is a good feature extractor. But at the last stage, self-attention shows superiority over convolution.\nThis is also consistent with the design patterns in the previous works where self-attention is mostly adopted in the last stages to replace the original 3×3 convolution, and convolutions at early stages are proved to be more effective for vision transformers\n","date":"2022-11-05T00:00:00Z","image":"https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/","title":"On the Integration of Self-Attention and Convolution"},{"content":"Dilated Convolution Common Conv\nDilated Conv\n空洞卷积在一定程度上能增大卷积神经网络的感受野，但是利用其设计语义分割网络则会存在如下两个问题。\n1. The Gridding Effect\n如果和之前的操作一样，仅仅只是反复叠加3*3的kernal的话，那么在过程中就会存在一定的信息损失。\n2. 对小物体的分割\n增大感受野对小物体的分割似乎没有好处\n因此图森组提出了较好的解决方法：Hybrid Dilated Convolution (HDC)\n叠加卷积的 dilation rate 不能有大于1的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。我们将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构。 需要满足$$M_i = max[M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$$， 其中$r_i$，$M_i$分别代表第i层的dilation rate和最大dilation rate。 来自如何理解空洞卷积（dilated convolution）？， 作者@刘诗昆\nDeconvolution deconv大致可以分成如下三个方面\nunsupervised learning CNN可视化 upsampling 呃。。一般来说上采样+卷积的性能比反卷积要好，况且反卷积存在棋盘格效应。\n来自如何理解深度学习中的deconvolution networks？，作者@谭旭\n","date":"2022-10-13T00:00:00Z","permalink":"https://SuperCarryDFY.github.io/Blog/p/dilated-convolution-deconvolution/","title":"Dilated Convolution \u0026 Deconvolution "},{"content":"\nThis paper is from ICCV 2019. It addressed detection tasks R-CNN based network. However, it simply uses a shared classifier( mlp, I think) and bbox regressor, which is put forward in R-CNN though, to get predictions. Also, it tries a new form loss function out and gets a relatively large promotion.\nABSTRUCT \u0026amp; INTRODUCTION The model is mainly consist of two modules, i.e.,\na meta feature learner. a light-weight feature reweighting module. The training process is corresponding to two-phase learning scheme,\nfirst learn meta features and good reweighting module from base classes. fine-tune the detection model to adapt to novel classes. Though it contains two-phase training, it\u0026rsquo;s an end-to-end method.\nMETHOD Reweighting Module\nThis module taking the support examples as input learns to embed support information into reweighting vectors and adjusts the contribution of each meta feature of the query image accordingly for the following detection prediction module. It is like what is used in BAM. However, in BAM it just uses global max pooling ( or global average pooling, whatever), which is not learnable, to get the prototype vectors. This paper uses a learnable layer to get prototype vectors. After that, they apply prototype vectors to obtain the class-specific feature Fi for novel class i by F_i = F \\times w_i, where \\times means channel-wise multiplication.\nShared Classifier \u0026amp; BBox Regressor\nThere aren\u0026rsquo;t any details about Shared Classifier \u0026amp; BBox Regressor module. I simply think this module just being MLP combines BBox Regressor.\nLearning Scheme\nIn the first stage, they just feed the model with abundant base images with annotations. In this way, the model can learn to coordinate the two modules in the desired way.\nIn the second stage, they fine-tune the model on both base and novel classes. The training procedure is the same as the first phase, except that it takes significantly fewer iterations for the model to converge.\nAfter two training phases, the model can do a test without a novel class as input (it should not be deemed as a novel class, for it has seen the classes in the second stage) and reweighting module, because it remembers the prototype vectors of all class and just do inference in novel images.\nLOSS FUNCTION It is intuitive to use binary cross-entropy as a detection loss function, regressing 1 if the object is the target class and 0 otherwise. However, binary cross-entropy strives to produce balanced positive and negative predictions and could not remove such false predictions. Instead, they adopt a softmax layer to calibrate the classification scores among different classes. It can be denoted as \\hat c_i = \\frac{e^{c_i}}{\\sum_{j=1}^Ne^{c_j}}. Then, the loss function is below： $$ L_c= -\\sum_{i=1}^{N}1(·,i)log(\\hat c_i) $$\nwhere 1(·, i) is an indicator function for whether the current anchor box really belongs to class i or not. Finally, the overall loss function is $$ L_{det} = L_c + L_bbx + L_{obj} $$\n","date":"2022-09-13T00:00:00Z","image":"https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/few-shot-object-detection-via-feature-reweighting/","title":"Few-shot Object Detection via Feature Reweighting"},{"content":"\nI read the paper in the first month when I came in MiLab. However, at that time I could not recognize the quality of this method (I usually don\u0026rsquo;t read the experiment carefully). It appeared when I was reading another paper called \u0026ldquo;Holistic Prototype Activation for Few-Shot Segmentation\u0026rdquo;. The HPA performs well but can not beat BAM (although HPA has less parameter to train, obviously). So I decided to read this paper again.\nABSTRACT \u0026amp; INTRODUCTION Previous problem:\nthe trained models are biased towards the seen classes instead of being ideally class-agnostic Contribution\nBAM, i.e., base and the meta has two branches, allowing model to learn what not to segment and what to segmentation, respectively.\nDesign a special loss in order to train two branches suitably.\nExtend the proposed approach to a more challenging setting, which simultaneously identifies the targets of base and novel classes.\nIn the following image, (a) is a classical method to address FSS task; (b) is BAM approach; (c) is the extension of BAM\nMETHOD They adopt two stage training method, which means they train base learner and meta learner separately.\nBASE LEARNER Query image goes through four ResNet blocks and becomes intermediate feature maps $f_b^q$. Then the decoder network $D_b$ yields the prediction result. $N_b$ represents the number of base categories. $$ P_b = softmax(D_b(f_b^q)) \\in R^{(1+N_b)\\times H\\times W} $$ Loss can be defined as $$ L_{base} = \\frac{1}{n_{bs}}\\sum_{i=1}^{n_{bs}}CE(P_{b;i},m^q_{b;i}) $$ It is worth noting that they do not employ the general FSS learning paradigm (update the parameter in each episode). And they train the base learner independently. They explain as follow:\nIt is unrealistic to additionally build such a large network on the basis of the original few-shot model, which will introduce too many parameters and slow down the inference speed.\nIt is unknown whether the base learner can be trained well with the episodic learning paradigm, so a two stage training strategy is eventually adopted.\nIn the ablation study, it shows that with two stage train, the model can perform better.\nMETA LEARNER This part is highly resemble similar to CANet, employing \u0026ldquo;expand \u0026amp; concatenate\u0026rdquo; operations. The loss can be described as $$ L_{meta} = \\frac{1}{n_e}\\sum_{i=1}^{n_e}BCE(p_{m;i},m_i^q) $$ , when $n_e$ denotes the number of training episodes in each batch\nEnsemble This part is designed to leverage the low-level feature to adjust the coarse predictions which is derived from meta learner.\nFirstly, we calculate the overall indicator $\\psi$ for guiding the adjustment process: $$ A_s = F_{reshape}(f_{low}^s) \\in R^{C_1\\times N},\\ G^s = A_sA^T \\in R^{C_1\\times C_1} $$ $G^s$ should be denoted as Gram matrix, BTW.\nGram matrix can be regarded as eccentric covariance matrix between features. Every number in gram matrix describe the relation between every two feature, about which two features appear simultaneously, which two features just offset from each other, etc.\n$$ \\psi =||G^s - G^q||_F $$\n$||\\ ||_F$ denotes the Frobenius norm of the input metirx.\nAfter that, the final segmentation pridections $P_f$ can be described as follow: $$ p_f^0 = F_{ensemble}(F_\\psi(p_m^0),p_b^f), \\ p_f=p_f^0(+)F_\\psi(p_m^1) $$ where $p_m$,$p_b$ denote the predictions of the meta learner and base learner respectively. The superscript \u0026ldquo;0\u0026rdquo; and \u0026ldquo;1\u0026rdquo; represent the background and foreground respectively. Both $F_ψ$ and $F_{ensemble}$ are 1×1 convolution operations with specific initial parameters.\nLOSS $$ L=L_{final} + \\lambda L_{meta} \\ L_{final} = \\frac{1}{n_e}\\sum_{i=1}^{n_e}BCE(p_i^q,m_i^q) $$\nwhere $L_{meta}$ is the loss function of the meta learner defined in the meta learner stage.\nExperiments ","date":"2022-08-28T00:00:00Z","image":"https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/","title":"Learning What Not to Segment A New Perspective on Few-Shot Segmentation"},{"content":"\nABSTRACT \u0026amp; INTRODUCTION 这篇实质是完成一个分类任务，并且能在unseen objects上也能辨认出其affordance，应该比较容易拿过来做few-shot任务。\nMETHOD train用这张图，test用下面object affordance recognition那张图\nAFFORDANCE TRANSFER LEARNING Efficient HOI Composition\nTo compose a new HOI by the object $\\hat{l_o}$ and verb $l_v$, we assign the label to the composite HOI as follows,\n$$ \\hat{y} = (\\hat{l_o}A_o) \\and (l_vA_v) $$\n，其中$A_o$和$A_v$是分别关于object和verb的同现矩阵co-occurrence matrix（？）\nInvalid HOI Elimination\n有些HOI是无效的（比如ride orange），所以把无效HOI在上式矩阵的对应位置清零了\nOBJECT AFFORDANCE RECOGNITION 这里主要解释如何在test phase做推理\n对每个affordance，我们随机抽取M（这里M=100）个instances，抽取完特征之后作为affordance feature bank\n对一个输入的object feature，我们把它和bank所有的affordance一一结合起来，把所有的HOI predictions都转换成affordance prediction（这有啥区别），然后就得到了有许多重复元素的affordance lists。一个元素重复得越多说明有这个affordance的可能习惯越大。\nOPTIMIZATION AND INFERENCE loss就比较常规 $$ L = L_{hoi_sp}+\\lambda_1L_{hoi}+\\lambda_2L_{ATL} $$ ，其中$\\lambda_1$,$\\lambda_2$都是超参数。\n","date":"2022-08-22T00:00:00Z","image":"https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/affordance-transfer-learning-for-human-object-interaction-detection/","title":"Affordance Transfer Learning for Human-Object Interaction Detection"},{"content":"\nABSTRACT \u0026amp; INTRODUCTION 这篇文章的工作是，输入一张图片，输出HOI特征，并且用HOI短语作为监督训练（基于CLIP）。\n他与其他HOI transfer工作不同的点在于，之前的工作对unseen object都是用discrete label作为输出，得到HOI。但是这就要求label中对应的HOI（就是每个动作）都预训练过，很难去识别interaction that out of the predefined list。这篇的思路主要是对文字和图像同时encode，然后寻找最近的匹配对，所以不存在这个问题。\nMETHOD 他们定义了HOI为 ${(b_p,b_o,a,o)}$（有些文章定义为triplet，其实都差不多，最重要的是verb），其中$b_p$和$b_o$是人和物的bounding box。$a$和$o$分别是human action和object category。\nPRELIMINARY 在preliminary中，他们简单设想了一些步骤，用Faster RCNN得到bounding box然后输入到CLIP，就能在Unseen数据集上得到SOTA，even without tuning.（废话，人家设计的时候也没管unseen HOI啊）\nPROPOSED METHOD visual input: ${(h,c,b_p,b_o)}$,\nh is feature representation for interactions $b_p,b_o$ is bounding box c is the confidence score for bounding box prediction text encoder: raw text of interactions\nsimilarity $h^Ts$, where h, s denote the output of visual encoder and text encoder. They are semantic features in the same dimension.\nViT-based Visual Encoder\n前半部分输入跟ViT一样，对图像分patch之后加position embedding，在首位置插入CLS后直接作为输入放到ViT中。其中，关于CLS的理解如下\nCLS的特点\n不基于图像内容 位置编码固定 好处\n该token随机初始化，能够编码整个序列的统计特性 本身不基于图像内容，避免对某个特定的token产生偏向性 但是和ViT不同的是，这里期望能识别出多种不同的HOI（ViT最开始是做分类任务的）。所以在原本的序列后面另外加入了M个CLS（留了M个空让网络自己去学）。\n然而，输入的patch（以下称为X）和作为HOI的CLS（以下称为H）在ViT中作计算时亦有不同。\nX就是正常经过Transformer block，不管H。其中MHA是多层注意力机制，LN是layer norm，MLP是2层感知机。 $$ X_l^{\u0026rsquo;} = MHA(X_{l-1})+X_{l-1} \\ X_l = MLP(LN(X_l^{\u0026rsquo;}))+X_l^{\u0026rsquo;} $$ H在做多层注意力机制时，需要聚合来自X的信息（但是不需要位置0的CLS，他们说如果不mask的话，HOI会直接copy位置0的信息） $$ H_l^{\u0026rsquo;}=MHA(H_{l-1},X_{l-1}^{[1:]}) + H_{l-1} \\ H_l = MLP(LN(H_l^{\u0026rsquo;}))+H_l^{\u0026rsquo;} $$ HOI Sequence Parser\n从transformer block出来的HOI序列有一个问题，没办法区分开来。这其实也好理解，因为输入的时候并没有带位置信息，实际上H之间都是等效的。\n所以作者故意在这个模块中使用Sequence manner 而不是 in parallel的方式。\nProject Head and Bounding Box Regressor\nProject Head其实就是一个线性层，把从transformer block出来的X映射到text encoder的输出的维度，方便做相似度计算，从而找到最近的tensor。\nLOSS LOSS部分没有很复杂，由两部分组成，分别是box head输出和project head的loss $$ L_m(i,\\phi) = L_b(\\hat{b_p^i},b_p^{\\phi_i})+L_b(\\hat{b_o^i},b_o^{\\phi_i})+L_h(\\hat{h_i},s_{\\phi_i}) $$ 其中$L_b$表示Bounding box 的loss，$L_h$代表CLIP那边的loss，包括text-to-visual和visual-to-text\n","date":"2022-08-22T00:00:00Z","image":"https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/","title":"Learning Transferable Human-Object Interaction Detector with Natural Language Supervision"},{"content":"Some papers are famous and have gotten several citations. They always appear when I\u0026rsquo;m reading current papers. So I decided to write down the most critical parts (concerned mainly with myself, in another way) from these papers on this page.\nLearning Deep Features for Discriminative Localization This is a well-known work from CVPR2016, which has got 6.5k citation numbers up-to-date.\nThere are two parts that seem to be important to me: the comparison between global max pooling and global average pooling, as well as the framework.\nGMP vs. GAP We believe that GAP loss encourages the network to identify the extent of the object as compared to GMP which encourages it to identify just one discriminative part.\nWhile GMP achieves similar classification performance as GAP, GAP outperforms GMP for localization.\nGAP can focus on a wide range of pixels while GMP only depends on the most significant feature.\nFRAMEWORK The output of the last convolutional layer is denoted as $f_k(x,y)$, while k means the channel. After GAP, which should be expressed as $F^k = \\sum_{x,y}f_k(x,y)$, for a given class, the input to the softmax $S_c$, is $\\Sigma_kw_k^cF_k$ where $w_k^c$ is weight corresponding to class c for unit k. $$ S_c = \\sum_kw_k^c\\sum_{x,y} f_k(x,y) = \\sum_{x,y}\\sum_k w_k^cf_k(x,y) $$ They did upsample in the middle of the framework to fit the size.\nABOUT weakly supervised: They meant weakly-supervised because the labels are image-level but localization is object-level\nLearning to Compare: Relation Network for Few-Shot Learning 这篇文章的思路相当简单，而且比原型网络那篇更好理解，是最早的metric-base few-shot learning的一批，并且现在依然流行（其实现在的方法就是在这个基础上修改，大体的框架是一样的）。\nFRAMEWORK framework基本上一眼就能看明白，$f_\\phi$和$g_\\phi$都是卷积，池化，batch norm操作，没什么特别的。concatenation似乎就是从这里来的，现在基本也都是把query feature和support feature concatenate起来。其过程可用如下公式表示 $$ r_{i,j} = g_\\phi(C(f_\\phi(x_i),f_\\phi(x_j))) $$ 其中$f_{\\phi}$，$g_{\\phi}$表示embedding module和relation module，函数$C(·，·)$表示concatenation。\nZERO-SHOT LEARNING 在zero-shot learning中，不再给support image，而是给semantic class embedding vector $v_c$，其过程可以用如下公式表示 $$ r_{i,j} = g_\\phi(C(f_{\\phi_1}(v_c),f_{\\phi_2}(x_j))) $$ 这里的DNN是训练好的模型，如VGG、Inception等。\n和prototypical network的区别\nRelation Network比原型网络的方法多了可学习的层，用来判别相似度度量，而在原型网络中是直接拿特定的相似度度量公式（如cosine similarity）\n","date":"2022-08-17T00:00:00Z","permalink":"https://SuperCarryDFY.github.io/Blog/p/combination-of-papers/","title":"Combination of Papers"},{"content":"\n从TPAMI20那篇过来的，主要想看一下哪里说的middle-level feature\nABSTRACT \u0026amp; INTRODUCTION consists of\na two-branch dense comparison module performs multi-level feature comparison between the support image and the query image an iterative optimization module iteratively refines the predicted results. 有趣的地方\n先前的工作，从1-shot拓展到k-shot时，都是用non-learnable fusion，这篇文章中用的是attention mechanism。 做test的时候，不再输入support image mask了，而是输入support image bounding box. METHOD DENSE COMPARISON MODULE 在CNN中\nfeature in low layers often relate to low-level cues 比如颜色，边缘。个人感觉是因为感受野不够大 feature in higher layers relate to object-level concepts 比如物品种类 因为他们要求模型训练完之后具备一定的generalization，而middle-level fature有可能会包含来自没见过的物体的part。比如说训练的时候见过小轿车，那么在测试中，我就比较容易根据middle-level中的轮胎来segment公交车。（make sense）\ndialated convolution空洞卷积\n以下来自知乎\n7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。\n传统卷积核的本质问题：pool减小图片尺寸，upsampling增大图片尺寸，在这个过程中肯定有信息丢失掉了。因此空洞卷积就是不通过pooling也能获得较大感受野的方法\ndialated convolution的优点：内部数据结构的保留和避免使用 down-sampling 。\n具体的，他们把Resnet分成4个block，只用block2和block3的输出，将其concat到一起。后面就是support feature与support feaure相乘来把背景像素清0，avg pool之后repeat到之前的维度和query feature concat到一起，比较常规。\nINERATIVE OPTIMIZATION MODULE 每一次迭代中，首先进行如下运算。其中$M$是output of the residual blocks，$x$是DCM模块的输出，$y_{t-1}$是上一个迭代块的输出，$F$是concat之后再经过两层卷积层。 $$ M_t = x + F(x,y_{t-1}) $$ 对$M_t$，再经过两层vanilla residual blocks。然后再经过ASSP模块输出。\n就差不多这么迭代n次\nATTENTION MECHANISM FOR K-SHOT SEGMENTATION 这也算attention嘛? query, key, value分别是什么？\n在attention模块中，经过两层卷积再softmax得到$\\hat\\lambda_k$，然后把$\\hat\\lambda_k$和经过卷积的support sample n相乘。\nABLATION STUDY Feature for Comparison\n这里说单个的话block2效果是最好的，整体上block2+block3的效果最好。嗯，比较可信。\n其中提了一嘴While block4 corresponds to high-level features, e.g., categories, and incorporates a great number of parameters (2048 channels), which makes it hard to optimize under the fewshot setting.\n深有同感！\nAttention vs. Feature Fusion vs. Mask Fusion\n从这里可以看出Attention效果是最好的。但是相比来说的话还是多了两层卷积，增加了参数。Feature-Avg表现不错，感觉跟attention也差不多了（主要是没有另外加参数）。Mask-Avg这么奇怪的想法竟然也有效，能+0.5。还有Mask-OR你要笑死我嘛，怎么还没1-shot高啊，怎么回事啊小老弟，纯纯的帮倒忙。\n","date":"2022-08-16T00:00:00Z","image":"https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/","title":"CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning"},{"content":"argparse Tutorial\nargparsemodule makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.\n把他看作一个字典好了\n@property \u0026amp; @abstractmethod: @property: 通过这个可以把一个函数当作属性来使用\n@abstractmethod: 来自ABCMeta库，描述如下\nThis module provides the infrastructure for defining abstract base classes (ABCs) in Python\n好像就是说被这个装饰的函数不能实例化，但是其子类如果实现了该抽象方法的话就可以被实例化\nnp.transpose() 类似torch.permute\ntorch.stack() torch.stack(inputs, dim=?) → Tensor\n沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 例子 # 假设是时间步T1的输出 T1 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 假设是时间步T2的输出 T2 = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]]) print(torch.stack((T1,T2),dim=0).shape) print(torch.stack((T1,T2),dim=1).shape) print(torch.stack((T1,T2),dim=2).shape) print(torch.stack((T1,T2),dim=3).shape) # outputs: torch.Size([2, 3, 3]) torch.Size([3, 2, 3]) torch.Size([3, 3, 2]) IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3) # 报错 torch.autograd.Variable tensor是硬币的话，那Variable就是钱包，它记录着里面的钱的多少，和钱的流向\nTensor是存在Variable中的.data里的，而cpu和gpu的数据是通过 .cpu()和.cuda()来转换的\ntorch.nn.functional.interpolate 1 torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode=\u0026#39;nearest\u0026#39;, align_corners=None) 根据给定的size或者scale_factor参数来对输入进行上/下采样，参数：\ninput\u0026ndash;tensor \u0026ndash; 输入张量 size \u0026ndash;int or Tuple \u0026ndash; 输出大小 scale_factor \u0026ndash;float or Tuple\u0026ndash;指定输出为输入的多少倍数。如果输入为tuple，其也要制定为tuple类型 mode \u0026ndash;str\u0026ndash; 可使用的上采样算法，有'nearest', 'linear', 'bilinear', 'bicubic' , 'trilinear'和'area'. 默认使用``'nearest' align_corners \u0026ndash;bool\u0026ndash; 几何上，我们认为输入和输出的像素是正方形，而不是点。如果设置为True，则输入和输出张量由其角像素的中心点对齐，从而保留角像素处的值。如果设置为False，则输入和输出张量由它们的角像素的角点对齐，插值使用边界外值的边值填充;当scale_factor保持不变时，使该操作独立于输入大小。仅当使用的算法为'linear', 'bilinear', 'bilinear'or 'trilinear'时可以使用。默认设置为``False` torch.save torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\nSaves an object to a disk file.\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; t = torch.tensor([1., 2.]) \u0026gt;\u0026gt;\u0026gt; torch.save(t, \u0026#39;tensor.pt\u0026#39;) \u0026gt;\u0026gt;\u0026gt; torch.load(\u0026#39;tensor.pt\u0026#39;) tensor([1., 2.]) In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt;\u0026gt;\u0026gt; bn = torch.nn.BatchNorm1d(3, track_running_stats=True) \u0026gt;\u0026gt;\u0026gt; list(bn.named_parameters()) [(\u0026#39;weight\u0026#39;, Parameter containing: tensor([1., 1., 1.], requires_grad=True)), (\u0026#39;bias\u0026#39;, Parameter containing: tensor([0., 0., 0.], requires_grad=True))] \u0026gt;\u0026gt;\u0026gt; list(bn.named_buffers()) [(\u0026#39;running_mean\u0026#39;, tensor([0., 0., 0.])), (\u0026#39;running_var\u0026#39;, tensor([1., 1., 1.])), (\u0026#39;num_batches_tracked\u0026#39;, tensor(0))] \u0026gt;\u0026gt;\u0026gt; bn.state_dict() OrderedDict([(\u0026#39;weight\u0026#39;, tensor([1., 1., 1.])), (\u0026#39;bias\u0026#39;, tensor([0., 0., 0.])), (\u0026#39;running_mean\u0026#39;, tensor([0., 0., 0.])), (\u0026#39;running_var\u0026#39;, tensor([1., 1., 1.])), (\u0026#39;num_batches_tracked\u0026#39;, tensor(0))]) In the tutorials, it is recommended to instead save only its state dict. Python modules have a function,load_state_dict(), to restore their states from a state dict.\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; torch.save(bn.state_dict(), \u0026#39;bn.pt\u0026#39;) \u0026gt;\u0026gt;\u0026gt; bn_state_dict = torch.load(\u0026#39;bn.pt\u0026#39;) \u0026gt;\u0026gt;\u0026gt; new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True) \u0026gt;\u0026gt;\u0026gt; new_bn.load_state_dict(bn_state_dict) \u0026lt;All keys matched successfully\u0026gt; BN Batch Normalization\nInternal Covariate Shift\n在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。 Batch Normalization具体做法\n对当前层的第j个维度做规范化（有m个样本），使得每一层输入的每个特征的分布均值为0，方差为1 考虑到规范化后容易使得底层网络学习到的信息丢失，因此引入两个可学习的参数$\\gamma$和$\\beta$，对规范后的数据进行线性变换。 tensor.contiguous 一些tensor操作（traspose，permute）和原tensor是共享内存的，不会改变底层数组的存储，但是如果要使用view方法的话，就要求对应tensor的数据占用内存是连续的。\nTensor.contiguous(memory_format=torch.contiguous_format) -\u0026gt; Tensor\nReturns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n如果想要改变形状并且直接改内存的话，就用reshape\nvalidation data sets 为了调整超参数 也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。\n吴恩达老师的视频中，如果当数据量不是很大的时候（万级别以下）的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1；但是当可用的数据很少的情况下也可以使用一些高级的方法，比如留出方，K折交叉验证等。\n引入k-fold交叉验证的模板\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 data = load_data() train, test = split(data) hyper_parameters = set_hyper() k = init_k_fold() for hyper in hyper_parameters: metrics = [] for index in range(k): fold_train, fold_valid = cv_split(index, k, train) # 用此时的超参数训练模型 model = fit(fold_train, hyper) current_metric = evaluate(model, fold_valid) metrics.append(current_metric) avg_metric = np.mean(metrics) std_metric = np.std(metrics) # compare metrics among different hypers best_hyper = update_best() # finally model = fit(train, best_hyper) metrics = evaluate(model, test) # show your final metrics tensorboard 1 2 3 4 5 6 7 8 9 10 from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(\u0026#39;logs/xxx\u0026#39;) # 打印模型 # model是你想打印的模型，inputs是forword时输入的参数，如果有多个就以元组形式输出，其中元素必须是tensor writer.add_graph(model,inputs) # 打印loss for n_iter in train: writer.add_scalar(\u0026#34;Loss/train\u0026#34;, train_loss, n_iter) GPU加速 torch.nn.DataParallel\nCLASStorch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n首先在前向过程中，你的输入数据会被划分成多个子部分（以下称为副本）送到不同的device中进行计算，而你的模型module是在每个device上进行复制一份，也就是说，输入的batch是会被平均分到每个device中去，但是你的模型module是要拷贝到每个devide中去的，每个模型module只需要处理每个副本即可，当然你要保证你的batch size大于你的gpu个数。然后在反向传播过程中，每个副本的梯度被累加到原始模块中。概括来说就是：DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。\n1 2 device_ids = [0, 1] net = torch.nn.DataParallel(net, device_ids=device_ids) 实际上optimizer也可以使用dataparallel优化，此时需要注意到下面第二行返回的是一个module，因此optimizer需要当module使用\n1 2 3 4 5 6 optimizer = torch.optim.SGD(net.parameters(), lr=lr) optimizer = nn.DataParallel(optimizer, device_ids=device_ids) # 优化器原本使用 optimizer.step() # 修改之后优化器使用 optimizer.module.step() 实际上pytorch不建议用DataParallel。。。\ntorch.nn.parallel.DistributedDataParallel\n相比DataParallel\nDataParallel是单进程多线程的，只用于单机情况，而DistributedDataParallel是多进程的，适用于单机和多机情况，真正实现分布式训练； DistributedDataParallel的训练更高效，因为每个进程都是独立的Python解释器，避免GIL问题，而且通信成本低其训练速度更快，基本上DataParallel已经被弃用； DistributedDataParallel中每个进程都有独立的优化器，执行自己的更新过程，但是梯度通过通信传递到每个进程，所有执行的内容是相同的； https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\nhttps://zhuanlan.zhihu.com/p/113694038\nhttps://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel\nbenchmark benchmark in osad\nIoU: 交并比。 $IoU = \\frac{traget \\cap prediction}{target \\cup prediction}$ CC: Pearson product-moment correlation coefficient,皮尔逊相关系数。 $\\rho_{X,Y}=\\frac{cov(X，Y)}{\\sigma_X\\sigma_Y}$ MAE：Mean Absolute Error，平均绝对误差 torch.cuda.synchronize() 因为torch里面执行都是异步的，这行代码的意思是等待所有进程运行完。\n1 2 3 4 5 6 # 例子 torch.cuda.synchronize() start = time.time() result = model(input) torch.cuda.synchronize() end = time.time() torch.bmm torch.bmm(input, mat2, *, out=None) → Tensor\nIf input is a $(b \\times n \\times m)$ tensor, mat2 is a $(b \\times m \\times p)$ tensor, out will be a$(b \\times n \\times p)$ tensor.\ninput and mat2 must be 3-D tensors each containing the same number of matrics.\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; input = torch.randn(10, 3, 4) \u0026gt;\u0026gt;\u0026gt; mat2 = torch.randn(10, 4, 5) \u0026gt;\u0026gt;\u0026gt; res = torch.bmm(input, mat2) \u0026gt;\u0026gt;\u0026gt; res.size() torch.Size([10, 3, 5]) @classmethod 和@staticmethod classmethod 装饰器\n对类方法，需要先实例化类，再对实例化的对象调用方法。\n而使用@classmethod和@staticmethod就可以不用实例化了，直接类名.方法名()来调用。两者都不需要self参数\n区别是@classmethod的函数在定义时需要cls参数，调用类的方法；@staticmethod装饰的函数不需要cls参数。\n","date":"2022-08-16T00:00:00Z","permalink":"https://SuperCarryDFY.github.io/Blog/p/pytorch/","title":"Pytorch"},{"content":"\nABSTRACT \u0026amp; INTRODUCTION 从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）\nMotivations-\u0026gt;现在的FSS主要有俩问题\n会把不属于本类的物体也分割进来 分不清边界 对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体在train的时候见过，那么这个物体肯定就不是要分割的物体了(好像有点transductive了哈，但是严格说又不是)\nThe proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)\n对于第二个问题，他们说关键就是要找出support和query中的本质联系（0.0），他们提出了基于DeepLabv3的CRD模型来解决这个问题。\nWe argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images\nMETHOD PROTOTYPE ACQUISITION 在这个模块生成base class的prototype $$ P_b^c = \\frac{1}{N}\\sum_{i=1}^{N_c}\\frac{1}{|\\hat{L_c^i}|}\\sum_{j=1}^{HW}F_c^{ij}\\hat{L_c^{ij}} $$ 其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。\nPROTOTYPE ACTIVATION MODULE holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。\n对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\\phi$是11的卷积再加激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间 $$ A_k^{ij}=g_\\phi(F_q^{ij}+P^k_h) $$ 然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别） $$ k^ = argmax_k(A_k^{ij}) \\ M^{ij}=A_{k^}^{ij} ,\\ if \\ k^=1 \\ otherwise \\ 0 $$\n下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为前面不是已经做了匹配度计算了么，所以直接在对应位置做expand就好了，具体可见下图\nCROSS-REFERENCE DECODER LOSS ","date":"2022-08-15T00:00:00Z","image":"https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/holistic-prototype-activation-for-few-shot-segmentation/","title":"Holistic Prototype Activation for Few-Shot Segmentation"},{"content":"One-shot Affordance Detetion 2106.14747.pdf (arxiv.org)\nAbstract 可供性检测就是通过一张图片识别物体潜在的动作。 OS-AD网络可以在所有候选图片中帮助发现普遍的可供性，并且学会适应感知未发现的可供性。 他们建立了一个数据集PAD ：4k Image；31 affordance；72个物体类别 Introduction 挑战OS-AD 给一张图片，告知其图片上的物体的行为，则可以察觉所有物体普遍的可供性 问题：现实生活中一个物体可能有多个affordance（例如沙发可以躺也可以睡），而具体用什么affordance取决于人在这个场景中的目的。抛去目的的指引，直接从一张图片中学习affordance会导致忽略了其他视觉上的对此时的任务有效的affordance 从行为中找暗示 采用collaboration learning去捕捉不同物体间的潜在关系，抵消物体不同的appearance，增加泛化性；OS-AD PLM，PTM，CEM 可供性检测应该能适用于各种环境： PAD 目标驱动可供性数据集 Related work Affordance Detection One-Shot Learn based on metric learning using the siamese neural network 度量学习；孪生神经网络 meta-learning and generation models 元学习 Methods Framework input: query images, human-object interactions ResNet50 -\u0026gt; 获得图像表现 $X$ and $ X_{sup} $ 输入$X_{sup}$和 人和物体的边界矩阵到PLM -\u0026gt; 提取human-object interaction信息，对action-purpose编码，发现人想要旋转的原因 输入feature representation和$X$到PTM里面 -\u0026gt; 让网络学会处理带affordance的信息 输入encoded feature 到CEM， 输出affordance Purpose Learning Module :star:On Exploring Undetermined Relationships for Visual Relationship Detection受到了这篇文章的启发，说instance（人或物）的特征可以指导网络哪里应该focus。\n先得到$M_O$和$M_H$ （这两者分别代表什么？作者说是为了让模型去分别focus on物体和个人，引入了注意力机制，其中GMP的作用是得到最显著的特征） 其中⊗ 代表element-wise product，元素对应位置相乘，$f_O$和$f_H$是$X_O$和$X_H$进行 global maximum pooling（GMP）后的值 $$ M_O = Softmax(f_O⊗X_{sup})⊗X_{sup} \\ M_H = Softmax(f_H⊗X_{sup})⊗X_{sup} $$ 作者说他们用$f_O$去指导网络应该focus on人物交互$M_{HO}$ $$ M_{HO}=Conv(f_O⊗X_H) $$ 最后得到encoding of the action purpose $F_{sup}$，其中\u0026quot; ·\u0026ldquo;代表position-wise dot product. $$ F_{sup} = MaxPooling((M_{HO}·M_H)+(M_{HO}·M_O)) $$\n输入：$X_{sup}$以及人和物体的边界框 输出：动作目的编码 $F_{sup}$ Purpose Transfer Module 通过attention机制，将action purpose传递到query image中，加强相关features $$ X_{T_i} = X_i + Softmax(X_i⊗F_{sup})⊗X_i,\\ where\\ i \\ in\\ [1,n] $$\nCollaboration Enhancement Module 交替使用E-step和M-step，得到一个紧凑的基集，重建query image的特征图。\n从PTM输入的$X_T = {X_{T_1},\u0026hellip;,X_{T_n}}$经过卷积得到$F={F_1,\u0026hellip;F_n}$\n初始化基集$\\mu$\nE-step估计隐变量$Z={Z_1,\u0026hellip;Z_n}$\n第k个basis 第j个像素 第i个图片 $Z_{ijk} = \\frac{\\kappa(f_{ij},\\mu_k)}{\\sum_{l=1}^{K}\\kappa(f_{ij},\\mu_l)}$ $f_{ij}$第i个图像的第j个位置的特征 $\\kappa$是指数核函数 M-step更新基集$\\mu$，并把$\\mu$作为$F$的加权平均\n$\\mu_k = \\frac{\\sum_{i=1}^n\\sum_{j=1}^Lz_{ijk}f_{ij}}{\\sum_{i=1}^n\\sum_{j=1}^Lz_{ijk}}$ 经过E-M的迭代后，我们用$\\mu$和$Z$去重建$X$并得到$F$\n$F_i=Z_i\\mu$ $\\tilde X_i=X_i+Conv(F_i)$ 前景知识 Expectation-Maximization (E-M)\nhttps://zhuanlan.zhihu.com/p/67120173. 初始化参数 根据初始化的参数，划分类别 根据最大似然估计重新计算参数 重复步骤1-3，迭代n次，参数收敛 期望最大化注意力机制\nEMANet\nhttps://www.jianshu.com/p/6bb799d256b1 作者写的知乎专栏：https://zhuanlan.zhihu.com/p/78018142 分为$A_E,A_M,A_R$三部分组成，前两者是EM算法的E步和M步\n假定输入的特征图为$X\\in R^{N\\times C}$，基初始值为$\\mu\\in R^{K\\times C}$ $A_E$步估计隐变量$Z\\in R^{N\\times K}$，则第k个基对第n个像素的权责可以计算为 $z_{nk}=\\frac{\\kappa(x_n,\\mu_k)}{\\sum_{j=1}^{K}\\kappa(x_n,\\mu_j)}$ 实现时可以用公式 $Z=softmax(\\lambda X(\\mu^T))$，其中$\\lambda$作为超参数控制$Z$的分布 $A_M$步更新基。$\\mu$被计算为$X$的加权平均。第k个基被个更新为 $\\mu_k=\\frac{\\sum_{n=1}^Nz_{nk}X_n}{\\sum_{n=1}^Nz_{nk}}$ $A_E$和$A_M$交替执行T步后，$\\mu$和$Z$近似收敛，可以用来对X重新评估 $\\tilde X=Z\\mu$ Decoder $$ P^m_i=Conv(Unsample(Conv(X^m_i)+P^{m+1}_i)),\\ where\\ m \\ in \\ [1,4] $$\n其中m是第m层，i表示 把检测结果在与原图相同的特征维度还原出来\n用交叉熵Cross-entropy来作为损失函数\nExperiments k-fold evaluation protocol 将数据集分成三部分，其中之二作为训练集，剩下作为测试集 Benchmark Setting IoU metric for segmentation task 切割任务 Mean Absolute Error (MAE) measure the absolute error between the prediction and ground truth E-measure 相关文章 E-measure: Enhanced-alignment Measure for Binary Foreground Map Evaluation a metric that combines local pixels and image-level average values to jointly capture image-level statistics and local pixel matching information. Pearson Correlation Coefficient (CC) 皮尔逊相关系数 两个变量之间的协方差和标准差的商 $$ p_{X,Y}=\\frac{cov(X,Y)}{\\sigma_x\\sigma_y} $$ measure the correlation between prediction and ground truth 其他训练参数\nAdam optimizer resnet50 The input is randomly clipped from 360×360 to 320×320 with random horizontal flipping. 随机裁剪+水平翻转 40 epochs on 1080ti learning rate 1e-4 the number of bases in CEM is $K=256$ E-M 迭代次数 3 Quantitative and Qualitative Comparisons 对比下来就是我们的模型很好很好\n","date":"2022-08-15T00:00:00Z","image":"https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/one-shot-affordance-detection/","title":"One-Shot Affordance Detection"},{"content":"\nINTRODUCTION 主要解决了两个问题：\nGeneralization Reduction \u0026amp; High-Level Features. [CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱） 他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。 Spatial Inconsistency. 因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。 RELATED WORK Few-Shot Learning\nmeta-learning 跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。 metric-learning Prototypical network 这篇文章比较偏向于metric-learning METHOD Prior for Few-Shot Segmentation CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。\nPrior Generation的具体做法\n先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask $$ X_Q=F(I_Q), \\ X_S = F(I_S)\\times M_S $$\n$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity $$ cos(x_q,x_s)=\\frac{x_q^Tx_s}{|x_q||x_s|},\\ \\ \\ \\ q,s\\in{1,2,\u0026hellip;,hw} $$\n对每一个$x_q \\in X_Q$来说，取其中最大的值作为correspondence value $$ c_q = max_{s\\in {1,2\u0026hellip;,hw}}(cos(x_q,x_s)) $$\n$$ C_Q = [c_1,c_2,\u0026hellip;,c_hw] \\in R^{hw\\times1} $$\n把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization $$ Y_Q = \\frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\\epsilon} $$\nFeature Enrichment Module 将support image和query image关联起来的方法\n对support image做global average pooling 不用说都感觉效果一般 multi-level spatial information 说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。 作者提出的FEM可以很好的解决问题。其中M的具体操作如下\nLoss Function $$ L = \\frac{\\sigma}{n}\\sum_{i=1}^{n}{L_1^i+L_2} $$\n主要选用交叉熵作为损失函数。\n$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成 具体来说，这个X应该是FEM模块中，每一层的feature在information concentration之前，interpolate后做交叉熵的值 $L_2$ 最后prediction和label的交叉熵。 ","date":"2022-08-15T00:00:00Z","image":"https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png","permalink":"https://SuperCarryDFY.github.io/Blog/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/","title":"Prior Guided Feature Enrichment Network for Few-Shot Segmentation"}]