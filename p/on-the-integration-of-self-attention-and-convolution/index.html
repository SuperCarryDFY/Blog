<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Introduction CNN The intrinsic characteristics impose crucial inductive biases for image processing. (Âπ≥Áßª‰∏çÂèòÊÄßÁ≠â)
Self-Attention The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )
Generally, we owed self-attention based model&amp;rsquo;s great performance to the global receptive field it have.
Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules."><title>On the Integration of Self-Attention and Convolution</title><link rel=canonical href=https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/><link rel=stylesheet href=/Blog/scss/style.min.ac77dcf8b111b51da39a92990f431923f210f3876d85798a2125667f96dc33a4.css><meta property="og:title" content="On the Integration of Self-Attention and Convolution"><meta property="og:description" content="Introduction CNN The intrinsic characteristics impose crucial inductive biases for image processing. (Âπ≥Áßª‰∏çÂèòÊÄßÁ≠â)
Self-Attention The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )
Generally, we owed self-attention based model&amp;rsquo;s great performance to the global receptive field it have.
Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules."><meta property="og:url" content="https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/"><meta property="og:site_name" content="Dai Fengyuan"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="others"><meta property="article:published_time" content="2022-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-05T00:00:00+00:00"><meta property="og:image" content="https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png"><meta name=twitter:title content="On the Integration of Self-Attention and Convolution"><meta name=twitter:description content="Introduction CNN The intrinsic characteristics impose crucial inductive biases for image processing. (Âπ≥Áßª‰∏çÂèòÊÄßÁ≠â)
Self-Attention The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )
Generally, we owed self-attention based model&amp;rsquo;s great performance to the global receptive field it have.
Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/Blog><img src=/Blog/img/avatar_hu9dfbd12334e37b5fd8ca00b30d694e40_207128_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>üëø</span></figure><div class=site-meta><h1 class=site-name><a href=/Blog>Dai Fengyuan</a></h1><h2 class=site-description>Bachelor at SHU.</h2></div></header><ol class=social-menu><li><a href=https://github.com/SuperCarryDFY/Blog target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/Blog/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/Blog/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/Blog/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/Blog/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/Blog/p/on-the-integration-of-self-attention-and-convolution/><img src=https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png loading=lazy alt="Featured image of post On the Integration of Self-Attention and Convolution"></a></div><div class=article-details><header class=article-category><a href=/Blog/categories/cv/>CV</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/Blog/p/on-the-integration-of-self-attention-and-convolution/>On the Integration of Self-Attention and Convolution</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 05, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p><img src=https://s2.loli.net/2022/11/05/q6tB2b4KcskzQZn.png loading=lazy alt=image.png></p><h2 id=introduction>Introduction</h2><p><strong>CNN</strong> The intrinsic characteristics impose crucial inductive biases for image processing. (Âπ≥Áßª‰∏çÂèòÊÄßÁ≠â)</p><p><strong>Self-Attention</strong> The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )</p><blockquote><p>Generally, we owed self-attention based model&rsquo;s great performance to the global receptive field it have.</p></blockquote><p>Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules.</p><p><strong>Contribution:</strong></p><ul><li>A strong underlying relation between self-attention and convolution is revealed</li><li>An elegant integration of the self-attention and convolution module, which enjoys the benefits of both worlds</li></ul><h2 id=method>Method</h2><p><img src=https://s2.loli.net/2022/11/05/4cCOJaNnGZPq56Q.png loading=lazy alt=image.png></p><h3 id=conv>Conv</h3><p>( This can be better illustrated by image a )</p><p>They decompose the Conv with K*K kernel to two stages. On the first stage, K*K kernel can be seen as many 1*1 kernel. On the second stage, we can gain finally feature map by shift each feature map and sum them.</p><h3 id=self-attn>Self-attn</h3><p>( This can be better illustrated by image b )</p><p>Like Conv, they treat self-attn with two steps. Firstly, 1*1 kernel conv ( euqal to Fully Connected Network) can be used to generate query, key and value. Then, they use query and key to gain weight and put it on the value.</p><h3 id=acmix>ACmix</h3><p>The first stage of Conv and Self-attn can be shared. In the second stage, there are two paths which corresponds to each paradigm. Then, two learnable parameters $\alpha$ and $\beta$ is set for reweighting each feature map for sum up.</p><h4 id=shift>Shift</h4><p><img src=https://s2.loli.net/2022/11/05/hKvFbsuZg6Qkt7w.png loading=lazy alt=image.png></p><p>In the Conv path, they add a small trick compared to normal Conv.</p><blockquote><p>Shifting tensors towards various directions practically breaks the data locality and is difficult to achieve vectorized implementation. This may greatly impair the actual efficiency of our module at the inference time. ( This part I don&rsquo;t understand perfectly)</p></blockquote><p>As a remedy, they turn to apply depthwise convolution with fixed kernels. Here comes to a question. Now that you use conv kernel to shift feature map, why not make it learnable? So they use learnable kernels and initialized as shift kernels.</p><h2 id=experiments>Experiments</h2><p>The idea is quite simple. So They need strong experiments result to support them. They use their module in many models and many task( classification, detection and segmentation ). All seems good.</p><p><img src=https://s2.loli.net/2022/11/05/TXMgFrQBpkd6Wcs.png loading=lazy alt=image.png></p><p><img src=https://s2.loli.net/2022/11/05/qn4bxQupHwck21A.png loading=lazy alt=image.png></p><h3 id=alpha-and-beta>alpha and beta</h3><p><img src=https://s2.loli.net/2022/11/05/VJ5gh2LsanSkfIH.png loading=lazy alt=image.png></p><p>At last, they check $\alpha$ and $\beta$ in each layer. This leads to a interesting conclusion. We can see that at early stage $\beta$ take dominate position. This means Conv is a good feature extractor. But at the last stage, self-attention shows superiority over convolution.</p><blockquote><p>This is also consistent with the design patterns in the previous works where self-attention is mostly adopted in the last stages to replace the original 3√ó3 convolution, and convolutions at early stages are proved to be more effective for vision transformers</p></blockquote></section><footer class=article-footer><section class=article-tags><a href=/Blog/tags/others/>others</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>CC BY-NC-ND</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/Blog/p/dilated-convolution-deconvolution/><div class=article-details><h2 class=article-title>Dilated Convolution & Deconvolution</h2></div></a></article><article class=has-image><a href=/Blog/p/few-shot-object-detection-via-feature-reweighting/><div class=article-image><img src=https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png loading=lazy data-key data-hash=https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png></div><div class=article-details><h2 class=article-title>Few-shot Object Detection via Feature Reweighting</h2></div></a></article><article class=has-image><a href=/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/><div class=article-image><img src=https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png loading=lazy data-key data-hash=https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png></div><div class=article-details><h2 class=article-title>Learning What Not to Segment A New Perspective on Few-Shot Segmentation</h2></div></a></article><article class=has-image><a href=/Blog/p/affordance-transfer-learning-for-human-object-interaction-detection/><div class=article-image><img src=https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png loading=lazy data-key data-hash=https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png></div><div class=article-details><h2 class=article-title>Affordance Transfer Learning for Human-Object Interaction Detection</h2></div></a></article><article class=has-image><a href=/Blog/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/><div class=article-image><img src=https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png loading=lazy data-key data-hash=https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png></div><div class=article-details><h2 class=article-title>Learning Transferable Human-Object Interaction Detector with Natural Language Supervision</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Dai Fengyuan</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.13.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#method>Method</a><ol><li><a href=#conv>Conv</a></li><li><a href=#self-attn>Self-attn</a></li><li><a href=#acmix>ACmix</a><ol><li><a href=#shift>Shift</a></li></ol></li></ol></li><li><a href=#experiments>Experiments</a><ol><li><a href=#alpha-and-beta>alpha and beta</a></li></ol></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/Blog/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>