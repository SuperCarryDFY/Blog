<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="argparse Tutorial
argparsemodule makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.
把他看作一个字典好了
@property &amp;amp; @abstractmethod: @property: 通过这个可以把一个函数当作属性来使用
@abstractmethod: 来自ABCMeta库，描述如下
This module provides the infrastructure for defining abstract base classes (ABCs) in Python"><title>Pytorch</title><link rel=canonical href=https://SuperCarryDFY.github.io/p/pytorch/><link rel=stylesheet href=/scss/style.min.ac77dcf8b111b51da39a92990f431923f210f3876d85798a2125667f96dc33a4.css><meta property="og:title" content="Pytorch"><meta property="og:description" content="argparse Tutorial
argparsemodule makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.
把他看作一个字典好了
@property &amp;amp; @abstractmethod: @property: 通过这个可以把一个函数当作属性来使用
@abstractmethod: 来自ABCMeta库，描述如下
This module provides the infrastructure for defining abstract base classes (ABCs) in Python"><meta property="og:url" content="https://SuperCarryDFY.github.io/p/pytorch/"><meta property="og:site_name" content="Dai Fengyuan"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="code"><meta property="article:published_time" content="2022-08-16T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-16T00:00:00+00:00"><meta name=twitter:title content="Pytorch"><meta name=twitter:description content="argparse Tutorial
argparsemodule makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.
把他看作一个字典好了
@property &amp;amp; @abstractmethod: @property: 通过这个可以把一个函数当作属性来使用
@abstractmethod: 来自ABCMeta库，描述如下
This module provides the infrastructure for defining abstract base classes (ABCs) in Python"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9dfbd12334e37b5fd8ca00b30d694e40_207128_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>👿</span></figure><div class=site-meta><h1 class=site-name><a href=/>Dai Fengyuan</a></h1><h2 class=site-description>Bachelor at SHU.</h2></div></header><ol class=social-menu><li><a href=https://github.com/SuperCarryDFY target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/others/>Others</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/pytorch/>Pytorch</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Aug 16, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><h2 id=argparse><strong>argparse</strong></h2><p><a class=link href=https://docs.python.org/3/library/argparse.html target=_blank rel=noopener>Tutorial</a></p><p><code>argparse</code>module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and <a class=link href=https://docs.python.org/3/library/argparse.html#module-argparse target=_blank rel=noopener><code>argparse</code></a> will figure out how to parse those out of <a class=link href=https://docs.python.org/3/library/sys.html#sys.argv target=_blank rel=noopener><code>sys.argv</code></a>. The <a class=link href=https://docs.python.org/3/library/argparse.html#module-argparse target=_blank rel=noopener><code>argparse</code></a> module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.</p><p>把他看作一个字典好了</p><h2 id=property--abstractmethod><strong>@property & @abstractmethod</strong>:</h2><ul><li><p>@property: 通过这个可以把一个函数当作属性来使用</p></li><li><p>@abstractmethod: 来自ABCMeta库，描述如下</p></li></ul><blockquote><p>This module provides the infrastructure for defining <a class=link href=https://docs.python.org/3/glossary.html#term-abstract-base-class target=_blank rel=noopener>abstract base classes</a> (ABCs) in Python</p></blockquote><p>好像就是说被这个装饰的函数不能实例化，但是其子类如果实现了该抽象方法的话就可以被实例化</p><h2 id=nptranspose>np.transpose()</h2><p>类似torch.permute</p><h2 id=torchstack>torch.stack()</h2><p>torch.stack(inputs, dim=?) → Tensor</p><p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 例子</span>
</span></span><span class=line><span class=cl><span class=c1># 假设是时间步T1的输出</span>
</span></span><span class=line><span class=cl><span class=n>T1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        		<span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        		<span class=p>[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=c1># 假设是时间步T2的输出</span>
</span></span><span class=line><span class=cl><span class=n>T2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>30</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        		<span class=p>[</span><span class=mi>40</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>60</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        		<span class=p>[</span><span class=mi>70</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>90</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>((</span><span class=n>T1</span><span class=p>,</span><span class=n>T2</span><span class=p>),</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>((</span><span class=n>T1</span><span class=p>,</span><span class=n>T2</span><span class=p>),</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>((</span><span class=n>T1</span><span class=p>,</span><span class=n>T2</span><span class=p>),</span><span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>((</span><span class=n>T1</span><span class=p>,</span><span class=n>T2</span><span class=p>),</span><span class=n>dim</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># outputs:</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=ne>IndexError</span><span class=p>:</span> <span class=n>Dimension</span> <span class=n>out</span> <span class=n>of</span> <span class=nb>range</span> <span class=p>(</span><span class=n>expected</span> <span class=n>to</span> <span class=n>be</span> <span class=ow>in</span> <span class=nb>range</span> <span class=n>of</span> <span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=n>but</span> <span class=n>got</span> <span class=mi>3</span><span class=p>)</span> <span class=c1># 报错</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=torchautogradvariable>torch.autograd.Variable</h2><p>tensor是硬币的话，那Variable就是钱包，它记录着里面的钱的多少，和钱的流向</p><p>Tensor是存在Variable中的.data里的，而cpu和gpu的数据是通过 .cpu()和.cuda()来转换的</p><h2 id=torchnnfunctionalinterpolate>torch.nn.functional.interpolate</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>interpolate</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>scale_factor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;nearest&#39;</span><span class=p>,</span> <span class=n>align_corners</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>根据给定的size或者scale_factor参数来对输入进行上/下采样，参数：</p><ul><li>input&ndash;tensor &ndash; 输入张量</li><li>size &ndash;int or Tuple &ndash; 输出大小</li><li>scale_factor &ndash;float or Tuple&ndash;指定输出为输入的多少倍数。如果输入为tuple，其也要制定为tuple类型</li><li>mode &ndash;str&ndash; 可使用的上采样算法，有<code>'nearest'</code>, <code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code> , <code>'trilinear'和'area'</code>. <code>默认使用``'nearest'</code></li><li>align_corners &ndash;bool&ndash; 几何上，我们认为输入和输出的像素是正方形，而不是点。如果设置为True，则输入和输出张量由其角像素的中心点对齐，从而保留角像素处的值。如果设置为False，则输入和输出张量由它们的角像素的角点对齐，插值使用边界外值的边值填充;<code>当scale_factor保持不变时</code>，使该操作独立于输入大小。仅当使用的算法为<code>'linear'</code>, <code>'bilinear', 'bilinear'</code>or <code>'trilinear'时可以使用。</code>默认设置为``False`</li></ul><h2 id=torchsave>torch.save</h2><blockquote><p>torch.save(<em>obj</em>, <em>f</em>, <em>pickle_module=pickle</em>, <em>pickle_protocol=DEFAULT_PROTOCOL</em>, <em>_use_new_zipfile_serialization=True</em>)</p></blockquote><p><a class=link href=https://pytorch.org/docs/stable/notes/serialization.html#saving-loading-tensors target=_blank rel=noopener>Saves an object to a disk file.</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=s1>&#39;tensor.pt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;tensor.pt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>bn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>track_running_stats</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>list</span><span class=p>(</span><span class=n>bn</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span> <span class=n>Parameter</span> <span class=n>containing</span><span class=p>:</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)),</span>
</span></span><span class=line><span class=cl> <span class=p>(</span><span class=s1>&#39;bias&#39;</span><span class=p>,</span> <span class=n>Parameter</span> <span class=n>containing</span><span class=p>:</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>list</span><span class=p>(</span><span class=n>bn</span><span class=o>.</span><span class=n>named_buffers</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;running_mean&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl> <span class=p>(</span><span class=s1>&#39;running_var&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl> <span class=p>(</span><span class=s1>&#39;num_batches_tracked&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>(</span><span class=mi>0</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>bn</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>OrderedDict</span><span class=p>([(</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl>             <span class=p>(</span><span class=s1>&#39;bias&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl>             <span class=p>(</span><span class=s1>&#39;running_mean&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl>             <span class=p>(</span><span class=s1>&#39;running_var&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>])),</span>
</span></span><span class=line><span class=cl>             <span class=p>(</span><span class=s1>&#39;num_batches_tracked&#39;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>(</span><span class=mi>0</span><span class=p>))])</span>
</span></span></code></pre></td></tr></table></div></div><p>In the tutorials, it is recommended to instead save only its state dict. Python modules have a function,<code>load_state_dict()</code>, to restore their states from a state dict.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>bn</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s1>&#39;bn.pt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>bn_state_dict</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;bn.pt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>new_bn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>track_running_stats</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>new_bn</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>bn_state_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&lt;</span><span class=n>All</span> <span class=n>keys</span> <span class=n>matched</span> <span class=n>successfully</span><span class=o>&gt;</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=bn>BN</h2><p>Batch Normalization</p><p><strong>Internal Covariate Shift</strong></p><ul><li>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</li></ul><p>Batch Normalization具体做法</p><ul><li>对当前层的第j个维度做规范化（有m个样本），使得每一层输入的每个特征的分布均值为0，方差为1</li><li>考虑到规范化后容易使得底层网络学习到的信息丢失，因此引入两个可学习的参数$\gamma$和$\beta$，对规范后的数据进行线性变换。</li></ul><h2 id=tensorcontiguous>tensor.contiguous</h2><p>一些tensor操作（traspose，permute）和原tensor是共享内存的，不会改变底层数组的存储，但是如果要使用view方法的话，就要求对应tensor的数据占用内存是连续的。</p><blockquote><p>Tensor.contiguous(<em>memory_format=torch.contiguous_format</em>) -> Tensor</p></blockquote><p>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor. If <code>self</code> tensor is already in the specified memory format, this function returns the <code>self</code> tensor.</p><p>如果想要改变形状并且直接改内存的话，就用reshape</p><h2 id=validation-data-sets>validation data sets</h2><ul><li>为了调整超参数</li></ul><blockquote><p>也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。</p><p>吴恩达老师的视频中，如果当数据量不是很大的时候（万级别以下）的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1；但是当可用的数据很少的情况下也可以使用一些高级的方法，比如留出方，K折交叉验证等。</p></blockquote><p>引入k-fold交叉验证的模板</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>train</span><span class=p>,</span> <span class=n>test</span> <span class=o>=</span> <span class=n>split</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>hyper_parameters</span> <span class=o>=</span> <span class=n>set_hyper</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>init_k_fold</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>hyper</span> <span class=ow>in</span> <span class=n>hyper_parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>index</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>fold_train</span><span class=p>,</span> <span class=n>fold_valid</span> <span class=o>=</span> <span class=n>cv_split</span><span class=p>(</span><span class=n>index</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 用此时的超参数训练模型</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>fit</span><span class=p>(</span><span class=n>fold_train</span><span class=p>,</span> <span class=n>hyper</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>current_metric</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>fold_valid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>metrics</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>current_metric</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_metric</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>metrics</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>std_metric</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>metrics</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># compare metrics among different hypers</span>
</span></span><span class=line><span class=cl>    <span class=n>best_hyper</span> <span class=o>=</span> <span class=n>update_best</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># finally</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>fit</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>best_hyper</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>metrics</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># show your final metrics</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=tensorboard>tensorboard</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.tensorboard</span> <span class=kn>import</span> <span class=n>SummaryWriter</span>
</span></span><span class=line><span class=cl><span class=n>writer</span> <span class=o>=</span> <span class=n>SummaryWriter</span><span class=p>(</span><span class=s1>&#39;logs/xxx&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印模型</span>
</span></span><span class=line><span class=cl><span class=c1># model是你想打印的模型，inputs是forword时输入的参数，如果有多个就以元组形式输出，其中元素必须是tensor</span>
</span></span><span class=line><span class=cl><span class=n>writer</span><span class=o>.</span><span class=n>add_graph</span><span class=p>(</span><span class=n>model</span><span class=p>,</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印loss</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>n_iter</span> <span class=ow>in</span> <span class=n>train</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>writer</span><span class=o>.</span><span class=n>add_scalar</span><span class=p>(</span><span class=s2>&#34;Loss/train&#34;</span><span class=p>,</span> <span class=n>train_loss</span><span class=p>,</span> <span class=n>n_iter</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=gpu加速>GPU加速</h2><p><strong>torch.nn.DataParallel</strong></p><blockquote><p><em>CLASS</em><code>torch.nn.DataParallel</code>(<em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>)</p><p>首先在前向过程中，你的输入数据会被划分成多个子部分（以下称为副本）送到不同的device中进行计算，而你的模型module是在每个device上进行复制一份，也就是说，输入的batch是会被平均分到每个device中去，但是你的模型module是要拷贝到每个devide中去的，每个模型module只需要处理每个副本即可，当然你要保证你的batch size大于你的gpu个数。然后在反向传播过程中，每个副本的梯度被累加到原始模块中。概括来说就是：DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>device_ids</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=n>device_ids</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>实际上optimizer也可以使用dataparallel优化，此时需要注意到下面第二行返回的是一个module，因此optimizer需要当module使用</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=n>device_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 优化器原本使用</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 修改之后优化器使用</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>module</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>实际上pytorch不建议用DataParallel。。。</p><p><strong>torch.nn.parallel.DistributedDataParallel</strong></p><p>相比DataParallel</p><ul><li><code>DataParallel</code>是单进程多线程的，只用于单机情况，而<code>DistributedDataParallel</code>是多进程的，适用于单机和多机情况，真正实现分布式训练；</li><li><code>DistributedDataParallel</code>的训练更高效，因为每个进程都是独立的Python解释器，避免GIL问题，而且通信成本低其训练速度更快，基本上<code>DataParallel</code>已经被弃用；</li><li><code>DistributedDataParallel</code>中每个进程都有独立的优化器，执行自己的更新过程，但是梯度通过通信传递到每个进程，所有执行的内容是相同的；</li></ul><p><a class=link href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html target=_blank rel=noopener>https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></p><p><a class=link href=https://zhuanlan.zhihu.com/p/113694038 target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/113694038</a></p><p><a class=link href=https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel target=_blank rel=noopener>https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel</a></p><h2 id=benchmark>benchmark</h2><p>benchmark in osad</p><ul><li>IoU: 交并比。<ul><li>$IoU = \frac{traget \cap prediction}{target \cup prediction}$</li></ul></li><li>CC: Pearson product-moment correlation coefficient,皮尔逊相关系数。<ul><li>$\rho_{X,Y}=\frac{cov(X，Y)}{\sigma_X\sigma_Y}$</li></ul></li><li>MAE：Mean Absolute Error，平均绝对误差</li></ul><h2 id=torchcudasynchronize>torch.cuda.synchronize()</h2><p>因为torch里面执行都是异步的，这行代码的意思是等待所有进程运行完。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 例子</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=torchbmm>torch.bmm</h2><blockquote><p>torch.bmm(<em>input</em>, <em>mat2</em>, *, <em>out=None</em>) → Tensor</p></blockquote><p>If <code>input</code> is a $(b \times n \times m)$ tensor, <code>mat2</code> is a $(b \times m \times p)$ tensor, <code>out</code> will be a$(b \times n \times p)$ tensor.</p><p>input and mat2 must be 3-D tensors each containing the same number of matrics.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>mat2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>res</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=n>mat2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>res</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=classmethod-和staticmethod>@classmethod 和@staticmethod</h2><p>classmethod 装饰器</p><p>对类方法，需要先实例化类，再对实例化的对象调用方法。</p><p>而使用@classmethod和@staticmethod就可以不用实例化了，直接类名.方法名()来调用。两者都不需要self参数</p><p>区别是@classmethod的函数在定义时需要cls参数，调用类的方法；@staticmethod装饰的函数不需要cls参数。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/code/>code</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>CC BY-NC-ND</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/combination-of-papers/><div class=article-details><h2 class=article-title>Combination of Papers</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Dai Fengyuan</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.13.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#argparse><strong>argparse</strong></a></li><li><a href=#property--abstractmethod><strong>@property & @abstractmethod</strong>:</a></li><li><a href=#nptranspose>np.transpose()</a></li><li><a href=#torchstack>torch.stack()</a></li><li><a href=#torchautogradvariable>torch.autograd.Variable</a></li><li><a href=#torchnnfunctionalinterpolate>torch.nn.functional.interpolate</a></li><li><a href=#torchsave>torch.save</a></li><li><a href=#bn>BN</a></li><li><a href=#tensorcontiguous>tensor.contiguous</a></li><li><a href=#validation-data-sets>validation data sets</a></li><li><a href=#tensorboard>tensorboard</a></li><li><a href=#gpu加速>GPU加速</a></li><li><a href=#benchmark>benchmark</a></li><li><a href=#torchcudasynchronize>torch.cuda.synchronize()</a></li><li><a href=#torchbmm>torch.bmm</a></li><li><a href=#classmethod-和staticmethod>@classmethod 和@staticmethod</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>