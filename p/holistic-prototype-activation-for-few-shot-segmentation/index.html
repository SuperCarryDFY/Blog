<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="这篇真的好复杂
ABSTRACT &amp;amp; INTRODUCTION 从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）
现在的FSS主要有俩问题
会把不属于本类的物体也分割进来 分不清边界 对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了
The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i."><title>Holistic Prototype Activation for Few-Shot Segmentation</title><link rel=canonical href=https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/><link rel=stylesheet href=/scss/style.min.ac77dcf8b111b51da39a92990f431923f210f3876d85798a2125667f96dc33a4.css><meta property="og:title" content="Holistic Prototype Activation for Few-Shot Segmentation"><meta property="og:description" content="这篇真的好复杂
ABSTRACT &amp;amp; INTRODUCTION 从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）
现在的FSS主要有俩问题
会把不属于本类的物体也分割进来 分不清边界 对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了
The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i."><meta property="og:url" content="https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/"><meta property="og:site_name" content="Dai Fengyuan"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Segmentation"><meta property="article:tag" content="Few-Shot Learning"><meta property="article:published_time" content="2022-08-15T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-15T00:00:00+00:00"><meta name=twitter:title content="Holistic Prototype Activation for Few-Shot Segmentation"><meta name=twitter:description content="这篇真的好复杂
ABSTRACT &amp;amp; INTRODUCTION 从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）
现在的FSS主要有俩问题
会把不属于本类的物体也分割进来 分不清边界 对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了
The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9dfbd12334e37b5fd8ca00b30d694e40_207128_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>👿</span></figure><div class=site-meta><h1 class=site-name><a href=/>Dai Fengyuan</a></h1><h2 class=site-description>Bachelor at SHU.</h2></div></header><ol class=menu id=main-menu><li><a href=/about><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>About</span></a></li><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/cv/>CV</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/holistic-prototype-activation-for-few-shot-segmentation/>Holistic Prototype Activation for Few-Shot Segmentation</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Aug 15, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>1 minute read</time></div></footer></div></header><section class=article-content><p><img src=https://s2.loli.net/2022/08/18/B18VowS6RfMEJKX.png loading=lazy alt=image.png></p><p>这篇真的好复杂</p><h2 id=abstract--introduction>ABSTRACT & INTRODUCTION</h2><p>从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）</p><p>现在的FSS主要有俩问题</p><ul><li>会把不属于本类的物体也分割进来</li><li>分不清边界</li></ul><p><img src=https://s2.loli.net/2022/08/18/OJjBCcq7IUnFfey.png loading=lazy alt=image.png></p><p>对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了</p><blockquote><p>The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)</p></blockquote><p>对于第二个问题，他们说关键就是要找出support和query中的本质联系（套话），他们提出了基于DeepLabv3的CRD模型来解决这个问题。</p><blockquote><p>We argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images</p></blockquote><h2 id=method>METHOD</h2><p><img src=https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png loading=lazy alt=image.png></p><h3 id=prototype-acquisition>PROTOTYPE ACQUISITION</h3><p>在这个模块生成base class的prototype
$$
P_b^c = \frac{1}{N}\sum_{i=1}^{N_c}\frac{1}{|\hat{L_c^i}|}\sum_{j=1}^{HW}F_c^{ij}\hat{L_c^{ij}}
$$
其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。</p><h3 id=prototype-activation-module>PROTOTYPE ACTIVATION MODULE</h3><p><img src=https://s2.loli.net/2022/08/18/SeOERlPDLBuTasG.png loading=lazy alt=image.png></p><p>holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。</p><p>对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\phi$是激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间
$$
A_k^{ij}=g_\phi(F_q^{ij}+P^k_h)
$$
然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别）
$$
k^* = argmax_k(A_k^{ij})
$$</p><p>$$
M^{ij}=A_{k^<em>}^{ij} ,\ if \ k^</em>=1 \ otherwise \ 0
$$</p><p>下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为我前面不是已经做了匹配度计算了么，所以我直接在对应位置做expand就好了，具体可见下图</p><p><img src=https://s2.loli.net/2022/08/18/HumfepLVWd6byYa.png loading=lazy alt=image.png></p><h3 id=cross-reference-decoder>CROSS-REFERENCE DECODER</h3><p>这一步就比较乱了，左一步右一步的，还跟deeplabv3有关，我可能要先去把deeplabv3看了才能真正理解。</p><p><img src=https://s2.loli.net/2022/08/18/uwkmMHxE4P8q15Y.png loading=lazy alt=image.png></p><h2 id=loss>LOSS</h2><p>loss部分没太看懂，$\alpha$好理解，也是多任务下比较常见的做法。就是$L_{act}$是咋算的没太看懂，不是直接跟label做cross-entropy吗</p><p><img src=https://s2.loli.net/2022/08/18/y95dGIcosCltvVN.png loading=lazy alt=image.png></p></section><footer class=article-footer><section class=article-tags><a href=/tags/segmentation/>Segmentation</a>
<a href=/tags/few-shot-learning/>Few-Shot Learning</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/><div class=article-details><h2 class=article-title>CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</h2></div></a></article><article class=has-image><a href=/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/><div class=article-image><img src=https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png loading=lazy data-key data-hash=https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png></div><div class=article-details><h2 class=article-title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</h2></div></a></article><article><a href=/p/one-shot-affordance-detection/><div class=article-details><h2 class=article-title>One-Shot Affordance Detection</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 Dai Fengyuan</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.13.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#abstract--introduction>ABSTRACT & INTRODUCTION</a></li><li><a href=#method>METHOD</a><ol><li><a href=#prototype-acquisition>PROTOTYPE ACQUISITION</a></li><li><a href=#prototype-activation-module>PROTOTYPE ACTIVATION MODULE</a></li><li><a href=#cross-reference-decoder>CROSS-REFERENCE DECODER</a></li></ol></li><li><a href=#loss>LOSS</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>