<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Dai Fengyuan</title><link>https://SuperCarryDFY.github.io/post/</link><description>Recent content in Posts on Dai Fengyuan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 15 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>One-Shot Affordance Detection</title><link>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</guid><description>&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2106.14747.pdf" target="_blank" rel="noopener"
>One-shot Affordance Detetion 2106.14747.pdf (arxiv.org)&lt;/a>&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;ul>
&lt;li>可供性检测就是通过一张图片识别物体潜在的动作。&lt;/li>
&lt;li>OS-AD网络可以在所有候选图片中帮助发现普遍的可供性，并且学会适应感知未发现的可供性。&lt;/li>
&lt;li>他们建立了一个数据集PAD ：4k Image；31 affordance；72个物体类别&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>挑战OS-AD 给一张图片，告知其图片上的物体的行为，则可以察觉所有物体普遍的可供性&lt;/li>
&lt;li>问题：现实生活中一个物体可能有多个affordance（例如沙发可以躺也可以睡），而具体用什么affordance取决于人在这个场景中的目的。抛去目的的指引，直接从一张图片中学习affordance会导致忽略了其他视觉上的对此时的任务有效的affordance
&lt;ul>
&lt;li>从行为中找暗示&lt;/li>
&lt;li>采用&lt;strong>collaboration learning&lt;/strong>去捕捉不同物体间的潜在关系，抵消物体不同的appearance，增加泛化性；OS-AD PLM，PTM，CEM&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可供性检测应该能适用于各种环境： PAD 目标驱动可供性数据集&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;ul>
&lt;li>Affordance Detection&lt;/li>
&lt;li>One-Shot Learn
&lt;ul>
&lt;li>based on metric learning using the siamese neural network 度量学习；孪生神经网络&lt;/li>
&lt;li>meta-learning and generation models 元学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="methods">Methods&lt;/h2>
&lt;h3 id="framework">Framework&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png"
loading="lazy"
alt="framework2.png"
>&lt;/p>
&lt;ul>
&lt;li>input: query images, human-object interactions&lt;/li>
&lt;li>ResNet50 -&amp;gt; 获得图像表现 $X$ and $$ X_{sup} $$&lt;/li>
&lt;li>输入$X_{sup}$和 人和物体的边界矩阵到PLM -&amp;gt; 提取human-object interaction信息，对action-purpose编码，发现人想要旋转的原因&lt;/li>
&lt;li>输入feature representation和$X$到PTM里面 -&amp;gt; 让网络学会处理带affordance的信息&lt;/li>
&lt;li>输入encoded feature 到CEM， 输出affordance&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-learning-module">Purpose Learning Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/HUW6kjnPJ29qX83.png"
loading="lazy"
alt="plm.png"
>&lt;/p>
&lt;p>:star:&lt;a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_On_Exploring_Undetermined_Relationships_for_Visual_Relationship_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener"
>On Exploring Undetermined Relationships for Visual Relationship Detection&lt;/a>受到了这篇文章的启发，说instance（人或物）的特征可以指导网络哪里应该focus。&lt;/p>
&lt;p>先得到$M_O$和$M_H$ &lt;strong>（这两者分别代表什么？作者说是为了让模型去分别focus on物体和个人，引入了注意力机制，其中GMP的作用是得到最显著的特征）&lt;/strong> 其中⊗ 代表element-wise product，元素对应位置相乘，$f_O$和$f_H$是$X_O$和$X_H$进行 global maximum pooling（GMP）后的值
$$
M_O = Softmax(f_O⊗X_{sup})⊗X_{sup} \
M_H = Softmax(f_H⊗X_{sup})⊗X_{sup}
$$
作者说他们用$f_O$去指导网络应该focus on人物交互$M_{HO}$
$$
M_{HO}=Conv(f_O⊗X_H)
$$
最后得到encoding of the action purpose $F_{sup}$，其中&amp;quot; ·&amp;ldquo;代表position-wise dot product.
$$
F_{sup} = MaxPooling((M_{HO}·M_H)+(M_{HO}·M_O))
$$&lt;/p>
&lt;ul>
&lt;li>输入：$X_{sup}$以及人和物体的边界框&lt;/li>
&lt;li>输出：动作目的编码 $F_{sup}$&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-transfer-module">Purpose Transfer Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/qARrZbuCIm4BipT.png"
loading="lazy"
alt="ptm.png"
>&lt;/p>
&lt;p>通过attention机制，将action purpose传递到query image中，加强相关features
$$
X_{T_i} = X_i + Softmax(X_i⊗F_{sup})⊗X_i,\ where\ i \ in\ [1,n]
$$&lt;/p>
&lt;h3 id="collaboration-enhancement-module">Collaboration Enhancement Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/lx5Jb4jkPhIuSN3.png"
loading="lazy"
alt="cem.png"
>&lt;/p>
&lt;p>交替使用E-step和M-step，得到一个紧凑的基集，重建query image的特征图。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从PTM输入的$X_T = {X_{T_1},&amp;hellip;,X_{T_n}}$经过卷积得到$F={F_1,&amp;hellip;F_n}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化基集$\mu$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>E-step估计隐变量$Z={Z_1,&amp;hellip;Z_n}$&lt;/p>
&lt;ul>
&lt;li>第k个basis 第j个像素 第i个图片&lt;/li>
&lt;li>$Z_{ijk} = \frac{\kappa(f_{ij},\mu_k)}{\sum_{l=1}^{K}\kappa(f_{ij},\mu_l)}$&lt;/li>
&lt;li>$f_{ij}$第i个图像的第j个位置的特征&lt;/li>
&lt;li>$\kappa$是指数核函数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>M-step更新基集$\mu$，并把$\mu$作为$F$的加权平均&lt;/p>
&lt;ul>
&lt;li>$\mu_k = \frac{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}f_{ij}}{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>经过E-M的迭代后，我们用$\mu$和$Z$去重建$X$并得到$F$&lt;/p>
&lt;ul>
&lt;li>$F_i=Z_i\mu$&lt;/li>
&lt;li>$\tilde X_i=X_i+Conv(F_i)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="前景知识">前景知识&lt;/h4>
&lt;p>Expectation-Maximization (E-M)&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/67120173" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/67120173&lt;/a>.&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>初始化参数&lt;/li>
&lt;li>根据初始化的参数，划分类别&lt;/li>
&lt;li>根据最大似然估计重新计算参数&lt;/li>
&lt;li>重复步骤1-3，迭代n次，参数收敛&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>期望最大化注意力机制&lt;/strong>&lt;/p>
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener"
>EMANet&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.jianshu.com/p/6bb799d256b1" target="_blank" rel="noopener"
>https://www.jianshu.com/p/6bb799d256b1&lt;/a>&lt;/li>
&lt;li>作者写的知乎专栏：https://zhuanlan.zhihu.com/p/78018142&lt;/li>
&lt;/ul>
&lt;p>分为$A_E,A_M,A_R$三部分组成，前两者是EM算法的E步和M步&lt;/p>
&lt;ul>
&lt;li>假定输入的特征图为$X\in R^{N\times C}$，基初始值为$\mu\in R^{K\times C}$&lt;/li>
&lt;li>$A_E$步估计隐变量$Z\in R^{N\times K}$，则第k个基对第n个像素的权责可以计算为
&lt;ul>
&lt;li>$z_{nk}=\frac{\kappa(x_n,\mu_k)}{\sum_{j=1}^{K}\kappa(x_n,\mu_j)}$&lt;/li>
&lt;li>实现时可以用公式 $Z=softmax(\lambda X(\mu^T))$，其中$\lambda$作为超参数控制$Z$的分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_M$步更新基。$\mu$被计算为$X$的加权平均。第k个基被个更新为
&lt;ul>
&lt;li>$\mu_k=\frac{\sum_{n=1}^Nz_{nk}X_n}{\sum_{n=1}^Nz_{nk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_E$和$A_M$交替执行T步后，$\mu$和$Z$近似收敛，可以用来对X重新评估
&lt;ul>
&lt;li>$\tilde X=Z\mu$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="decoder">Decoder&lt;/h3>
&lt;p>$$
P^m_i=Conv(Unsample(Conv(X^m_i)+P^{m+1}_i)),\ where\ m \ in \ [1,4]
$$&lt;/p>
&lt;ul>
&lt;li>其中m是第m层，i表示&lt;/li>
&lt;/ul>
&lt;p>把检测结果在与原图相同的特征维度还原出来&lt;/p>
&lt;p>用交叉熵Cross-entropy来作为损失函数&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;ul>
&lt;li>k-fold evaluation protocol 将数据集分成三部分，其中之二作为训练集，剩下作为测试集&lt;/li>
&lt;/ul>
&lt;h3 id="benchmark-setting">Benchmark Setting&lt;/h3>
&lt;ul>
&lt;li>IoU metric
&lt;ul>
&lt;li>for segmentation task 切割任务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mean Absolute Error (MAE)
&lt;ul>
&lt;li>measure the absolute error between the prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>E-measure(?)&lt;/strong> 相关文章 &lt;a class="link" href="https://github.com/DengPingFan/E-measure" target="_blank" rel="noopener"
>E-measure: Enhanced-alignment Measure for Binary Foreground Map Evaluation&lt;/a>
&lt;ul>
&lt;li>a metric that combines local pixels and image-level average values to jointly capture image-level statistics and local pixel matching information.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Pearson Correlation Coefficient (CC)&lt;/strong>
&lt;ul>
&lt;li>皮尔逊相关系数 两个变量之间的协方差和标准差的商 $$ p_{X,Y}=\frac{cov(X,Y)}{\sigma_x\sigma_y} $$&lt;/li>
&lt;li>measure the correlation between prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>其他训练参数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Adam optimizer&lt;/li>
&lt;li>resnet50&lt;/li>
&lt;li>The input is randomly clipped from 360×360 to 320×320 with random horizontal flipping. 随机裁剪+水平翻转&lt;/li>
&lt;li>40 epochs on 1080ti&lt;/li>
&lt;li>learning rate 1e-4&lt;/li>
&lt;li>the number of bases in CEM is $K=256$&lt;/li>
&lt;li>E-M 迭代次数 3&lt;/li>
&lt;/ul>
&lt;h3 id="quantitative-and-qualitative-comparisons">Quantitative and Qualitative Comparisons&lt;/h3>
&lt;p>对比下来就是我们的模型很好很好&lt;/p></description></item><item><title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</guid><description>&lt;p>&lt;img src="https://SuperCarryDFY.github.io/img/title1.png"
loading="lazy"
alt="title1.png"
>&lt;/p>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>主要解决了两个问题：&lt;/p>
&lt;ul>
&lt;li>Generalization Reduction &amp;amp; High-Level Features.
&lt;ul>
&lt;li>[CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱）&lt;/li>
&lt;li>他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Spatial Inconsistency.
&lt;ul>
&lt;li>因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">RELATED WORK&lt;/h2>
&lt;p>&lt;strong>Few-Shot Learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>meta-learning
&lt;ul>
&lt;li>跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>metric-learning
&lt;ul>
&lt;li>Prototypical network&lt;/li>
&lt;li>这篇文章比较偏向于metric-learning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png"
loading="lazy"
alt="framework1.png"
>&lt;/p>
&lt;h3 id="prior-for-few-shot-segmentation">Prior for Few-Shot Segmentation&lt;/h3>
&lt;p>CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。&lt;/p>
&lt;p>Prior Generation的具体做法&lt;/p>
&lt;ul>
&lt;li>
&lt;p>先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask
$$
X_Q=F(I_Q), \ X_S = F(I_S)\times M_S
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity
$$
cos(x_q,x_s)=\frac{x_q^Tx_s}{|x_q||x_s|},\ \ \ \ q,s\in{1,2,&amp;hellip;,hw}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每一个$x_q \in X_Q$来说，取其中最大的值作为correspondence value
$$
c_q = max_{s\in {1,2&amp;hellip;,hw}}(cos(x_q,x_s))
$$&lt;/p>
&lt;p>$$
C_Q = [c_1,c_2,&amp;hellip;,c_hw] \in R^{hw\times1}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization
$$
Y_Q = \frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\epsilon}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="feature-enrichment-module">Feature Enrichment Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/rhadcOZ1iPuQ7nH.png"
loading="lazy"
alt="module1.png"
>&lt;/p>
&lt;p>将support image和query image关联起来的方法&lt;/p>
&lt;ul>
&lt;li>对support image做global average pooling
&lt;ul>
&lt;li>不用说都感觉效果一般&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>multi-level spatial information
&lt;ul>
&lt;li>说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>作者提出的FEM可以很好的解决问题。其中M的具体操作如下&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/P9ux8joOMAlyv3t.png"
loading="lazy"
alt="module2.png"
>&lt;/p>
&lt;h3 id="loss-function">Loss Function&lt;/h3>
&lt;p>$$
L = \frac{\sigma}{n}\sum_{i=1}^{n}{L_1^i+L_2}
$$&lt;/p>
&lt;p>主要选用交叉熵作为损失函数。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成（？）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$L_2$ 最后prediction和label的交叉熵。&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title/><link>https://SuperCarryDFY.github.io/p/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/</guid><description>&lt;h2 id="argparse">&lt;strong>argparse&lt;/strong>&lt;/h2>
&lt;p>&lt;a class="link" href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener"
>Tutorial&lt;/a>&lt;/p>
&lt;p>&lt;code>argparse&lt;/code>module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and &lt;a class="link" href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"
>&lt;code>argparse&lt;/code>&lt;/a> will figure out how to parse those out of &lt;a class="link" href="https://docs.python.org/3/library/sys.html#sys.argv" target="_blank" rel="noopener"
>&lt;code>sys.argv&lt;/code>&lt;/a>. The &lt;a class="link" href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"
>&lt;code>argparse&lt;/code>&lt;/a> module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.&lt;/p>
&lt;p>把他看作一个字典好了&lt;/p>
&lt;h2 id="property--abstractmethod">&lt;strong>@property &amp;amp; @abstractmethod&lt;/strong>:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>@property: 通过这个可以把一个函数当作属性来使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>@abstractmethod: 来自ABCMeta库，描述如下&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>This module provides the infrastructure for defining &lt;a class="link" href="https://docs.python.org/3/glossary.html#term-abstract-base-class" target="_blank" rel="noopener"
>abstract base classes&lt;/a> (ABCs) in Python&lt;/p>
&lt;/blockquote>
&lt;p>好像就是说被这个装饰的函数不能实例化，但是其子类如果实现了该抽象方法的话就可以被实例化&lt;/p>
&lt;h2 id="nptranspose">np.transpose()&lt;/h2>
&lt;p>类似torch.permute&lt;/p>
&lt;h2 id="torchstack">torch.stack()&lt;/h2>
&lt;p>torch.stack(inputs, dim=?) → Tensor&lt;/p>
&lt;p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 例子&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 假设是时间步T1的输出&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">T1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">9&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 假设是时间步T2的输出&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">T2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">20&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">40&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">50&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">60&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">70&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">80&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">90&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># outputs:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="ne">IndexError&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dimension&lt;/span> &lt;span class="n">out&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="nb">range&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expected&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">but&lt;/span> &lt;span class="n">got&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 报错&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="torchautogradvariable">torch.autograd.Variable&lt;/h2>
&lt;p>tensor是硬币的话，那Variable就是钱包，它记录着里面的钱的多少，和钱的流向&lt;/p>
&lt;p>Tensor是存在Variable中的.data里的，而cpu和gpu的数据是通过 .cpu()和.cuda()来转换的&lt;/p>
&lt;h2 id="torchnnfunctionalinterpolate">torch.nn.functional.interpolate&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">interpolate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scale_factor&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;nearest&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">align_corners&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>根据给定的size或者scale_factor参数来对输入进行上/下采样，参数：&lt;/p>
&lt;ul>
&lt;li>input&amp;ndash;tensor &amp;ndash; 输入张量&lt;/li>
&lt;li>size &amp;ndash;int or Tuple &amp;ndash; 输出大小&lt;/li>
&lt;li>scale_factor &amp;ndash;float or Tuple&amp;ndash;指定输出为输入的多少倍数。如果输入为tuple，其也要制定为tuple类型&lt;/li>
&lt;li>mode &amp;ndash;str&amp;ndash; 可使用的上采样算法，有&lt;code>'nearest'&lt;/code>, &lt;code>'linear'&lt;/code>, &lt;code>'bilinear'&lt;/code>, &lt;code>'bicubic'&lt;/code> , &lt;code>'trilinear'和'area'&lt;/code>. &lt;code>默认使用``'nearest'&lt;/code>&lt;/li>
&lt;li>align_corners &amp;ndash;bool&amp;ndash; 几何上，我们认为输入和输出的像素是正方形，而不是点。如果设置为True，则输入和输出张量由其角像素的中心点对齐，从而保留角像素处的值。如果设置为False，则输入和输出张量由它们的角像素的角点对齐，插值使用边界外值的边值填充;&lt;code>当scale_factor保持不变时&lt;/code>，使该操作独立于输入大小。仅当使用的算法为&lt;code>'linear'&lt;/code>, &lt;code>'bilinear', 'bilinear'&lt;/code>or &lt;code>'trilinear'时可以使用。&lt;/code>默认设置为``False`&lt;/li>
&lt;/ul>
&lt;h2 id="torchsave">torch.save&lt;/h2>
&lt;blockquote>
&lt;p>torch.save(&lt;em>obj&lt;/em>, &lt;em>f&lt;/em>, &lt;em>pickle_module=pickle&lt;/em>, &lt;em>pickle_protocol=DEFAULT_PROTOCOL&lt;/em>, &lt;em>_use_new_zipfile_serialization=True&lt;/em>)&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://pytorch.org/docs/stable/notes/serialization.html#saving-loading-tensors" target="_blank" rel="noopener"
>Saves an object to a disk file.&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;tensor.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;tensor.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">BatchNorm1d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">track_running_stats&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">named_parameters&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="s1">&amp;#39;weight&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Parameter&lt;/span> &lt;span class="n">containing&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bias&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Parameter&lt;/span> &lt;span class="n">containing&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">))]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">named_buffers&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="s1">&amp;#39;running_mean&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_var&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;num_batches_tracked&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state_dict&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">OrderedDict&lt;/span>&lt;span class="p">([(&lt;/span>&lt;span class="s1">&amp;#39;weight&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bias&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_mean&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_var&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;num_batches_tracked&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>In the tutorials, it is recommended to instead save only its state dict. Python modules have a function,&lt;code>load_state_dict()&lt;/code>, to restore their states from a state dict.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state_dict&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="s1">&amp;#39;bn.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn_state_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bn.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">new_bn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">BatchNorm1d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">track_running_stats&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">new_bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_state_dict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn_state_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">All&lt;/span> &lt;span class="n">keys&lt;/span> &lt;span class="n">matched&lt;/span> &lt;span class="n">successfully&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="bn">BN&lt;/h2>
&lt;p>Batch Normalization&lt;/p>
&lt;p>&lt;strong>Internal Covariate Shift&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。&lt;/li>
&lt;/ul>
&lt;p>Batch Normalization具体做法&lt;/p>
&lt;ul>
&lt;li>对当前层的第j个维度做规范化（有m个样本），使得每一层输入的每个特征的分布均值为0，方差为1&lt;/li>
&lt;li>考虑到规范化后容易使得底层网络学习到的信息丢失，因此引入两个可学习的参数$\gamma$和$\beta$，对规范后的数据进行线性变换。&lt;/li>
&lt;/ul>
&lt;h2 id="tensorcontiguous">tensor.contiguous&lt;/h2>
&lt;p>一些tensor操作（traspose，permute）和原tensor是共享内存的，不会改变底层数组的存储，但是如果要使用view方法的话，就要求对应tensor的数据占用内存是连续的。&lt;/p>
&lt;blockquote>
&lt;p>Tensor.contiguous(&lt;em>memory_format=torch.contiguous_format&lt;/em>) -&amp;gt; Tensor&lt;/p>
&lt;/blockquote>
&lt;p>Returns a contiguous in memory tensor containing the same data as &lt;code>self&lt;/code> tensor. If &lt;code>self&lt;/code> tensor is already in the specified memory format, this function returns the &lt;code>self&lt;/code> tensor.&lt;/p>
&lt;p>如果想要改变形状并且直接改内存的话，就用reshape&lt;/p>
&lt;h2 id="validation-data-sets">validation data sets&lt;/h2>
&lt;ul>
&lt;li>为了调整超参数&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。&lt;/p>
&lt;p>吴恩达老师的视频中，如果当数据量不是很大的时候（万级别以下）的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1；但是当可用的数据很少的情况下也可以使用一些高级的方法，比如留出方，K折交叉验证等。&lt;/p>
&lt;/blockquote>
&lt;p>引入k-fold交叉验证的模板&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">load_data&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hyper_parameters&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">set_hyper&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init_k_fold&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">hyper&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">hyper_parameters&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">metrics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fold_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fold_valid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cv_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 用此时的超参数训练模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fold_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hyper&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fold_valid&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">metrics&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_metric&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">avg_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">metrics&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">std_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">std&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">metrics&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># compare metrics among different hypers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_hyper&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update_best&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># finally&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_hyper&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">metrics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># show your final metrics&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="tensorboard">tensorboard&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">torch.utils.tensorboard&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SummaryWriter&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">writer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">SummaryWriter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;logs/xxx&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 打印模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># model是你想打印的模型，inputs是forword时输入的参数，如果有多个就以元组形式输出，其中元素必须是tensor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">writer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_graph&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 打印loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">n_iter&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">train&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">writer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_scalar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Loss/train&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_iter&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="gpu加速">GPU加速&lt;/h2>
&lt;p>&lt;strong>torch.nn.DataParallel&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>CLASS&lt;/em>&lt;code>torch.nn.DataParallel&lt;/code>(&lt;em>module&lt;/em>, &lt;em>device_ids=None&lt;/em>, &lt;em>output_device=None&lt;/em>, &lt;em>dim=0&lt;/em>)&lt;/p>
&lt;p>首先在前向过程中，你的输入数据会被划分成多个子部分（以下称为副本）送到不同的device中进行计算，而你的模型module是在每个device上进行复制一份，也就是说，输入的batch是会被平均分到每个device中去，但是你的模型module是要拷贝到每个devide中去的，每个模型module只需要处理每个副本即可，当然你要保证你的batch size大于你的gpu个数。然后在反向传播过程中，每个副本的梯度被累加到原始模块中。概括来说就是：DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">device_ids&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataParallel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际上optimizer也可以使用dataparallel优化，此时需要注意到下面第二行返回的是一个module，因此optimizer需要当module使用&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataParallel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">optimizer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 优化器原本使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 修改之后优化器使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际上pytorch不建议用DataParallel。。。&lt;/p>
&lt;p>&lt;strong>torch.nn.parallel.DistributedDataParallel&lt;/strong>&lt;/p>
&lt;p>相比DataParallel&lt;/p>
&lt;ul>
&lt;li>&lt;code>DataParallel&lt;/code>是单进程多线程的，只用于单机情况，而&lt;code>DistributedDataParallel&lt;/code>是多进程的，适用于单机和多机情况，真正实现分布式训练；&lt;/li>
&lt;li>&lt;code>DistributedDataParallel&lt;/code>的训练更高效，因为每个进程都是独立的Python解释器，避免GIL问题，而且通信成本低其训练速度更快，基本上&lt;code>DataParallel&lt;/code>已经被弃用；&lt;/li>
&lt;li>&lt;code>DistributedDataParallel&lt;/code>中每个进程都有独立的优化器，执行自己的更新过程，但是梯度通过通信传递到每个进程，所有执行的内容是相同的；&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener"
>https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/113694038" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/113694038&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel" target="_blank" rel="noopener"
>https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel&lt;/a>&lt;/p>
&lt;h2 id="benchmark">benchmark&lt;/h2>
&lt;p>benchmark in osad&lt;/p>
&lt;ul>
&lt;li>IoU: 交并比。
&lt;ul>
&lt;li>$IoU = \frac{traget \cap prediction}{target \cup prediction}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CC: Pearson product-moment correlation coefficient,皮尔逊相关系数。
&lt;ul>
&lt;li>$\rho_{X,Y}=\frac{cov(X，Y)}{\sigma_X\sigma_Y}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MAE：Mean Absolute Error，平均绝对误差&lt;/li>
&lt;/ul>
&lt;h2 id="torchcudasynchronize">torch.cuda.synchronize()&lt;/h2>
&lt;p>因为torch里面执行都是异步的，这行代码的意思是等待所有进程运行完。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 例子&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">synchronize&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">start&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">synchronize&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="torchbmm">torch.bmm&lt;/h2>
&lt;blockquote>
&lt;p>torch.bmm(&lt;em>input&lt;/em>, &lt;em>mat2&lt;/em>, *, &lt;em>out=None&lt;/em>) → Tensor&lt;/p>
&lt;/blockquote>
&lt;p>If &lt;code>input&lt;/code> is a $(b \times n \times m)$ tensor, &lt;code>mat2&lt;/code> is a $(b \times m \times p)$ tensor, &lt;code>out&lt;/code> will be a$(b \times n \times p)$ tensor.&lt;/p>
&lt;p>input and mat2 must be 3-D tensors each containing the same number of matrics.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">input&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">mat2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bmm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mat2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item></channel></rss>