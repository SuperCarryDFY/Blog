<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Dai Fengyuan</title><link>https://SuperCarryDFY.github.io/post/</link><description>Recent content in Posts on Dai Fengyuan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 28 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning What Not to Segment A New Perspective on Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png" alt="Featured image of post Learning What Not to Segment A New Perspective on Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>I read the paper in the first month when I came in MiLab. However, at that time I could not recognize the quality of this method (I usually don&amp;rsquo;t read the experiment carefully). It appeared when I was reading another paper called &amp;ldquo;Holistic Prototype Activation for Few-Shot Segmentation&amp;rdquo;. The HPA performs well but can not beat BAM (although HPA have less parameter to train, obviously). So I decided to read this paper again.&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>Previous problem:&lt;/p>
&lt;ul>
&lt;li>the trained models are biased towards the seen classes instead of being ideally class-agnostic&lt;/li>
&lt;/ul>
&lt;p>Contribution&lt;/p>
&lt;ul>
&lt;li>
&lt;p>BAM, i.e., base and the meta has two branches, allowing model to learn what not to segment and what to segmentation, respectively.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Design a special loss in order to train two branches suitably.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extend the proposed approach to a more challenging setting, which simultaneously identifies the targets of base and novel classes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In the following image, (a) is a classical method to address FSS task; (b) is BAM approach; (c) is the extension of BAM&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/hOT72tJIqimkDKL.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>They adopt two stage training method, which means they train base learner and meta learner separately.&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/4KBGJQmrc71nMu2.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="base-learner">BASE LEARNER&lt;/h3>
&lt;p>Query image goes through four ResNet blocks and becomes intermediate feature maps $f_b^q$. Then the decoder network $D_b$ yields the prediction result. $N_b$ represents the number of base categories.
$$
P_b = softmax(D_b(f_b^q)) \in R^{(1+N_b)\times H\times W}
$$
Loss can be defined as
$$
L_{base} = \frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}CE(P_{b;i},m^q_{b;i})
$$
It is worth noting that they do not employ the general FSS learning paradigm (update the parameter in each episode). And they train the base learner independently. They explain as follow:&lt;/p>
&lt;blockquote>
&lt;p>It is unrealistic to additionally build such a large network on the basis of the original few-shot model, which will introduce too many parameters and slow down the inference speed.&lt;/p>
&lt;p>It is unknown whether the base learner can be trained well with the episodic learning paradigm, so a two stage training strategy is eventually adopted.&lt;/p>
&lt;/blockquote>
&lt;p>In the ablation study, it shows that with two stage train, the model can perform better.&lt;/p>
&lt;h3 id="meta-learner">META LEARNER&lt;/h3>
&lt;p>This part is highly resemble similar to CANet, employing &amp;ldquo;expand &amp;amp; concatenate&amp;rdquo; operations. The loss can be described as
$$
L_{meta} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_{m;i},m_i^q)
$$
, when $n_e$ denotes the number of training episodes in each batch&lt;/p>
&lt;h3 id="ensemble">Ensemble&lt;/h3>
&lt;p>This part is designed to leverage the low-level feature to adjust the coarse predictions which is derived from meta learner.&lt;/p>
&lt;p>Firstly, we calculate the overall indicator $\psi$ for guiding the adjustment process:
$$
A_s = F_{reshape}(f_{low}^s) \in R^{C_1\times N},\
G^s = A_sA^T \in R^{C_1\times C_1}
$$
$G^s$ should be denoted as Gram matrix, BTW.&lt;/p>
&lt;blockquote>
&lt;p>Gram matrix can be regarded as eccentric covariance matrix between features. Every number in gram matrix describe the relation between every two feature, about which two features appear simultaneously, which two features just offset from each other, etc.&lt;/p>
&lt;/blockquote>
&lt;p>$$
\psi =||G^s - G^q||_F
$$&lt;/p>
&lt;p>$||\ ||_F$ denotes the Frobenius norm of the input metirx.&lt;/p>
&lt;p>After that, the final segmentation pridections $P_f$ can be described as follow:
$$
p_f^0 = F_{ensemble}(F_\psi(p_m^0),p_b^f), \
p_f=p_f^0(+)F_\psi(p_m^1)
$$
where $p_m$,$p_b$ denote the predictions of the meta learner and base learner respectively. The superscript &amp;ldquo;0&amp;rdquo; and &amp;ldquo;1&amp;rdquo; represent the background and foreground respectively. Both $F_ψ$ and $F_{ensemble}$ are 1×1 convolution operations with specific initial parameters.&lt;/p>
&lt;h3 id="loss">LOSS&lt;/h3>
&lt;p>$$
L=L_{final} + \lambda L_{meta} \
L_{final} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_i^q,m_i^q)
$$&lt;/p>
&lt;p>where $L_{meta}$ is the loss function of the meta learner defined in the meta learner stage.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/1E4h7VSepL2wHUG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/JuMlP1aGcUwxI94.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>Affordance Transfer Learning for Human-Object Interaction Detection</title><link>https://SuperCarryDFY.github.io/p/affordance-transfer-learning-for-human-object-interaction-detection/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/affordance-transfer-learning-for-human-object-interaction-detection/</guid><description>&lt;img src="https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png" alt="Featured image of post Affordance Transfer Learning for Human-Object Interaction Detection" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/avbpq9mtxy5IdCF.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>这篇实质是完成一个分类任务，并且能在unseen objects上也能辨认出其affordance，应该比较容易拿过来做few-shot任务。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/UltqHo4P2MOs9Lf.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>train用这张图，test用下面object affordance recognition那张图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/9m3qWEVyUQIHfgB.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="affordance-transfer-learning">AFFORDANCE TRANSFER LEARNING&lt;/h3>
&lt;p>&lt;strong>Efficient HOI Composition&lt;/strong>&lt;/p>
&lt;p>To compose a new HOI by the object $\hat{l_o}$ and verb $l_v$, we assign the label to the composite HOI as follows,
$$
\hat{y} = (\hat{l_o}A_o) \and (l_vA_v)
$$
，其中$A_o$和$A_v$是分别关于object和verb的同现矩阵co-occurrence matrix（？）&lt;/p>
&lt;p>&lt;strong>Invalid HOI Elimination&lt;/strong>&lt;/p>
&lt;p>他说有些HOI是无效的（比如ride orange），所以（手动？）把无效HOI在上式矩阵的对应位置清零了。这段好怪&lt;/p>
&lt;h3 id="object-affordance-recognition">OBJECT AFFORDANCE RECOGNITION&lt;/h3>
&lt;p>这里主要解释如何在test phase做推理&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/CaRPrfTUmpjAqtX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对每个affordance，我们随机抽取M（这里M=100）个instances，抽取完特征之后作为affordance feature bank&lt;/p>
&lt;p>对一个输入的object feature，我们把它和bank所有的affordance一一结合起来，把所有的HOI predictions都转换成affordance prediction（这有啥区别），然后就得到了有许多重复元素的affordance lists。一个元素重复得越多说明有这个affordance的可能习惯越大。&lt;/p>
&lt;h3 id="optimization-and-inference">OPTIMIZATION AND INFERENCE&lt;/h3>
&lt;p>loss就比较常规
$$
L = L_{hoi_sp}+\lambda_1L_{hoi}+\lambda_2L_{ATL}
$$
，其中$\lambda_1$,$\lambda_2$都是超参数。&lt;/p></description></item><item><title>Learning Transferable Human-Object Interaction Detector with Natural Language Supervision</title><link>https://SuperCarryDFY.github.io/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/learning-transferable-human-object-interaction-detector-with-natural-language-supervision/</guid><description>&lt;img src="https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png" alt="Featured image of post Learning Transferable Human-Object Interaction Detector with Natural Language Supervision" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/gWoakEicOMpR68j.png"
loading="lazy"
alt="image-20220822150720449"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>这篇文章的工作是，输入一张图片，输出HOI特征，并且用HOI短语作为监督训练（基于CLIP）。&lt;/p>
&lt;p>他与其他HOI transfer工作不同的点在于，之前的工作对unseen object都是用discrete label作为输出，得到HOI。但是这就要求label中对应的HOI（就是每个动作）都预训练过，很难去识别interaction that out of the predefined list。这篇的思路主要是对文字和图像同时encode，然后寻找最近的匹配对，所以不存在这个问题。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/IGi64rQnLkNBma3.png"
loading="lazy"
alt="image-20220822152554566"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/d7nPavWht2iRBue.png"
loading="lazy"
alt="image-20220822152833390"
>&lt;/p>
&lt;p>他们定义了HOI为 ${(b_p,b_o,a,o)}$（有些文章定义为triplet，其实都差不多，最重要的是verb），其中$b_p$和$b_o$是人和物的bounding box。$a$和$o$分别是human action和object category。&lt;/p>
&lt;h3 id="preliminary">PRELIMINARY&lt;/h3>
&lt;p>在preliminary中，他们简单设想了一些步骤，用Faster RCNN得到bounding box然后输入到CLIP，就能在Unseen数据集上得到SOTA，even without tuning.（废话，人家设计的时候也没管unseen HOI啊）&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/22/36FEm9MqlODH4wT.png"
loading="lazy"
alt="image-20220822155533112"
>&lt;/p>
&lt;h3 id="proposed-method">PROPOSED METHOD&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>visual input: ${(h,c,b_p,b_o)}$,&lt;/p>
&lt;ul>
&lt;li>h is feature representation for interactions&lt;/li>
&lt;li>$b_p,b_o$ is bounding box&lt;/li>
&lt;li>c is the confidence score for bounding box prediction&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>text encoder: raw text of interactions&lt;/p>
&lt;/li>
&lt;li>
&lt;p>similarity $h^Ts$, where h, s denote the output of visual encoder and text encoder. They are semantic features in the same dimension.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ViT-based Visual Encoder&lt;/strong>&lt;/p>
&lt;p>前半部分输入跟ViT一样，对图像分patch之后加position embedding，在首位置插入CLS后直接作为输入放到ViT中。其中，关于CLS的理解如下&lt;/p>
&lt;blockquote>
&lt;p>CLS的特点&lt;/p>
&lt;ul>
&lt;li>不基于图像内容&lt;/li>
&lt;li>位置编码固定&lt;/li>
&lt;/ul>
&lt;p>好处&lt;/p>
&lt;ul>
&lt;li>该token随机初始化，能够编码整个序列的统计特性&lt;/li>
&lt;li>本身不基于图像内容，避免对某个特定的token产生偏向性&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>但是和ViT不同的是，这里期望能识别出多种不同的HOI（ViT最开始是做分类任务的）。所以在原本的序列后面另外加入了M个CLS（留了M个空让网络自己去学）。&lt;/p>
&lt;p>然而，输入的patch（以下称为X）和作为HOI的CLS（以下称为H）在ViT中作计算时亦有不同。&lt;/p>
&lt;p>X就是正常经过Transformer block，不管H。其中MHA是多层注意力机制，LN是layer norm，MLP是2层感知机。
$$
X_l^{&amp;rsquo;} = MHA(X_{l-1})+X_{l-1} \
X_l = MLP(LN(X_l^{&amp;rsquo;}))+X_l^{&amp;rsquo;}
$$
H在做多层注意力机制时，需要聚合来自X的信息（但是不需要位置0的CLS，他们说如果不mask的话，HOI会直接copy位置0的信息）
$$
H_l^{&amp;rsquo;}=MHA(H_{l-1},X_{l-1}^{[1:]}) + H_{l-1} \
H_l = MLP(LN(H_l^{&amp;rsquo;}))+H_l^{&amp;rsquo;}
$$
&lt;strong>HOI Sequence Parser&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/X8iwQBC4c9SYFyI.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从transformer block出来的HOI序列有一个问题，没办法区分开来。这其实也好理解，因为输入的时候并没有带位置信息，实际上H之间都是等效的。&lt;/p>
&lt;p>所以作者故意在这个模块中使用Sequence manner 而不是 in parallel的方式。&lt;/p>
&lt;p>&lt;strong>Project Head and Bounding Box Regressor&lt;/strong>&lt;/p>
&lt;p>Project Head其实就是一个线性层，把从transformer block出来的X映射到text encoder的输出的维度，方便做相似度计算，从而找到最近的tensor。&lt;/p>
&lt;p>因为我不懂detection，所以Regressor不太关心。&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>LOSS部分没有很复杂，由两部分组成，分别是box head输出和project head的loss
$$
L_m(i,\phi) = L_b(\hat{b_p^i},b_p^{\phi_i})+L_b(\hat{b_o^i},b_o^{\phi_i})+L_h(\hat{h_i},s_{\phi_i})
$$
其中$L_b$表示Bounding box 的loss，$L_h$代表CLIP那边的loss，包括text-to-visual和visual-to-text&lt;/p></description></item><item><title>Combination of Papers</title><link>https://SuperCarryDFY.github.io/p/combination-of-papers/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/combination-of-papers/</guid><description>&lt;p>Some papers are famous and have gotten several citations. They Always appear when I&amp;rsquo;m reading current papers. So I decided to write down the most critical parts (concerned mainly by myself, in another way) from these papers on this page.&lt;/p>
&lt;h1 id="learning-deep-features-for-discriminative-localization">Learning Deep Features for Discriminative Localization&lt;/h1>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/17/xeEUAgGitC461Q7.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>This is a well-known work from CVPR2016, which has got 6.5k citation numbers up-to-date.&lt;/p>
&lt;p>There are two parts seems to be important to me: &lt;strong>comparison between global max pooling and global average pooling&lt;/strong>, as well as &lt;strong>framework&lt;/strong>.&lt;/p>
&lt;h2 id="gmp-vs-gap">GMP vs. GAP&lt;/h2>
&lt;blockquote>
&lt;p>We believe that GAP loss encourages the network to identify the extent of the object as compared to GMP which encourages it to identify just one discriminative part.&lt;/p>
&lt;p>while GMP achieves similar classification performance as GAP, GAP outperforms GMP for localization.&lt;/p>
&lt;/blockquote>
&lt;p>GAP can focus on a wide range of pixels while GMP only depends on the most significant feature.&lt;/p>
&lt;h2 id="framework">FRAMEWORK&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/17/g3qjOyweVAtlNR1.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>The output of the last convolutional layer is denoted as $f_k(x,y)$, while k means the channel. After GAP, which should be expressed as $F^k = \sum_{x,y}f_k(x,y)$, for a given class, the input to the softmax $S_c$, is $\Sigma_kw_k^cF_k$ where $w_k^c$ is weight corresponding to class c for unit k.
$$
S_c = \sum_kw_k^c\sum_{x,y} f_k(x,y) = \sum_{x,y}\sum_k w_k^cf_k(x,y)
$$
They did upsampling in the middle of the framework to fit the size.&lt;/p>
&lt;p>ABOUT &lt;strong>weakly-supervised&lt;/strong>: They meant weakly-supervised because the labels is image-level but localization is object-level&lt;/p>
&lt;h1 id="learning-to-compare-relation-network-for-few-shot-learning">Learning to Compare: Relation Network for Few-Shot Learning&lt;/h1>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/20/mdSbWiDs8HYKRoJ.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这篇文章的思路相当简单，而且比原型网络那篇更好理解，是最早的metric-base few-shot learning的一批，并且现在依然流行（其实现在的方法就是在这个基础上修改，大体的框架是一样的）。&lt;/p>
&lt;h2 id="framework-1">FRAMEWORK&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/20/2fjUiYmbQ58cSTp.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>framework基本上一眼就能看明白，$f_\phi$和$g_\phi$都是卷积，池化，batch norm操作，没什么特别的。concatenation似乎就是从这里来的，现在基本也都是把query feature和support feature concatenate起来。其过程可用如下公式表示
$$
r_{i,j} = g_\phi(C(f_\phi(x_i),f_\phi(x_j)))
$$
其中$f_{\phi}$，$g_{\phi}$表示embedding module和relation module，函数$C(·，·)$表示concatenation。&lt;/p>
&lt;h2 id="zero-shot-learning">ZERO-SHOT LEARNING&lt;/h2>
&lt;p>在zero-shot learning中，不再给support image，而是给semantic class embedding vector $v_c$，其过程可以用如下公式表示
$$
r_{i,j} = g_\phi(C(f_{\phi_1}(v_c),f_{\phi_2}(x_j)))
$$
&lt;img src="https://s2.loli.net/2022/08/20/zASjlHLM4GRkfJ1.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里的DNN是训练好的模型，如VGG、Inception等。&lt;/p>
&lt;p>和prototypical network的区别&lt;/p>
&lt;blockquote>
&lt;p>Relation Network比原型网络的方法多了可学习的层，用来判别相似度度量，而在原型网络中是直接拿特定的相似度度量公式（如cosine similarity）&lt;/p>
&lt;/blockquote></description></item><item><title>CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</title><link>https://SuperCarryDFY.github.io/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png" alt="Featured image of post CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/AiIQyMX2cZBlStK.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从TPAMI20那篇过来的，主要想看一下哪里说的middle-level feature&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>consists of&lt;/p>
&lt;ul>
&lt;li>a two-branch dense comparison module
&lt;ul>
&lt;li>performs multi-level feature comparison between the support image and the query image&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>an iterative optimization module
&lt;ul>
&lt;li>iteratively refines the predicted results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>fancy的地方&lt;/p>
&lt;ul>
&lt;li>先前的工作，从1-shot拓展到k-shot时，都是用non-learnable fusion，这篇文章中用的是attention mechanism。&lt;/li>
&lt;li>做test的时候，不再输入support image mask了，而是输入support image bounding box.&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/Ea6tJYghbZvXCiW.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="dense-comparison-module">DENSE COMPARISON MODULE&lt;/h3>
&lt;p>在CNN中&lt;/p>
&lt;ul>
&lt;li>feature in low layers often relate to low-level cues 比如颜色，边缘。个人感觉是因为感受野不够大&lt;/li>
&lt;li>feature in higher layers relate to object-level concepts 比如物品种类&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>因为他们要求模型训练完之后具备一定的generalization，而middle-level fature有可能会包含来自没见过的物体的part。比如说训练的时候见过小轿车，那么在测试中，我就比较容易根据middle-level中的轮胎来segment公交车。（make sense）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>dialated convolution&lt;/strong>空洞卷积&lt;/p>
&lt;p>以下来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>知乎&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。&lt;/p>
&lt;p>传统卷积核的本质问题：pool减小图片尺寸，upsampling增大图片尺寸，在这个过程中肯定有信息丢失掉了。因此空洞卷积就是不通过pooling也能获得较大感受野的方法&lt;/p>
&lt;p>dialated convolution的优点：内部数据结构的保留和避免使用 down-sampling 。&lt;/p>
&lt;/blockquote>
&lt;p>具体的，他们把Resnet分成4个block，只用block2和block3的输出，将其concat到一起。后面就是support feature与support feaure相乘来把背景像素清0，avg pool之后repeat到之前的维度和query feature concat到一起，比较常规。&lt;/p>
&lt;h3 id="inerative-optimization-module">INERATIVE OPTIMIZATION MODULE&lt;/h3>
&lt;p>每一次迭代中，首先进行如下运算。其中$M$是output of the residual blocks，$x$是DCM模块的输出，$y_{t-1}$是上一个迭代块的输出，$F$是concat之后再经过两层卷积层。
$$
M_t = x + F(x,y_{t-1})
$$
对$M_t$，再经过两层vanilla residual blocks。然后再经过(ASSP)[https://arxiv.org/abs/1706.05587]模块输出。&lt;/p>
&lt;p>就差不多这么迭代n次&lt;/p>
&lt;h3 id="attention-mechanism-for-k-shot-segmentation">ATTENTION MECHANISM FOR K-SHOT SEGMENTATION&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/ZEuUP7zm2i9IKks.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;strong>这也算attention嘛? query, key, value分别是什么？&lt;/strong>&lt;/p>
&lt;p>在attention模块中，经过两层卷积再softmax得到$\hat\lambda_k$，然后把$\hat\lambda_k$和经过卷积的support sample n相乘。&lt;/p>
&lt;h2 id="ablation-study">ABLATION STUDY&lt;/h2>
&lt;p>&lt;strong>Feature for Comparison&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/8ZJEQznfqDu9sTj.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里说单个的话block2效果是最好的，整体上block2+block3的效果最好。嗯，比较可信。&lt;/p>
&lt;blockquote>
&lt;p>其中提了一嘴While block4 corresponds to high-level features, e.g., categories, and incorporates a great number of parameters (2048 channels), which makes it hard to optimize under the fewshot setting.&lt;/p>
&lt;p>深有同感！我改topformer的时候就是纠结这个地方&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Attention vs. Feature Fusion vs. Mask Fusion&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/layPuvoMD6Xrgkc.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从这里可以看出Attention效果是最好的。但是相比来说的话还是多了两层卷积，增加了参数。Feature-Avg表现不错，感觉跟attention也差不多了（主要是没有另外加参数）。Mask-Avg这么奇怪的想法竟然也有效，能+0.5。还有Mask-OR你要笑死我嘛，怎么还没1-shot高啊，怎么回事啊小老弟，纯纯的帮倒忙。&lt;/p></description></item><item><title>Pytorch</title><link>https://SuperCarryDFY.github.io/p/pytorch/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/pytorch/</guid><description>&lt;h2 id="argparse">&lt;strong>argparse&lt;/strong>&lt;/h2>
&lt;p>&lt;a class="link" href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener"
>Tutorial&lt;/a>&lt;/p>
&lt;p>&lt;code>argparse&lt;/code>module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and &lt;a class="link" href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"
>&lt;code>argparse&lt;/code>&lt;/a> will figure out how to parse those out of &lt;a class="link" href="https://docs.python.org/3/library/sys.html#sys.argv" target="_blank" rel="noopener"
>&lt;code>sys.argv&lt;/code>&lt;/a>. The &lt;a class="link" href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"
>&lt;code>argparse&lt;/code>&lt;/a> module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.&lt;/p>
&lt;p>把他看作一个字典好了&lt;/p>
&lt;h2 id="property--abstractmethod">&lt;strong>@property &amp;amp; @abstractmethod&lt;/strong>:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>@property: 通过这个可以把一个函数当作属性来使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>@abstractmethod: 来自ABCMeta库，描述如下&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>This module provides the infrastructure for defining &lt;a class="link" href="https://docs.python.org/3/glossary.html#term-abstract-base-class" target="_blank" rel="noopener"
>abstract base classes&lt;/a> (ABCs) in Python&lt;/p>
&lt;/blockquote>
&lt;p>好像就是说被这个装饰的函数不能实例化，但是其子类如果实现了该抽象方法的话就可以被实例化&lt;/p>
&lt;h2 id="nptranspose">np.transpose()&lt;/h2>
&lt;p>类似torch.permute&lt;/p>
&lt;h2 id="torchstack">torch.stack()&lt;/h2>
&lt;p>torch.stack(inputs, dim=?) → Tensor&lt;/p>
&lt;p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 例子&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 假设是时间步T1的输出&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">T1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">9&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 假设是时间步T2的输出&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">T2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">20&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">40&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">50&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">60&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">70&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">80&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">90&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">T1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># outputs:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="ne">IndexError&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dimension&lt;/span> &lt;span class="n">out&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="nb">range&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expected&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">but&lt;/span> &lt;span class="n">got&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 报错&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="torchautogradvariable">torch.autograd.Variable&lt;/h2>
&lt;p>tensor是硬币的话，那Variable就是钱包，它记录着里面的钱的多少，和钱的流向&lt;/p>
&lt;p>Tensor是存在Variable中的.data里的，而cpu和gpu的数据是通过 .cpu()和.cuda()来转换的&lt;/p>
&lt;h2 id="torchnnfunctionalinterpolate">torch.nn.functional.interpolate&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">interpolate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scale_factor&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;nearest&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">align_corners&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>根据给定的size或者scale_factor参数来对输入进行上/下采样，参数：&lt;/p>
&lt;ul>
&lt;li>input&amp;ndash;tensor &amp;ndash; 输入张量&lt;/li>
&lt;li>size &amp;ndash;int or Tuple &amp;ndash; 输出大小&lt;/li>
&lt;li>scale_factor &amp;ndash;float or Tuple&amp;ndash;指定输出为输入的多少倍数。如果输入为tuple，其也要制定为tuple类型&lt;/li>
&lt;li>mode &amp;ndash;str&amp;ndash; 可使用的上采样算法，有&lt;code>'nearest'&lt;/code>, &lt;code>'linear'&lt;/code>, &lt;code>'bilinear'&lt;/code>, &lt;code>'bicubic'&lt;/code> , &lt;code>'trilinear'和'area'&lt;/code>. &lt;code>默认使用``'nearest'&lt;/code>&lt;/li>
&lt;li>align_corners &amp;ndash;bool&amp;ndash; 几何上，我们认为输入和输出的像素是正方形，而不是点。如果设置为True，则输入和输出张量由其角像素的中心点对齐，从而保留角像素处的值。如果设置为False，则输入和输出张量由它们的角像素的角点对齐，插值使用边界外值的边值填充;&lt;code>当scale_factor保持不变时&lt;/code>，使该操作独立于输入大小。仅当使用的算法为&lt;code>'linear'&lt;/code>, &lt;code>'bilinear', 'bilinear'&lt;/code>or &lt;code>'trilinear'时可以使用。&lt;/code>默认设置为``False`&lt;/li>
&lt;/ul>
&lt;h2 id="torchsave">torch.save&lt;/h2>
&lt;blockquote>
&lt;p>torch.save(&lt;em>obj&lt;/em>, &lt;em>f&lt;/em>, &lt;em>pickle_module=pickle&lt;/em>, &lt;em>pickle_protocol=DEFAULT_PROTOCOL&lt;/em>, &lt;em>_use_new_zipfile_serialization=True&lt;/em>)&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://pytorch.org/docs/stable/notes/serialization.html#saving-loading-tensors" target="_blank" rel="noopener"
>Saves an object to a disk file.&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;tensor.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;tensor.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">BatchNorm1d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">track_running_stats&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">named_parameters&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="s1">&amp;#39;weight&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Parameter&lt;/span> &lt;span class="n">containing&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bias&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Parameter&lt;/span> &lt;span class="n">containing&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">))]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">named_buffers&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="s1">&amp;#39;running_mean&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_var&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;num_batches_tracked&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state_dict&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">OrderedDict&lt;/span>&lt;span class="p">([(&lt;/span>&lt;span class="s1">&amp;#39;weight&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bias&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_mean&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;running_var&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="p">])),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;num_batches_tracked&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>In the tutorials, it is recommended to instead save only its state dict. Python modules have a function,&lt;code>load_state_dict()&lt;/code>, to restore their states from a state dict.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state_dict&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="s1">&amp;#39;bn.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bn_state_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;bn.pt&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">new_bn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">BatchNorm1d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">track_running_stats&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">new_bn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_state_dict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bn_state_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">All&lt;/span> &lt;span class="n">keys&lt;/span> &lt;span class="n">matched&lt;/span> &lt;span class="n">successfully&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="bn">BN&lt;/h2>
&lt;p>Batch Normalization&lt;/p>
&lt;p>&lt;strong>Internal Covariate Shift&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。&lt;/li>
&lt;/ul>
&lt;p>Batch Normalization具体做法&lt;/p>
&lt;ul>
&lt;li>对当前层的第j个维度做规范化（有m个样本），使得每一层输入的每个特征的分布均值为0，方差为1&lt;/li>
&lt;li>考虑到规范化后容易使得底层网络学习到的信息丢失，因此引入两个可学习的参数$\gamma$和$\beta$，对规范后的数据进行线性变换。&lt;/li>
&lt;/ul>
&lt;h2 id="tensorcontiguous">tensor.contiguous&lt;/h2>
&lt;p>一些tensor操作（traspose，permute）和原tensor是共享内存的，不会改变底层数组的存储，但是如果要使用view方法的话，就要求对应tensor的数据占用内存是连续的。&lt;/p>
&lt;blockquote>
&lt;p>Tensor.contiguous(&lt;em>memory_format=torch.contiguous_format&lt;/em>) -&amp;gt; Tensor&lt;/p>
&lt;/blockquote>
&lt;p>Returns a contiguous in memory tensor containing the same data as &lt;code>self&lt;/code> tensor. If &lt;code>self&lt;/code> tensor is already in the specified memory format, this function returns the &lt;code>self&lt;/code> tensor.&lt;/p>
&lt;p>如果想要改变形状并且直接改内存的话，就用reshape&lt;/p>
&lt;h2 id="validation-data-sets">validation data sets&lt;/h2>
&lt;ul>
&lt;li>为了调整超参数&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。&lt;/p>
&lt;p>吴恩达老师的视频中，如果当数据量不是很大的时候（万级别以下）的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1；但是当可用的数据很少的情况下也可以使用一些高级的方法，比如留出方，K折交叉验证等。&lt;/p>
&lt;/blockquote>
&lt;p>引入k-fold交叉验证的模板&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">load_data&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hyper_parameters&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">set_hyper&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init_k_fold&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">hyper&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">hyper_parameters&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">metrics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fold_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fold_valid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cv_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 用此时的超参数训练模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fold_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hyper&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fold_valid&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">metrics&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_metric&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">avg_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">metrics&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">std_metric&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">std&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">metrics&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># compare metrics among different hypers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_hyper&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update_best&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># finally&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_hyper&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">metrics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># show your final metrics&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="tensorboard">tensorboard&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">torch.utils.tensorboard&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SummaryWriter&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">writer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">SummaryWriter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;logs/xxx&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 打印模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># model是你想打印的模型，inputs是forword时输入的参数，如果有多个就以元组形式输出，其中元素必须是tensor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">writer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_graph&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 打印loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">n_iter&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">train&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">writer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_scalar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Loss/train&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_iter&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="gpu加速">GPU加速&lt;/h2>
&lt;p>&lt;strong>torch.nn.DataParallel&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>CLASS&lt;/em>&lt;code>torch.nn.DataParallel&lt;/code>(&lt;em>module&lt;/em>, &lt;em>device_ids=None&lt;/em>, &lt;em>output_device=None&lt;/em>, &lt;em>dim=0&lt;/em>)&lt;/p>
&lt;p>首先在前向过程中，你的输入数据会被划分成多个子部分（以下称为副本）送到不同的device中进行计算，而你的模型module是在每个device上进行复制一份，也就是说，输入的batch是会被平均分到每个device中去，但是你的模型module是要拷贝到每个devide中去的，每个模型module只需要处理每个副本即可，当然你要保证你的batch size大于你的gpu个数。然后在反向传播过程中，每个副本的梯度被累加到原始模块中。概括来说就是：DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">device_ids&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataParallel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际上optimizer也可以使用dataparallel优化，此时需要注意到下面第二行返回的是一个module，因此optimizer需要当module使用&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataParallel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">optimizer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 优化器原本使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 修改之后优化器使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际上pytorch不建议用DataParallel。。。&lt;/p>
&lt;p>&lt;strong>torch.nn.parallel.DistributedDataParallel&lt;/strong>&lt;/p>
&lt;p>相比DataParallel&lt;/p>
&lt;ul>
&lt;li>&lt;code>DataParallel&lt;/code>是单进程多线程的，只用于单机情况，而&lt;code>DistributedDataParallel&lt;/code>是多进程的，适用于单机和多机情况，真正实现分布式训练；&lt;/li>
&lt;li>&lt;code>DistributedDataParallel&lt;/code>的训练更高效，因为每个进程都是独立的Python解释器，避免GIL问题，而且通信成本低其训练速度更快，基本上&lt;code>DataParallel&lt;/code>已经被弃用；&lt;/li>
&lt;li>&lt;code>DistributedDataParallel&lt;/code>中每个进程都有独立的优化器，执行自己的更新过程，但是梯度通过通信传递到每个进程，所有执行的内容是相同的；&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener"
>https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/113694038" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/113694038&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel" target="_blank" rel="noopener"
>https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel&lt;/a>&lt;/p>
&lt;h2 id="benchmark">benchmark&lt;/h2>
&lt;p>benchmark in osad&lt;/p>
&lt;ul>
&lt;li>IoU: 交并比。
&lt;ul>
&lt;li>$IoU = \frac{traget \cap prediction}{target \cup prediction}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CC: Pearson product-moment correlation coefficient,皮尔逊相关系数。
&lt;ul>
&lt;li>$\rho_{X,Y}=\frac{cov(X，Y)}{\sigma_X\sigma_Y}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MAE：Mean Absolute Error，平均绝对误差&lt;/li>
&lt;/ul>
&lt;h2 id="torchcudasynchronize">torch.cuda.synchronize()&lt;/h2>
&lt;p>因为torch里面执行都是异步的，这行代码的意思是等待所有进程运行完。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 例子&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">synchronize&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">start&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">synchronize&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="torchbmm">torch.bmm&lt;/h2>
&lt;blockquote>
&lt;p>torch.bmm(&lt;em>input&lt;/em>, &lt;em>mat2&lt;/em>, *, &lt;em>out=None&lt;/em>) → Tensor&lt;/p>
&lt;/blockquote>
&lt;p>If &lt;code>input&lt;/code> is a $(b \times n \times m)$ tensor, &lt;code>mat2&lt;/code> is a $(b \times m \times p)$ tensor, &lt;code>out&lt;/code> will be a$(b \times n \times p)$ tensor.&lt;/p>
&lt;p>input and mat2 must be 3-D tensors each containing the same number of matrics.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">input&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">mat2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bmm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mat2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Holistic Prototype Activation for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png" alt="Featured image of post Holistic Prototype Activation for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/B18VowS6RfMEJKX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这篇真的好复杂&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）&lt;/p>
&lt;p>现在的FSS主要有俩问题&lt;/p>
&lt;ul>
&lt;li>会把不属于本类的物体也分割进来&lt;/li>
&lt;li>分不清边界&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/OJjBCcq7IUnFfey.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了&lt;/p>
&lt;blockquote>
&lt;p>The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)&lt;/p>
&lt;/blockquote>
&lt;p>对于第二个问题，他们说关键就是要找出support和query中的本质联系（套话），他们提出了基于DeepLabv3的CRD模型来解决这个问题。&lt;/p>
&lt;blockquote>
&lt;p>We argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images&lt;/p>
&lt;/blockquote>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="prototype-acquisition">PROTOTYPE ACQUISITION&lt;/h3>
&lt;p>在这个模块生成base class的prototype
$$
P_b^c = \frac{1}{N}\sum_{i=1}^{N_c}\frac{1}{|\hat{L_c^i}|}\sum_{j=1}^{HW}F_c^{ij}\hat{L_c^{ij}}
$$
其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。&lt;/p>
&lt;h3 id="prototype-activation-module">PROTOTYPE ACTIVATION MODULE&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/SeOERlPDLBuTasG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。&lt;/p>
&lt;p>对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\phi$是1&lt;em>1的卷积再加激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间
$$
A_k^{ij}=g_\phi(F_q^{ij}+P^k_h)
$$
然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别）
$$
k^&lt;/em> = argmax_k(A_k^{ij}) \
M^{ij}=A_{k^&lt;em>}^{ij} ,\ if \ k^&lt;/em>=1 \ otherwise \ 0
$$&lt;/p>
&lt;p>下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为我前面不是已经做了匹配度计算了么，所以我直接在对应位置做expand就好了，具体可见下图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/HumfepLVWd6byYa.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="cross-reference-decoder">CROSS-REFERENCE DECODER&lt;/h3>
&lt;p>这一步就比较乱了，左一步右一步的，还跟deeplabv3有关，我可能要先去把deeplabv3看了才能真正理解。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/uwkmMHxE4P8q15Y.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>loss部分没太看懂，$\alpha$好理解，也是多任务下比较常见的做法。就是$L_{act}$是咋算的没太看懂，不是直接跟label做cross-entropy吗&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/y95dGIcosCltvVN.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>One-Shot Affordance Detection</title><link>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png" alt="Featured image of post One-Shot Affordance Detection" />&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2106.14747.pdf" target="_blank" rel="noopener"
>One-shot Affordance Detetion 2106.14747.pdf (arxiv.org)&lt;/a>&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;ul>
&lt;li>可供性检测就是通过一张图片识别物体潜在的动作。&lt;/li>
&lt;li>OS-AD网络可以在所有候选图片中帮助发现普遍的可供性，并且学会适应感知未发现的可供性。&lt;/li>
&lt;li>他们建立了一个数据集PAD ：4k Image；31 affordance；72个物体类别&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>挑战OS-AD 给一张图片，告知其图片上的物体的行为，则可以察觉所有物体普遍的可供性&lt;/li>
&lt;li>问题：现实生活中一个物体可能有多个affordance（例如沙发可以躺也可以睡），而具体用什么affordance取决于人在这个场景中的目的。抛去目的的指引，直接从一张图片中学习affordance会导致忽略了其他视觉上的对此时的任务有效的affordance
&lt;ul>
&lt;li>从行为中找暗示&lt;/li>
&lt;li>采用&lt;strong>collaboration learning&lt;/strong>去捕捉不同物体间的潜在关系，抵消物体不同的appearance，增加泛化性；OS-AD PLM，PTM，CEM&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可供性检测应该能适用于各种环境： PAD 目标驱动可供性数据集&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;ul>
&lt;li>Affordance Detection&lt;/li>
&lt;li>One-Shot Learn
&lt;ul>
&lt;li>based on metric learning using the siamese neural network 度量学习；孪生神经网络&lt;/li>
&lt;li>meta-learning and generation models 元学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="methods">Methods&lt;/h2>
&lt;h3 id="framework">Framework&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png"
loading="lazy"
alt="framework2.png"
>&lt;/p>
&lt;ul>
&lt;li>input: query images, human-object interactions&lt;/li>
&lt;li>ResNet50 -&amp;gt; 获得图像表现 $X$ and $ X_{sup} $&lt;/li>
&lt;li>输入$X_{sup}$和 人和物体的边界矩阵到PLM -&amp;gt; 提取human-object interaction信息，对action-purpose编码，发现人想要旋转的原因&lt;/li>
&lt;li>输入feature representation和$X$到PTM里面 -&amp;gt; 让网络学会处理带affordance的信息&lt;/li>
&lt;li>输入encoded feature 到CEM， 输出affordance&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-learning-module">Purpose Learning Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/HUW6kjnPJ29qX83.png"
loading="lazy"
alt="plm.png"
>&lt;/p>
&lt;p>:star:&lt;a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_On_Exploring_Undetermined_Relationships_for_Visual_Relationship_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener"
>On Exploring Undetermined Relationships for Visual Relationship Detection&lt;/a>受到了这篇文章的启发，说instance（人或物）的特征可以指导网络哪里应该focus。&lt;/p>
&lt;p>先得到$M_O$和$M_H$ &lt;strong>（这两者分别代表什么？作者说是为了让模型去分别focus on物体和个人，引入了注意力机制，其中GMP的作用是得到最显著的特征）&lt;/strong> 其中⊗ 代表element-wise product，元素对应位置相乘，$f_O$和$f_H$是$X_O$和$X_H$进行 global maximum pooling（GMP）后的值
$$
M_O = Softmax(f_O⊗X_{sup})⊗X_{sup} \
M_H = Softmax(f_H⊗X_{sup})⊗X_{sup}
$$
作者说他们用$f_O$去指导网络应该focus on人物交互$M_{HO}$
$$
M_{HO}=Conv(f_O⊗X_H)
$$
最后得到encoding of the action purpose $F_{sup}$，其中&amp;quot; ·&amp;ldquo;代表position-wise dot product.
$$
F_{sup} = MaxPooling((M_{HO}·M_H)+(M_{HO}·M_O))
$$&lt;/p>
&lt;ul>
&lt;li>输入：$X_{sup}$以及人和物体的边界框&lt;/li>
&lt;li>输出：动作目的编码 $F_{sup}$&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-transfer-module">Purpose Transfer Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/qARrZbuCIm4BipT.png"
loading="lazy"
alt="ptm.png"
>&lt;/p>
&lt;p>通过attention机制，将action purpose传递到query image中，加强相关features
$$
X_{T_i} = X_i + Softmax(X_i⊗F_{sup})⊗X_i,\ where\ i \ in\ [1,n]
$$&lt;/p>
&lt;h3 id="collaboration-enhancement-module">Collaboration Enhancement Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/lx5Jb4jkPhIuSN3.png"
loading="lazy"
alt="cem.png"
>&lt;/p>
&lt;p>交替使用E-step和M-step，得到一个紧凑的基集，重建query image的特征图。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从PTM输入的$X_T = {X_{T_1},&amp;hellip;,X_{T_n}}$经过卷积得到$F={F_1,&amp;hellip;F_n}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化基集$\mu$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>E-step估计隐变量$Z={Z_1,&amp;hellip;Z_n}$&lt;/p>
&lt;ul>
&lt;li>第k个basis 第j个像素 第i个图片&lt;/li>
&lt;li>$Z_{ijk} = \frac{\kappa(f_{ij},\mu_k)}{\sum_{l=1}^{K}\kappa(f_{ij},\mu_l)}$&lt;/li>
&lt;li>$f_{ij}$第i个图像的第j个位置的特征&lt;/li>
&lt;li>$\kappa$是指数核函数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>M-step更新基集$\mu$，并把$\mu$作为$F$的加权平均&lt;/p>
&lt;ul>
&lt;li>$\mu_k = \frac{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}f_{ij}}{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>经过E-M的迭代后，我们用$\mu$和$Z$去重建$X$并得到$F$&lt;/p>
&lt;ul>
&lt;li>$F_i=Z_i\mu$&lt;/li>
&lt;li>$\tilde X_i=X_i+Conv(F_i)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="前景知识">前景知识&lt;/h4>
&lt;p>Expectation-Maximization (E-M)&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/67120173" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/67120173&lt;/a>.&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>初始化参数&lt;/li>
&lt;li>根据初始化的参数，划分类别&lt;/li>
&lt;li>根据最大似然估计重新计算参数&lt;/li>
&lt;li>重复步骤1-3，迭代n次，参数收敛&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>期望最大化注意力机制&lt;/strong>&lt;/p>
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener"
>EMANet&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.jianshu.com/p/6bb799d256b1" target="_blank" rel="noopener"
>https://www.jianshu.com/p/6bb799d256b1&lt;/a>&lt;/li>
&lt;li>作者写的知乎专栏：https://zhuanlan.zhihu.com/p/78018142&lt;/li>
&lt;/ul>
&lt;p>分为$A_E,A_M,A_R$三部分组成，前两者是EM算法的E步和M步&lt;/p>
&lt;ul>
&lt;li>假定输入的特征图为$X\in R^{N\times C}$，基初始值为$\mu\in R^{K\times C}$&lt;/li>
&lt;li>$A_E$步估计隐变量$Z\in R^{N\times K}$，则第k个基对第n个像素的权责可以计算为
&lt;ul>
&lt;li>$z_{nk}=\frac{\kappa(x_n,\mu_k)}{\sum_{j=1}^{K}\kappa(x_n,\mu_j)}$&lt;/li>
&lt;li>实现时可以用公式 $Z=softmax(\lambda X(\mu^T))$，其中$\lambda$作为超参数控制$Z$的分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_M$步更新基。$\mu$被计算为$X$的加权平均。第k个基被个更新为
&lt;ul>
&lt;li>$\mu_k=\frac{\sum_{n=1}^Nz_{nk}X_n}{\sum_{n=1}^Nz_{nk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_E$和$A_M$交替执行T步后，$\mu$和$Z$近似收敛，可以用来对X重新评估
&lt;ul>
&lt;li>$\tilde X=Z\mu$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="decoder">Decoder&lt;/h3>
&lt;p>$$
P^m_i=Conv(Unsample(Conv(X^m_i)+P^{m+1}_i)),\ where\ m \ in \ [1,4]
$$&lt;/p>
&lt;ul>
&lt;li>其中m是第m层，i表示&lt;/li>
&lt;/ul>
&lt;p>把检测结果在与原图相同的特征维度还原出来&lt;/p>
&lt;p>用交叉熵Cross-entropy来作为损失函数&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;ul>
&lt;li>k-fold evaluation protocol 将数据集分成三部分，其中之二作为训练集，剩下作为测试集&lt;/li>
&lt;/ul>
&lt;h3 id="benchmark-setting">Benchmark Setting&lt;/h3>
&lt;ul>
&lt;li>IoU metric
&lt;ul>
&lt;li>for segmentation task 切割任务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mean Absolute Error (MAE)
&lt;ul>
&lt;li>measure the absolute error between the prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>E-measure(?)&lt;/strong> 相关文章 &lt;a class="link" href="https://github.com/DengPingFan/E-measure" target="_blank" rel="noopener"
>E-measure: Enhanced-alignment Measure for Binary Foreground Map Evaluation&lt;/a>
&lt;ul>
&lt;li>a metric that combines local pixels and image-level average values to jointly capture image-level statistics and local pixel matching information.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Pearson Correlation Coefficient (CC)&lt;/strong>
&lt;ul>
&lt;li>皮尔逊相关系数 两个变量之间的协方差和标准差的商 $$ p_{X,Y}=\frac{cov(X,Y)}{\sigma_x\sigma_y} $$&lt;/li>
&lt;li>measure the correlation between prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>其他训练参数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Adam optimizer&lt;/li>
&lt;li>resnet50&lt;/li>
&lt;li>The input is randomly clipped from 360×360 to 320×320 with random horizontal flipping. 随机裁剪+水平翻转&lt;/li>
&lt;li>40 epochs on 1080ti&lt;/li>
&lt;li>learning rate 1e-4&lt;/li>
&lt;li>the number of bases in CEM is $K=256$&lt;/li>
&lt;li>E-M 迭代次数 3&lt;/li>
&lt;/ul>
&lt;h3 id="quantitative-and-qualitative-comparisons">Quantitative and Qualitative Comparisons&lt;/h3>
&lt;p>对比下来就是我们的模型很好很好&lt;/p></description></item><item><title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png" alt="Featured image of post Prior Guided Feature Enrichment Network for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/cs5Rm6YUpuX3QFf.png"
loading="lazy"
alt="title1.png"
>&lt;/p>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>主要解决了两个问题：&lt;/p>
&lt;ul>
&lt;li>Generalization Reduction &amp;amp; High-Level Features.
&lt;ul>
&lt;li>[CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱）&lt;/li>
&lt;li>他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Spatial Inconsistency.
&lt;ul>
&lt;li>因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">RELATED WORK&lt;/h2>
&lt;p>&lt;strong>Few-Shot Learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>meta-learning
&lt;ul>
&lt;li>跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>metric-learning
&lt;ul>
&lt;li>Prototypical network&lt;/li>
&lt;li>这篇文章比较偏向于metric-learning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png"
loading="lazy"
alt="framework1.png"
>&lt;/p>
&lt;h3 id="prior-for-few-shot-segmentation">Prior for Few-Shot Segmentation&lt;/h3>
&lt;p>CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。&lt;/p>
&lt;p>Prior Generation的具体做法&lt;/p>
&lt;ul>
&lt;li>
&lt;p>先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask
$$
X_Q=F(I_Q), \ X_S = F(I_S)\times M_S
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity
$$
cos(x_q,x_s)=\frac{x_q^Tx_s}{|x_q||x_s|},\ \ \ \ q,s\in{1,2,&amp;hellip;,hw}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每一个$x_q \in X_Q$来说，取其中最大的值作为correspondence value
$$
c_q = max_{s\in {1,2&amp;hellip;,hw}}(cos(x_q,x_s))
$$&lt;/p>
&lt;p>$$
C_Q = [c_1,c_2,&amp;hellip;,c_hw] \in R^{hw\times1}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization
$$
Y_Q = \frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\epsilon}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="feature-enrichment-module">Feature Enrichment Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/rhadcOZ1iPuQ7nH.png"
loading="lazy"
alt="module1.png"
>&lt;/p>
&lt;p>将support image和query image关联起来的方法&lt;/p>
&lt;ul>
&lt;li>对support image做global average pooling
&lt;ul>
&lt;li>不用说都感觉效果一般&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>multi-level spatial information
&lt;ul>
&lt;li>说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>作者提出的FEM可以很好的解决问题。其中M的具体操作如下&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/P9ux8joOMAlyv3t.png"
loading="lazy"
alt="module2.png"
>&lt;/p>
&lt;h3 id="loss-function">Loss Function&lt;/h3>
&lt;p>$$
L = \frac{\sigma}{n}\sum_{i=1}^{n}{L_1^i+L_2}
$$&lt;/p>
&lt;p>主要选用交叉熵作为损失函数。&lt;/p>
&lt;ul>
&lt;li>$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成&lt;/li>
&lt;li>具体来说，这个X应该是FEM模块中，每一层的feature在information concentration之前，interpolate后做交叉熵的值&lt;/li>
&lt;li>$L_2$ 最后prediction和label的交叉熵。&lt;/li>
&lt;/ul>
&lt;p>&lt;!-- raw HTML omitted -->问题：实际上两个loss离得很近（计算L1的feature其实经过concat然后稍微卷积一下就到计算L2的feature了），那这样效果不是跟直接把学习率调大1一倍差不多吗）&lt;!-- raw HTML omitted -->&lt;/p></description></item></channel></rss>