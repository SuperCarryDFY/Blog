<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Segmentation on Fengyuan Dai</title><link>https://SuperCarryDFY.github.io/Blog/tags/segmentation/</link><description>Recent content in Segmentation on Fengyuan Dai</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 28 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/Blog/tags/segmentation/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning What Not to Segment A New Perspective on Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/learning-what-not-to-segment-a-new-perspective-on-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png" alt="Featured image of post Learning What Not to Segment A New Perspective on Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/cUieTg3LOa1ZN2k.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>I read the paper in the first month when I came in MiLab. However, at that time I could not recognize the quality of this method (I usually don&amp;rsquo;t read the experiment carefully). It appeared when I was reading another paper called &amp;ldquo;Holistic Prototype Activation for Few-Shot Segmentation&amp;rdquo;. The HPA performs well but can not beat BAM (although HPA has less parameter to train, obviously). So I decided to read this paper again.&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>Previous problem:&lt;/p>
&lt;ul>
&lt;li>the trained models are biased towards the seen classes instead of being ideally class-agnostic&lt;/li>
&lt;/ul>
&lt;p>Contribution&lt;/p>
&lt;ul>
&lt;li>
&lt;p>BAM, i.e., base and the meta has two branches, allowing model to learn what not to segment and what to segmentation, respectively.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Design a special loss in order to train two branches suitably.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extend the proposed approach to a more challenging setting, which simultaneously identifies the targets of base and novel classes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In the following image, (a) is a classical method to address FSS task; (b) is BAM approach; (c) is the extension of BAM&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/hOT72tJIqimkDKL.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>They adopt two stage training method, which means they train base learner and meta learner separately.&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/25/4KBGJQmrc71nMu2.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="base-learner">BASE LEARNER&lt;/h3>
&lt;p>Query image goes through four ResNet blocks and becomes intermediate feature maps $f_b^q$. Then the decoder network $D_b$ yields the prediction result. $N_b$ represents the number of base categories.
$$
P_b = softmax(D_b(f_b^q)) \in R^{(1+N_b)\times H\times W}
$$
Loss can be defined as
$$
L_{base} = \frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}CE(P_{b;i},m^q_{b;i})
$$
It is worth noting that they do not employ the general FSS learning paradigm (update the parameter in each episode). And they train the base learner independently. They explain as follow:&lt;/p>
&lt;blockquote>
&lt;p>It is unrealistic to additionally build such a large network on the basis of the original few-shot model, which will introduce too many parameters and slow down the inference speed.&lt;/p>
&lt;p>It is unknown whether the base learner can be trained well with the episodic learning paradigm, so a two stage training strategy is eventually adopted.&lt;/p>
&lt;/blockquote>
&lt;p>In the ablation study, it shows that with two stage train, the model can perform better.&lt;/p>
&lt;h3 id="meta-learner">META LEARNER&lt;/h3>
&lt;p>This part is highly resemble similar to CANet, employing &amp;ldquo;expand &amp;amp; concatenate&amp;rdquo; operations. The loss can be described as
$$
L_{meta} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_{m;i},m_i^q)
$$
, when $n_e$ denotes the number of training episodes in each batch&lt;/p>
&lt;h3 id="ensemble">Ensemble&lt;/h3>
&lt;p>This part is designed to leverage the low-level feature to adjust the coarse predictions which is derived from meta learner.&lt;/p>
&lt;p>Firstly, we calculate the overall indicator $\psi$ for guiding the adjustment process:
$$
A_s = F_{reshape}(f_{low}^s) \in R^{C_1\times N},\
G^s = A_sA^T \in R^{C_1\times C_1}
$$
$G^s$ should be denoted as Gram matrix, BTW.&lt;/p>
&lt;blockquote>
&lt;p>Gram matrix can be regarded as eccentric covariance matrix between features. Every number in gram matrix describe the relation between every two feature, about which two features appear simultaneously, which two features just offset from each other, etc.&lt;/p>
&lt;/blockquote>
&lt;p>$$
\psi =||G^s - G^q||_F
$$&lt;/p>
&lt;p>$||\ ||_F$ denotes the Frobenius norm of the input metirx.&lt;/p>
&lt;p>After that, the final segmentation pridections $P_f$ can be described as follow:
$$
p_f^0 = F_{ensemble}(F_\psi(p_m^0),p_b^f), \
p_f=p_f^0(+)F_\psi(p_m^1)
$$
where $p_m$,$p_b$ denote the predictions of the meta learner and base learner respectively. The superscript &amp;ldquo;0&amp;rdquo; and &amp;ldquo;1&amp;rdquo; represent the background and foreground respectively. Both $F_ψ$ and $F_{ensemble}$ are 1×1 convolution operations with specific initial parameters.&lt;/p>
&lt;h3 id="loss">LOSS&lt;/h3>
&lt;p>$$
L=L_{final} + \lambda L_{meta} \
L_{final} = \frac{1}{n_e}\sum_{i=1}^{n_e}BCE(p_i^q,m_i^q)
$$&lt;/p>
&lt;p>where $L_{meta}$ is the loss function of the meta learner defined in the meta learner stage.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/1E4h7VSepL2wHUG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/27/JuMlP1aGcUwxI94.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</title><link>https://SuperCarryDFY.github.io/Blog/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png" alt="Featured image of post CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/AiIQyMX2cZBlStK.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从TPAMI20那篇过来的，主要想看一下哪里说的middle-level feature&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>consists of&lt;/p>
&lt;ul>
&lt;li>a two-branch dense comparison module
&lt;ul>
&lt;li>performs multi-level feature comparison between the support image and the query image&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>an iterative optimization module
&lt;ul>
&lt;li>iteratively refines the predicted results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>有趣的地方&lt;/p>
&lt;ul>
&lt;li>先前的工作，从1-shot拓展到k-shot时，都是用non-learnable fusion，这篇文章中用的是attention mechanism。&lt;/li>
&lt;li>做test的时候，不再输入support image mask了，而是输入support image bounding box.&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/Ea6tJYghbZvXCiW.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="dense-comparison-module">DENSE COMPARISON MODULE&lt;/h3>
&lt;p>在CNN中&lt;/p>
&lt;ul>
&lt;li>feature in low layers often relate to low-level cues 比如颜色，边缘。个人感觉是因为感受野不够大&lt;/li>
&lt;li>feature in higher layers relate to object-level concepts 比如物品种类&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>因为他们要求模型训练完之后具备一定的generalization，而middle-level fature有可能会包含来自没见过的物体的part。比如说训练的时候见过小轿车，那么在测试中，我就比较容易根据middle-level中的轮胎来segment公交车。（make sense）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>dialated convolution&lt;/strong>空洞卷积&lt;/p>
&lt;p>以下来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>知乎&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。&lt;/p>
&lt;p>传统卷积核的本质问题：pool减小图片尺寸，upsampling增大图片尺寸，在这个过程中肯定有信息丢失掉了。因此空洞卷积就是不通过pooling也能获得较大感受野的方法&lt;/p>
&lt;p>dialated convolution的优点：内部数据结构的保留和避免使用 down-sampling 。&lt;/p>
&lt;/blockquote>
&lt;p>具体的，他们把Resnet分成4个block，只用block2和block3的输出，将其concat到一起。后面就是support feature与support feaure相乘来把背景像素清0，avg pool之后repeat到之前的维度和query feature concat到一起，比较常规。&lt;/p>
&lt;h3 id="inerative-optimization-module">INERATIVE OPTIMIZATION MODULE&lt;/h3>
&lt;p>每一次迭代中，首先进行如下运算。其中$M$是output of the residual blocks，$x$是DCM模块的输出，$y_{t-1}$是上一个迭代块的输出，$F$是concat之后再经过两层卷积层。
$$
M_t = x + F(x,y_{t-1})
$$
对$M_t$，再经过两层vanilla residual blocks。然后再经过&lt;a class="link" href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener"
>ASSP&lt;/a>模块输出。&lt;/p>
&lt;p>就差不多这么迭代n次&lt;/p>
&lt;h3 id="attention-mechanism-for-k-shot-segmentation">ATTENTION MECHANISM FOR K-SHOT SEGMENTATION&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/ZEuUP7zm2i9IKks.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;strong>这也算attention嘛? query, key, value分别是什么？&lt;/strong>&lt;/p>
&lt;p>在attention模块中，经过两层卷积再softmax得到$\hat\lambda_k$，然后把$\hat\lambda_k$和经过卷积的support sample n相乘。&lt;/p>
&lt;h2 id="ablation-study">ABLATION STUDY&lt;/h2>
&lt;p>&lt;strong>Feature for Comparison&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/8ZJEQznfqDu9sTj.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里说单个的话block2效果是最好的，整体上block2+block3的效果最好。嗯，比较可信。&lt;/p>
&lt;blockquote>
&lt;p>其中提了一嘴While block4 corresponds to high-level features, e.g., categories, and incorporates a great number of parameters (2048 channels), which makes it hard to optimize under the fewshot setting.&lt;/p>
&lt;p>深有同感！&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Attention vs. Feature Fusion vs. Mask Fusion&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/layPuvoMD6Xrgkc.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从这里可以看出Attention效果是最好的。但是相比来说的话还是多了两层卷积，增加了参数。Feature-Avg表现不错，感觉跟attention也差不多了（主要是没有另外加参数）。Mask-Avg这么奇怪的想法竟然也有效，能+0.5。还有Mask-OR你要笑死我嘛，怎么还没1-shot高啊，怎么回事啊小老弟，纯纯的帮倒忙。&lt;/p></description></item><item><title>Holistic Prototype Activation for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/holistic-prototype-activation-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/holistic-prototype-activation-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png" alt="Featured image of post Holistic Prototype Activation for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/B18VowS6RfMEJKX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）&lt;/p>
&lt;p>Motivations-&amp;gt;现在的FSS主要有俩问题&lt;/p>
&lt;ul>
&lt;li>会把不属于本类的物体也分割进来&lt;/li>
&lt;li>分不清边界&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/OJjBCcq7IUnFfey.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体在train的时候见过，那么这个物体肯定就不是要分割的物体了(好像有点transductive了哈，但是严格说又不是)&lt;/p>
&lt;blockquote>
&lt;p>The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)&lt;/p>
&lt;/blockquote>
&lt;p>对于第二个问题，他们说关键就是要找出support和query中的本质联系（0.0），他们提出了基于DeepLabv3的CRD模型来解决这个问题。&lt;/p>
&lt;blockquote>
&lt;p>We argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images&lt;/p>
&lt;/blockquote>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="prototype-acquisition">PROTOTYPE ACQUISITION&lt;/h3>
&lt;p>在这个模块生成base class的prototype
$$
P_b^c = \frac{1}{N}\sum_{i=1}^{N_c}\frac{1}{|\hat{L_c^i}|}\sum_{j=1}^{HW}F_c^{ij}\hat{L_c^{ij}}
$$
其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。&lt;/p>
&lt;h3 id="prototype-activation-module">PROTOTYPE ACTIVATION MODULE&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/SeOERlPDLBuTasG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。&lt;/p>
&lt;p>对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\phi$是1&lt;em>1的卷积再加激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间
$$
A_k^{ij}=g_\phi(F_q^{ij}+P^k_h)
$$
然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别）
$$
k^&lt;/em> = argmax_k(A_k^{ij}) \
M^{ij}=A_{k^&lt;em>}^{ij} ,\ if \ k^&lt;/em>=1 \ otherwise \ 0
$$&lt;/p>
&lt;p>下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为前面不是已经做了匹配度计算了么，所以直接在对应位置做expand就好了，具体可见下图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/HumfepLVWd6byYa.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="cross-reference-decoder">CROSS-REFERENCE DECODER&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/uwkmMHxE4P8q15Y.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/y95dGIcosCltvVN.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/Blog/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png" alt="Featured image of post Prior Guided Feature Enrichment Network for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/cs5Rm6YUpuX3QFf.png"
loading="lazy"
alt="title1.png"
>&lt;/p>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>主要解决了两个问题：&lt;/p>
&lt;ul>
&lt;li>Generalization Reduction &amp;amp; High-Level Features.
&lt;ul>
&lt;li>[CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱）&lt;/li>
&lt;li>他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Spatial Inconsistency.
&lt;ul>
&lt;li>因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">RELATED WORK&lt;/h2>
&lt;p>&lt;strong>Few-Shot Learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>meta-learning
&lt;ul>
&lt;li>跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>metric-learning
&lt;ul>
&lt;li>Prototypical network&lt;/li>
&lt;li>这篇文章比较偏向于metric-learning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png"
loading="lazy"
alt="framework1.png"
>&lt;/p>
&lt;h3 id="prior-for-few-shot-segmentation">Prior for Few-Shot Segmentation&lt;/h3>
&lt;p>CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。&lt;/p>
&lt;p>Prior Generation的具体做法&lt;/p>
&lt;ul>
&lt;li>
&lt;p>先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask
$$
X_Q=F(I_Q), \ X_S = F(I_S)\times M_S
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity
$$
cos(x_q,x_s)=\frac{x_q^Tx_s}{|x_q||x_s|},\ \ \ \ q,s\in{1,2,&amp;hellip;,hw}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每一个$x_q \in X_Q$来说，取其中最大的值作为correspondence value
$$
c_q = max_{s\in {1,2&amp;hellip;,hw}}(cos(x_q,x_s))
$$&lt;/p>
&lt;p>$$
C_Q = [c_1,c_2,&amp;hellip;,c_hw] \in R^{hw\times1}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization
$$
Y_Q = \frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\epsilon}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="feature-enrichment-module">Feature Enrichment Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/rhadcOZ1iPuQ7nH.png"
loading="lazy"
alt="module1.png"
>&lt;/p>
&lt;p>将support image和query image关联起来的方法&lt;/p>
&lt;ul>
&lt;li>对support image做global average pooling
&lt;ul>
&lt;li>不用说都感觉效果一般&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>multi-level spatial information
&lt;ul>
&lt;li>说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>作者提出的FEM可以很好的解决问题。其中M的具体操作如下&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/P9ux8joOMAlyv3t.png"
loading="lazy"
alt="module2.png"
>&lt;/p>
&lt;h3 id="loss-function">Loss Function&lt;/h3>
&lt;p>$$
L = \frac{\sigma}{n}\sum_{i=1}^{n}{L_1^i+L_2}
$$&lt;/p>
&lt;p>主要选用交叉熵作为损失函数。&lt;/p>
&lt;ul>
&lt;li>$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成&lt;/li>
&lt;li>具体来说，这个X应该是FEM模块中，每一层的feature在information concentration之前，interpolate后做交叉熵的值&lt;/li>
&lt;li>$L_2$ 最后prediction和label的交叉熵。&lt;/li>
&lt;/ul></description></item></channel></rss>