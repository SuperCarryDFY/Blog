<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Few-Shot Learning on Dai Fengyuan</title><link>https://SuperCarryDFY.github.io/tags/few-shot-learning/</link><description>Recent content in Few-Shot Learning on Dai Fengyuan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 16 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/tags/few-shot-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</title><link>https://SuperCarryDFY.github.io/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/canet-class-agnostic-segmentation-networks-with-iterative-refinement-and-attentive-few-shot-learning/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png" alt="Featured image of post CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/AiIQyMX2cZBlStK.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从TPAMI20那篇过来的，主要想看一下哪里说的middle-level feature&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/9DYVUmMyPNJnwkq.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>consists of&lt;/p>
&lt;ul>
&lt;li>a two-branch dense comparison module
&lt;ul>
&lt;li>performs multi-level feature comparison between the support image and the query image&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>an iterative optimization module
&lt;ul>
&lt;li>iteratively refines the predicted results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>fancy的地方&lt;/p>
&lt;ul>
&lt;li>先前的工作，从1-shot拓展到k-shot时，都是用non-learnable fusion，这篇文章中用的是attention mechanism。&lt;/li>
&lt;li>做test的时候，不再输入support image mask了，而是输入support image bounding box.&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/Ea6tJYghbZvXCiW.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="dense-comparison-module">DENSE COMPARISON MODULE&lt;/h3>
&lt;p>在CNN中&lt;/p>
&lt;ul>
&lt;li>feature in low layers often relate to low-level cues 比如颜色，边缘。个人感觉是因为感受野不够大&lt;/li>
&lt;li>feature in higher layers relate to object-level concepts 比如物品种类&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>因为他们要求模型训练完之后具备一定的generalization，而middle-level fature有可能会包含来自没见过的物体的part。比如说训练的时候见过小轿车，那么在测试中，我就比较容易根据middle-level中的轮胎来segment公交车。（make sense）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>dialated convolution&lt;/strong>空洞卷积&lt;/p>
&lt;p>以下来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>知乎&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。&lt;/p>
&lt;p>传统卷积核的本质问题：pool减小图片尺寸，upsampling增大图片尺寸，在这个过程中肯定有信息丢失掉了。因此空洞卷积就是不通过pooling也能获得较大感受野的方法&lt;/p>
&lt;p>dialated convolution的优点：内部数据结构的保留和避免使用 down-sampling 。&lt;/p>
&lt;/blockquote>
&lt;p>具体的，他们把Resnet分成4个block，只用block2和block3的输出，将其concat到一起。后面就是support feature与support feaure相乘来把背景像素清0，avg pool之后repeat到之前的维度和query feature concat到一起，比较常规。&lt;/p>
&lt;h3 id="inerative-optimization-module">INERATIVE OPTIMIZATION MODULE&lt;/h3>
&lt;p>每一次迭代中，首先进行如下运算。其中$M$是output of the residual blocks，$x$是DCM模块的输出，$y_{t-1}$是上一个迭代块的输出，$F$是concat之后再经过两层卷积层。
$$
M_t = x + F(x,y_{t-1})
$$
对$M_t$，再经过两层vanilla residual blocks。然后再经过(ASSP)[https://arxiv.org/abs/1706.05587]模块输出。&lt;/p>
&lt;p>就差不多这么迭代n次&lt;/p>
&lt;h3 id="attention-mechanism-for-k-shot-segmentation">ATTENTION MECHANISM FOR K-SHOT SEGMENTATION&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/ZEuUP7zm2i9IKks.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;strong>这也算attention嘛? query, key, value分别是什么？&lt;/strong>&lt;/p>
&lt;p>在attention模块中，经过两层卷积再softmax得到$\hat\lambda_k$，然后把$\hat\lambda_k$和经过卷积的support sample n相乘。&lt;/p>
&lt;h2 id="ablation-study">ABLATION STUDY&lt;/h2>
&lt;p>&lt;strong>Feature for Comparison&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/8ZJEQznfqDu9sTj.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里说单个的话block2效果是最好的，整体上block2+block3的效果最好。嗯，比较可信。&lt;/p>
&lt;blockquote>
&lt;p>其中提了一嘴While block4 corresponds to high-level features, e.g., categories, and incorporates a great number of parameters (2048 channels), which makes it hard to optimize under the fewshot setting.&lt;/p>
&lt;p>深有同感！我改topformer的时候就是纠结这个地方&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Attention vs. Feature Fusion vs. Mask Fusion&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/layPuvoMD6Xrgkc.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>从这里可以看出Attention效果是最好的。但是相比来说的话还是多了两层卷积，增加了参数。Feature-Avg表现不错，感觉跟attention也差不多了（主要是没有另外加参数）。Mask-Avg这么奇怪的想法竟然也有效，能+0.5。还有Mask-OR你要笑死我嘛，怎么还没1-shot高啊，怎么回事啊小老弟，纯纯的帮倒忙。&lt;/p></description></item><item><title>Holistic Prototype Activation for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/holistic-prototype-activation-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png" alt="Featured image of post Holistic Prototype Activation for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/B18VowS6RfMEJKX.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这篇真的好复杂&lt;/p>
&lt;h2 id="abstract--introduction">ABSTRACT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>从Few-shot learning角度，这篇也是基于metric-based的（我都没看到过几篇meta-based的文章，不知道是不是已经被淘汰了）&lt;/p>
&lt;p>现在的FSS主要有俩问题&lt;/p>
&lt;ul>
&lt;li>会把不属于本类的物体也分割进来&lt;/li>
&lt;li>分不清边界&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/OJjBCcq7IUnFfey.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>对于第一个问题，他们说在meta-training阶段就让模型直接学会识别base-dataset，这样后面做test的时候，如果说一个物体我在train的时候见过，那么这个物体肯定就不是我要分割的物体了&lt;/p>
&lt;blockquote>
&lt;p>The proposed Prototype Activation Module (PAM) integrates the base prototypes with the current novel prototype to form a holistic prototype set, and then activates the region of each target in the query image based on this set. If the object area is activated with high confidence by base prototypes, the corresponding location will be erased in the final activation map (i.e., set to 0)&lt;/p>
&lt;/blockquote>
&lt;p>对于第二个问题，他们说关键就是要找出support和query中的本质联系（套话），他们提出了基于DeepLabv3的CRD模型来解决这个问题。&lt;/p>
&lt;blockquote>
&lt;p>We argue that, with limited samples available, the crux of improving foreground activation accuracy is to capture the co-existence characteristics between support and query images&lt;/p>
&lt;/blockquote>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/wB47rIvCWOyTZVi.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="prototype-acquisition">PROTOTYPE ACQUISITION&lt;/h3>
&lt;p>在这个模块生成base class的prototype
$$
P_b^c = \frac{1}{N}\sum_{i=1}^{N_c}\frac{1}{|\hat{L_c^i}|}\sum_{j=1}^{HW}F_c^{ij}\hat{L_c^{ij}}
$$
其中$P_b^c$代表了class c的原型，size = 1x1xC。$F$，$\hat{L}$分别是经过CNN的feature map和downsampled mask label with size HxWx1。&lt;/p>
&lt;h3 id="prototype-activation-module">PROTOTYPE ACTIVATION MODULE&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/SeOERlPDLBuTasG.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>holistic prototype $P_h$是novel class（in meta-test phase, when in meta-train it belong to base category）$P_n$和$P_b$的集合。&lt;/p>
&lt;p>对每个class k，计算$F_q$和$P_n^k$的匹配度，其中+代表concatenated，$g_\phi$是激活函数，这里用的是sigmoid，为了让output在[0,1]这个区间
$$
A_k^{ij}=g_\phi(F_q^{ij}+P^k_h)
$$
然后他先获得一个maximum score，这个怎么得呢？好聪明啊。主要思路就是，如果说这个像素的跟base dataset里面的prototype相似度更高，那我不管这个像素跟query的相似度有多高，直接把这个像素对应的位置置为0。（因为这个像素更有可能属于base dataset中已经学习过的类别）
$$
k^* = argmax_k(A_k^{ij})
$$&lt;/p>
&lt;p>$$
M^{ij}=A_{k^&lt;em>}^{ij} ,\ if \ k^&lt;/em>=1 \ otherwise \ 0
$$&lt;/p>
&lt;p>下面这一步也很聪明，因为一般做FSS的话，都是有个expand + concat的操作的。然而这里有好多个原型，他就不做全局的expand了，因为我前面不是已经做了匹配度计算了么，所以我直接在对应位置做expand就好了，具体可见下图&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/HumfepLVWd6byYa.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="cross-reference-decoder">CROSS-REFERENCE DECODER&lt;/h3>
&lt;p>这一步就比较乱了，左一步右一步的，还跟deeplabv3有关，我可能要先去把deeplabv3看了才能真正理解。&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/uwkmMHxE4P8q15Y.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="loss">LOSS&lt;/h2>
&lt;p>loss部分没太看懂，$\alpha$好理解，也是多任务下比较常见的做法。就是$L_{act}$是咋算的没太看懂，不是直接跟label做cross-entropy吗&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/18/y95dGIcosCltvVN.png"
loading="lazy"
alt="image.png"
>&lt;/p></description></item><item><title>One-Shot Affordance Detection</title><link>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/one-shot-affordance-detection/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png" alt="Featured image of post One-Shot Affordance Detection" />&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2106.14747.pdf" target="_blank" rel="noopener"
>One-shot Affordance Detetion 2106.14747.pdf (arxiv.org)&lt;/a>&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;ul>
&lt;li>可供性检测就是通过一张图片识别物体潜在的动作。&lt;/li>
&lt;li>OS-AD网络可以在所有候选图片中帮助发现普遍的可供性，并且学会适应感知未发现的可供性。&lt;/li>
&lt;li>他们建立了一个数据集PAD ：4k Image；31 affordance；72个物体类别&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>挑战OS-AD 给一张图片，告知其图片上的物体的行为，则可以察觉所有物体普遍的可供性&lt;/li>
&lt;li>问题：现实生活中一个物体可能有多个affordance（例如沙发可以躺也可以睡），而具体用什么affordance取决于人在这个场景中的目的。抛去目的的指引，直接从一张图片中学习affordance会导致忽略了其他视觉上的对此时的任务有效的affordance
&lt;ul>
&lt;li>从行为中找暗示&lt;/li>
&lt;li>采用&lt;strong>collaboration learning&lt;/strong>去捕捉不同物体间的潜在关系，抵消物体不同的appearance，增加泛化性；OS-AD PLM，PTM，CEM&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可供性检测应该能适用于各种环境： PAD 目标驱动可供性数据集&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;ul>
&lt;li>Affordance Detection&lt;/li>
&lt;li>One-Shot Learn
&lt;ul>
&lt;li>based on metric learning using the siamese neural network 度量学习；孪生神经网络&lt;/li>
&lt;li>meta-learning and generation models 元学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="methods">Methods&lt;/h2>
&lt;h3 id="framework">Framework&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/GYdogZcblesFhuk.png"
loading="lazy"
alt="framework2.png"
>&lt;/p>
&lt;ul>
&lt;li>input: query images, human-object interactions&lt;/li>
&lt;li>ResNet50 -&amp;gt; 获得图像表现 $X$ and $ X_{sup} $&lt;/li>
&lt;li>输入$X_{sup}$和 人和物体的边界矩阵到PLM -&amp;gt; 提取human-object interaction信息，对action-purpose编码，发现人想要旋转的原因&lt;/li>
&lt;li>输入feature representation和$X$到PTM里面 -&amp;gt; 让网络学会处理带affordance的信息&lt;/li>
&lt;li>输入encoded feature 到CEM， 输出affordance&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-learning-module">Purpose Learning Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/HUW6kjnPJ29qX83.png"
loading="lazy"
alt="plm.png"
>&lt;/p>
&lt;p>:star:&lt;a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_On_Exploring_Undetermined_Relationships_for_Visual_Relationship_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener"
>On Exploring Undetermined Relationships for Visual Relationship Detection&lt;/a>受到了这篇文章的启发，说instance（人或物）的特征可以指导网络哪里应该focus。&lt;/p>
&lt;p>先得到$M_O$和$M_H$ &lt;strong>（这两者分别代表什么？作者说是为了让模型去分别focus on物体和个人，引入了注意力机制，其中GMP的作用是得到最显著的特征）&lt;/strong> 其中⊗ 代表element-wise product，元素对应位置相乘，$f_O$和$f_H$是$X_O$和$X_H$进行 global maximum pooling（GMP）后的值
$$
M_O = Softmax(f_O⊗X_{sup})⊗X_{sup} \
M_H = Softmax(f_H⊗X_{sup})⊗X_{sup}
$$
作者说他们用$f_O$去指导网络应该focus on人物交互$M_{HO}$
$$
M_{HO}=Conv(f_O⊗X_H)
$$
最后得到encoding of the action purpose $F_{sup}$，其中&amp;quot; ·&amp;ldquo;代表position-wise dot product.
$$
F_{sup} = MaxPooling((M_{HO}·M_H)+(M_{HO}·M_O))
$$&lt;/p>
&lt;ul>
&lt;li>输入：$X_{sup}$以及人和物体的边界框&lt;/li>
&lt;li>输出：动作目的编码 $F_{sup}$&lt;/li>
&lt;/ul>
&lt;h3 id="purpose-transfer-module">Purpose Transfer Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/qARrZbuCIm4BipT.png"
loading="lazy"
alt="ptm.png"
>&lt;/p>
&lt;p>通过attention机制，将action purpose传递到query image中，加强相关features
$$
X_{T_i} = X_i + Softmax(X_i⊗F_{sup})⊗X_i,\ where\ i \ in\ [1,n]
$$&lt;/p>
&lt;h3 id="collaboration-enhancement-module">Collaboration Enhancement Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/lx5Jb4jkPhIuSN3.png"
loading="lazy"
alt="cem.png"
>&lt;/p>
&lt;p>交替使用E-step和M-step，得到一个紧凑的基集，重建query image的特征图。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从PTM输入的$X_T = {X_{T_1},&amp;hellip;,X_{T_n}}$经过卷积得到$F={F_1,&amp;hellip;F_n}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化基集$\mu$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>E-step估计隐变量$Z={Z_1,&amp;hellip;Z_n}$&lt;/p>
&lt;ul>
&lt;li>第k个basis 第j个像素 第i个图片&lt;/li>
&lt;li>$Z_{ijk} = \frac{\kappa(f_{ij},\mu_k)}{\sum_{l=1}^{K}\kappa(f_{ij},\mu_l)}$&lt;/li>
&lt;li>$f_{ij}$第i个图像的第j个位置的特征&lt;/li>
&lt;li>$\kappa$是指数核函数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>M-step更新基集$\mu$，并把$\mu$作为$F$的加权平均&lt;/p>
&lt;ul>
&lt;li>$\mu_k = \frac{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}f_{ij}}{\sum_{i=1}^n\sum_{j=1}^Lz_{ijk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>经过E-M的迭代后，我们用$\mu$和$Z$去重建$X$并得到$F$&lt;/p>
&lt;ul>
&lt;li>$F_i=Z_i\mu$&lt;/li>
&lt;li>$\tilde X_i=X_i+Conv(F_i)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="前景知识">前景知识&lt;/h4>
&lt;p>Expectation-Maximization (E-M)&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/67120173" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/67120173&lt;/a>.&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>初始化参数&lt;/li>
&lt;li>根据初始化的参数，划分类别&lt;/li>
&lt;li>根据最大似然估计重新计算参数&lt;/li>
&lt;li>重复步骤1-3，迭代n次，参数收敛&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>期望最大化注意力机制&lt;/strong>&lt;/p>
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener"
>EMANet&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.jianshu.com/p/6bb799d256b1" target="_blank" rel="noopener"
>https://www.jianshu.com/p/6bb799d256b1&lt;/a>&lt;/li>
&lt;li>作者写的知乎专栏：https://zhuanlan.zhihu.com/p/78018142&lt;/li>
&lt;/ul>
&lt;p>分为$A_E,A_M,A_R$三部分组成，前两者是EM算法的E步和M步&lt;/p>
&lt;ul>
&lt;li>假定输入的特征图为$X\in R^{N\times C}$，基初始值为$\mu\in R^{K\times C}$&lt;/li>
&lt;li>$A_E$步估计隐变量$Z\in R^{N\times K}$，则第k个基对第n个像素的权责可以计算为
&lt;ul>
&lt;li>$z_{nk}=\frac{\kappa(x_n,\mu_k)}{\sum_{j=1}^{K}\kappa(x_n,\mu_j)}$&lt;/li>
&lt;li>实现时可以用公式 $Z=softmax(\lambda X(\mu^T))$，其中$\lambda$作为超参数控制$Z$的分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_M$步更新基。$\mu$被计算为$X$的加权平均。第k个基被个更新为
&lt;ul>
&lt;li>$\mu_k=\frac{\sum_{n=1}^Nz_{nk}X_n}{\sum_{n=1}^Nz_{nk}}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A_E$和$A_M$交替执行T步后，$\mu$和$Z$近似收敛，可以用来对X重新评估
&lt;ul>
&lt;li>$\tilde X=Z\mu$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="decoder">Decoder&lt;/h3>
&lt;p>$$
P^m_i=Conv(Unsample(Conv(X^m_i)+P^{m+1}_i)),\ where\ m \ in \ [1,4]
$$&lt;/p>
&lt;ul>
&lt;li>其中m是第m层，i表示&lt;/li>
&lt;/ul>
&lt;p>把检测结果在与原图相同的特征维度还原出来&lt;/p>
&lt;p>用交叉熵Cross-entropy来作为损失函数&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;ul>
&lt;li>k-fold evaluation protocol 将数据集分成三部分，其中之二作为训练集，剩下作为测试集&lt;/li>
&lt;/ul>
&lt;h3 id="benchmark-setting">Benchmark Setting&lt;/h3>
&lt;ul>
&lt;li>IoU metric
&lt;ul>
&lt;li>for segmentation task 切割任务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mean Absolute Error (MAE)
&lt;ul>
&lt;li>measure the absolute error between the prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>E-measure(?)&lt;/strong> 相关文章 &lt;a class="link" href="https://github.com/DengPingFan/E-measure" target="_blank" rel="noopener"
>E-measure: Enhanced-alignment Measure for Binary Foreground Map Evaluation&lt;/a>
&lt;ul>
&lt;li>a metric that combines local pixels and image-level average values to jointly capture image-level statistics and local pixel matching information.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Pearson Correlation Coefficient (CC)&lt;/strong>
&lt;ul>
&lt;li>皮尔逊相关系数 两个变量之间的协方差和标准差的商 $$ p_{X,Y}=\frac{cov(X,Y)}{\sigma_x\sigma_y} $$&lt;/li>
&lt;li>measure the correlation between prediction and ground truth&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>其他训练参数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Adam optimizer&lt;/li>
&lt;li>resnet50&lt;/li>
&lt;li>The input is randomly clipped from 360×360 to 320×320 with random horizontal flipping. 随机裁剪+水平翻转&lt;/li>
&lt;li>40 epochs on 1080ti&lt;/li>
&lt;li>learning rate 1e-4&lt;/li>
&lt;li>the number of bases in CEM is $K=256$&lt;/li>
&lt;li>E-M 迭代次数 3&lt;/li>
&lt;/ul>
&lt;h3 id="quantitative-and-qualitative-comparisons">Quantitative and Qualitative Comparisons&lt;/h3>
&lt;p>对比下来就是我们的模型很好很好&lt;/p></description></item><item><title>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title><link>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/prior-guided-feature-enrichment-network-for-few-shot-segmentation/</guid><description>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png" alt="Featured image of post Prior Guided Feature Enrichment Network for Few-Shot Segmentation" />&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/cs5Rm6YUpuX3QFf.png"
loading="lazy"
alt="title1.png"
>&lt;/p>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>主要解决了两个问题：&lt;/p>
&lt;ul>
&lt;li>Generalization Reduction &amp;amp; High-Level Features.
&lt;ul>
&lt;li>[CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning]指出high-level feature cause performance drop. （估计是因为使用high-level feature会使得模型泛化能力变弱）&lt;/li>
&lt;li>他们用imagenet上pre-train出来的模块，生成“prior”。因为prior是用high-level feature训练出来的，并且只是在imagenet上训练，所以不失generalization ability。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Spatial Inconsistency.
&lt;ul>
&lt;li>因为support image有限，有时候support image和query image上的物体的姿势之类的可能变化很大。他们提出了Feature Enrichment Module，去解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">RELATED WORK&lt;/h2>
&lt;p>&lt;strong>Few-Shot Learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>meta-learning
&lt;ul>
&lt;li>跟memory有关。似乎是基于RNN的模型（比如LSTM）修改的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>metric-learning
&lt;ul>
&lt;li>Prototypical network&lt;/li>
&lt;li>这篇文章比较偏向于metric-learning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/sO1AU62fiEWBKbQ.png"
loading="lazy"
alt="framework1.png"
>&lt;/p>
&lt;h3 id="prior-for-few-shot-segmentation">Prior for Few-Shot Segmentation&lt;/h3>
&lt;p>CANet表现好主要是通过backbone提取了middle-level feature，并且CANet说middle-level里面有unseen class的object part。但是我们的解释与之相反。&lt;/p>
&lt;p>Prior Generation的具体做法&lt;/p>
&lt;ul>
&lt;li>
&lt;p>先利用backbone network对输入的query和support进行特征提取，其中$M_S$代表Supprort image mask
$$
X_Q=F(I_Q), \ X_S = F(I_S)\times M_S
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$Y_Q$表征了$X_Q$和$X_S$在像素维度上的一致性。如果一个$X_Q$上的像素在$Y_Q$上有比较大的值，说明这个像素在support image上更有可能有至少一个像素。为了计算$Y_Q$，首先计算cosine similarity
$$
cos(x_q,x_s)=\frac{x_q^Tx_s}{|x_q||x_s|},\ \ \ \ q,s\in{1,2,&amp;hellip;,hw}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每一个$x_q \in X_Q$来说，取其中最大的值作为correspondence value
$$
c_q = max_{s\in {1,2&amp;hellip;,hw}}(cos(x_q,x_s))
$$&lt;/p>
&lt;p>$$
C_Q = [c_1,c_2,&amp;hellip;,c_hw] \in R^{hw\times1}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把$C_Q$ reshape 到h*w*1的空间，作为$Y_Q$，然后做一个normalization
$$
Y_Q = \frac{Y_Q-min(Y_Q)}{max(Y_Q)-min(Y_Q)+\epsilon}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="feature-enrichment-module">Feature Enrichment Module&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/rhadcOZ1iPuQ7nH.png"
loading="lazy"
alt="module1.png"
>&lt;/p>
&lt;p>将support image和query image关联起来的方法&lt;/p>
&lt;ul>
&lt;li>对support image做global average pooling
&lt;ul>
&lt;li>不用说都感觉效果一般&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>multi-level spatial information
&lt;ul>
&lt;li>说有两点不好，分别是merge的时候缺少specific refinement，和relation across different scales is ignored。这两点看看就好了，我感觉作者说有这两点问题主要是他自己在这两点做了一些trick。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>作者提出的FEM可以很好的解决问题。其中M的具体操作如下&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/16/P9ux8joOMAlyv3t.png"
loading="lazy"
alt="module2.png"
>&lt;/p>
&lt;h3 id="loss-function">Loss Function&lt;/h3>
&lt;p>$$
L = \frac{\sigma}{n}\sum_{i=1}^{n}{L_1^i+L_2}
$$&lt;/p>
&lt;p>主要选用交叉熵作为损失函数。&lt;/p>
&lt;ul>
&lt;li>$L_1^i$ FEM出来的n层spatial size中的第i层的X，通过intermediate supervision生成&lt;/li>
&lt;li>具体来说，这个X应该是FEM模块中，每一层的feature在information concentration之前，interpolate后做交叉熵的值&lt;/li>
&lt;li>$L_2$ 最后prediction和label的交叉熵。&lt;/li>
&lt;/ul>
&lt;p>&lt;!-- raw HTML omitted -->问题：实际上两个loss离得很近（计算L1的feature其实经过concat然后稍微卷积一下就到计算L2的feature了），那这样效果不是跟直接把学习率调大1一倍差不多吗）&lt;!-- raw HTML omitted -->&lt;/p></description></item></channel></rss>