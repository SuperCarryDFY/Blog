<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>others on Fengyuan Dai</title><link>https://SuperCarryDFY.github.io/Blog/tags/others/</link><description>Recent content in others on Fengyuan Dai</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 02 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/Blog/tags/others/index.xml" rel="self" type="application/rss+xml"/><item><title>Somethings</title><link>https://SuperCarryDFY.github.io/Blog/p/somethings/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/somethings/</guid><description>&lt;h2 id="关于为什么nlp中diffusion不火">关于为什么NLP中diffusion不火&lt;/h2>
&lt;p>谈及AIGC，CV or multimodal上最火的就是diffusion，但是在NLP中还是GPT为首的transformer autoregressive方式比较火，原因是什么。&lt;/p>
&lt;p>&lt;em>2023.8.2&lt;/em>&lt;/p>
&lt;p>看了ICLR2023 “DIFFUSEQ: SEQUENCE TO SEQUENCE TEXT GENERATION WITH DIFFUSION MODELS” 引发的思考。&lt;/p>
&lt;blockquote>
&lt;p>任务不同。multimodal的任务无非是生成图片，不论是unconditional还是conditional。而对于NLP，unconditonal生成seq并没有什么实际价值（胡言乱语有什么用呢）；而对于conditional，NLP的任务定义也与multimodal稍有不同：前者的condition是句子，而后者的condition通常为属性（比如说笑，哭，动漫化）。对后者来说，属性的种类有限，可以为每一个属性训练一个单独的classifier。而对前者，很难说每个句子都设计一个classifier。上面这篇文章解决的就是这个问题，把condition当作输入送入classifier-free diffusion，但是也只能做到和GPT-2 comparable。&lt;/p>
&lt;p>ICML2022“ On the Learning of Non-Autoregressive Transformers” 提到NAT方法在用max log likelihood时suffer from conditional total correlation。这篇文章我简单看了下，conditional total correlation大概就是说其实NAT的方法在生成当前token时没有考虑前一时刻的token（它同时生成整个序列的token的嘛），所以说并不会考虑token之间的相关性，而只会考虑此token在整个句子中的分布概率。所以说对于NLP的任务来说，AT天生好于NAT。&lt;/p>
&lt;p>其实上述两个问题在ICLR这篇文章中都已经解决了（对后者他在diffusion中用的模型是transformer，即用AT的方式生成latent embedding）。可能也没有涉及为啥diffusion在NLP中不火的本质，但是这篇文章的效果确实跟transformer-based的方法比是有差距的。&lt;/p>
&lt;/blockquote>
&lt;p>这里有个问题，ICML这篇文章其实并没有指定NLP，他就是说用最大log相似作为目标的时候，NAT会有问题。但是DDPM这篇文章用的也是最大log相似（应该说生成任务用的都是最大log相似）并且不是AT模型，DDPM是否也存在这个问题？&lt;/p></description></item><item><title>Sorts of Normalization</title><link>https://SuperCarryDFY.github.io/Blog/p/sorts-of-normalization/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/sorts-of-normalization/</guid><description>&lt;h2 id="batchnorm">BatchNorm&lt;/h2>
&lt;p>BatchNorm need to&lt;/p>
&lt;h2 id="layernorm">LayerNorm&lt;/h2>
&lt;p>LayerNorm is mostly used in NLP. Because the length of the sentences is not always same, so batchnorm is not suitable. LayerNorm normalizes the input along the word-dimention.&lt;/p>
&lt;blockquote>
&lt;p>Noted that the input is [bs, length,embeddings]. LayerNorm normalizes the embeddings for each words.&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">normalized_shape&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Union&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Size&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">eps&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1e-05&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">elementwise_affine&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>to be continue&amp;hellip;&lt;/p></description></item><item><title>About Equivariant and Invariant</title><link>https://SuperCarryDFY.github.io/Blog/p/about-equivariant-and-invariant/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/about-equivariant-and-invariant/</guid><description>&lt;p>Most of the contents can be found from &lt;a class="link" href="https://arxiv.org/pdf/2102.09844.pdf" target="_blank" rel="noopener"
>E(n) Equivariant Graph Neural Networks&lt;/a>&lt;/p>
&lt;h2 id="define-equivariance">Define Equivariance&lt;/h2>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_1.png"
width="404"
height="121"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_1_hu15c9de64f8332cd2382f678f735e835e_27443_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_1_hu15c9de64f8332cd2382f678f735e835e_27443_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Defination"
class="gallery-image"
data-flex-grow="333"
data-flex-basis="801px"
>&lt;/p>
&lt;p>Then the Translation, Rotation, Permutation equivariance can be defined as followes:&lt;/p>
&lt;p>&lt;img src="https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_2.png"
width="393"
height="228"
srcset="https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_2_hua9b3f815df9309341ec9a2770774bfd7_60738_480x0_resize_box_3.png 480w, https://SuperCarryDFY.github.io/Blog/Blog/p/about-equivariant-and-invariant/Fig_2_hua9b3f815df9309341ec9a2770774bfd7_60738_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Translation, Rotation, Permutation equivariance"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;h2 id="define-invariance">Define Invariance&lt;/h2>
&lt;p>Similar to equivariance, invariance meas after a function, the output remains invariance when the input performs translation, rotation or permutation.&lt;/p>
&lt;p>PS. the input and output can be features.&lt;/p></description></item><item><title>On the Integration of Self-Attention and Convolution</title><link>https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/</link><pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/on-the-integration-of-self-attention-and-convolution/</guid><description>&lt;img src="https://s2.loli.net/2022/11/05/9Lt7nI36HupazUE.png" alt="Featured image of post On the Integration of Self-Attention and Convolution" />&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/q6tB2b4KcskzQZn.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;strong>CNN&lt;/strong> The intrinsic characteristics impose crucial inductive biases for image processing. (平移不变性等)&lt;/p>
&lt;p>&lt;strong>Self-Attention&lt;/strong> The flexibility enables the attention module to focus on different regions adaptively and capture more informative features. ( bigger receptive field )&lt;/p>
&lt;blockquote>
&lt;p>Generally, we owed self-attention based model&amp;rsquo;s great performance to the global receptive field it have.&lt;/p>
&lt;/blockquote>
&lt;p>Considering the different and complementary properties of convolution and self-attention, there exists a potential possibility to benefit from both paradigms by integrating these modules.&lt;/p>
&lt;p>&lt;strong>Contribution:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>A strong underlying relation between self-attention and convolution is revealed&lt;/li>
&lt;li>An elegant integration of the self-attention and convolution module, which enjoys the benefits of both worlds&lt;/li>
&lt;/ul>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/4cCOJaNnGZPq56Q.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="conv">Conv&lt;/h3>
&lt;p>( This can be better illustrated by image a )&lt;/p>
&lt;p>They decompose the Conv with K*K kernel to two stages. On the first stage, K*K kernel can be seen as many 1*1 kernel. On the second stage, we can gain finally feature map by shift each feature map and sum them.&lt;/p>
&lt;h3 id="self-attn">Self-attn&lt;/h3>
&lt;p>( This can be better illustrated by image b )&lt;/p>
&lt;p>Like Conv, they treat self-attn with two steps. Firstly, 1*1 kernel conv ( euqal to Fully Connected Network) can be used to generate query, key and value. Then, they use query and key to gain weight and put it on the value.&lt;/p>
&lt;h3 id="acmix">ACmix&lt;/h3>
&lt;p>The first stage of Conv and Self-attn can be shared. In the second stage, there are two paths which corresponds to each paradigm. Then, two learnable parameters $\alpha$ and $\beta$ is set for reweighting each feature map for sum up.&lt;/p>
&lt;h4 id="shift">Shift&lt;/h4>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/hKvFbsuZg6Qkt7w.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>In the Conv path, they add a small trick compared to normal Conv.&lt;/p>
&lt;blockquote>
&lt;p>Shifting tensors towards various directions practically breaks the data locality and is difficult to achieve vectorized implementation. This may greatly impair the actual efficiency of our module at the inference time. ( This part I don&amp;rsquo;t understand perfectly)&lt;/p>
&lt;/blockquote>
&lt;p>As a remedy, they turn to apply depthwise convolution with fixed kernels. Here comes to a question. Now that you use conv kernel to shift feature map, why not make it learnable? So they use learnable kernels and initialized as shift kernels.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>The idea is quite simple. So They need strong experiments result to support them. They use their module in many models and many task( classification, detection and segmentation ). All seems good.&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/TXMgFrQBpkd6Wcs.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/qn4bxQupHwck21A.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;h3 id="alpha-and-beta">alpha and beta&lt;/h3>
&lt;p>&lt;img src="https://s2.loli.net/2022/11/05/VJ5gh2LsanSkfIH.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>At last, they check $\alpha$ and $\beta$ in each layer. This leads to a interesting conclusion. We can see that at early stage $\beta$ take dominate position. This means Conv is a good feature extractor. But at the last stage, self-attention shows superiority over convolution.&lt;/p>
&lt;blockquote>
&lt;p>This is also consistent with the design patterns in the previous works where self-attention is mostly adopted in the last stages to replace the original 3×3 convolution, and convolutions at early stages are proved to be more effective for vision transformers&lt;/p>
&lt;/blockquote></description></item><item><title>Dilated Convolution &amp; Deconvolution</title><link>https://SuperCarryDFY.github.io/Blog/p/dilated-convolution-deconvolution/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/dilated-convolution-deconvolution/</guid><description>&lt;h2 id="dilated-convolution">Dilated Convolution&lt;/h2>
&lt;p>Common Conv&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/50/v2-d552433faa8363df84c53b905443a556_720w.webp?source=1940ef5c"
loading="lazy"
alt="动图"
>&lt;/p>
&lt;p>Dilated Conv&lt;/p>
&lt;p>&lt;img src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif?raw=true"
loading="lazy"
alt="image-20221023221652075"
>&lt;/p>
&lt;p>空洞卷积在一定程度上能增大卷积神经网络的感受野，但是利用其设计语义分割网络则会存在如下两个问题。&lt;/p>
&lt;p>&lt;strong>1. The Gridding Effect&lt;/strong>&lt;/p>
&lt;p>如果和之前的操作一样，仅仅只是反复叠加3*3的kernal的话，那么在过程中就会存在一定的信息损失。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/v2-478a6b82e1508a147712af63d6472d9a_r.jpg?source=1940ef5c"
loading="lazy"
alt="img"
>&lt;/p>
&lt;p>&lt;strong>2. 对小物体的分割&lt;/strong>&lt;/p>
&lt;p>增大感受野对小物体的分割似乎没有好处&lt;/p>
&lt;p>因此图森组提出了较好的解决方法：Hybrid Dilated Convolution (HDC)&lt;/p>
&lt;ol>
&lt;li>叠加卷积的 dilation rate 不能有大于1的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。我们将 dilation rate 设计成锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构。&lt;/li>
&lt;li>需要满足$$M_i = max[M_{i+1}-2r_i, M_{i+1}-2(M_{i+1}-r_i), r_i]$$， 其中$r_i$，$M_i$分别代表第i层的dilation rate和最大dilation rate。&lt;/li>
&lt;/ol>
&lt;p>来自&lt;a class="link" href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener"
>如何理解空洞卷积（dilated convolution）？&lt;/a>， 作者@&lt;a class="link" href="https://www.zhihu.com/people/lorenmt" target="_blank" rel="noopener"
>刘诗昆&lt;/a>&lt;/p>
&lt;h2 id="deconvolution">Deconvolution&lt;/h2>
&lt;p>deconv大致可以分成如下三个方面&lt;/p>
&lt;ul>
&lt;li>unsupervised learning&lt;/li>
&lt;li>CNN可视化&lt;/li>
&lt;li>upsampling&lt;/li>
&lt;/ul>
&lt;p>呃。。一般来说上采样+卷积的性能比反卷积要好，况且反卷积存在棋盘格效应。&lt;/p>
&lt;p>来自&lt;a class="link" href="https://www.zhihu.com/question/43609045/answer/132235276" target="_blank" rel="noopener"
>如何理解深度学习中的deconvolution networks？&lt;/a>，作者@&lt;a class="link" href="https://www.zhihu.com/people/xutan" target="_blank" rel="noopener"
>谭旭&lt;/a>&lt;/p></description></item><item><title>Combination of Papers</title><link>https://SuperCarryDFY.github.io/Blog/p/combination-of-papers/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/Blog/p/combination-of-papers/</guid><description>&lt;p>Some papers are famous and have gotten several citations. They always appear when I&amp;rsquo;m reading current papers. So I decided to write down the most critical parts (concerned mainly with myself, in another way) from these papers on this page.&lt;/p>
&lt;h1 id="learning-deep-features-for-discriminative-localization">Learning Deep Features for Discriminative Localization&lt;/h1>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/17/xeEUAgGitC461Q7.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>This is a well-known work from CVPR2016, which has got 6.5k citation numbers up-to-date.&lt;/p>
&lt;p>There are two parts that seem to be important to me: &lt;strong>the comparison between global max pooling and global average pooling&lt;/strong>, as well as &lt;strong>the framework&lt;/strong>.&lt;/p>
&lt;h2 id="gmp-vs-gap">GMP vs. GAP&lt;/h2>
&lt;blockquote>
&lt;p>We believe that GAP loss encourages the network to identify the extent of the object as compared to GMP which encourages it to identify just one discriminative part.&lt;/p>
&lt;p>While GMP achieves similar classification performance as GAP, GAP outperforms GMP for localization.&lt;/p>
&lt;/blockquote>
&lt;p>GAP can focus on a wide range of pixels while GMP only depends on the most significant feature.&lt;/p>
&lt;h2 id="framework">FRAMEWORK&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/17/g3qjOyweVAtlNR1.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>The output of the last convolutional layer is denoted as $f_k(x,y)$, while k means the channel. After GAP, which should be expressed as $F^k = \sum_{x,y}f_k(x,y)$, for a given class, the input to the softmax $S_c$, is $\Sigma_kw_k^cF_k$ where $w_k^c$ is weight corresponding to class c for unit k.
$$
S_c = \sum_kw_k^c\sum_{x,y} f_k(x,y) = \sum_{x,y}\sum_k w_k^cf_k(x,y)
$$
They did upsample in the middle of the framework to fit the size.&lt;/p>
&lt;p>ABOUT &lt;strong>weakly supervised&lt;/strong>: They meant weakly-supervised because the labels are image-level but localization is object-level&lt;/p>
&lt;h1 id="learning-to-compare-relation-network-for-few-shot-learning">Learning to Compare: Relation Network for Few-Shot Learning&lt;/h1>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/20/mdSbWiDs8HYKRoJ.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这篇文章的思路相当简单，而且比原型网络那篇更好理解，是最早的metric-base few-shot learning的一批，并且现在依然流行（其实现在的方法就是在这个基础上修改，大体的框架是一样的）。&lt;/p>
&lt;h2 id="framework-1">FRAMEWORK&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/08/20/2fjUiYmbQ58cSTp.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>framework基本上一眼就能看明白，$f_\phi$和$g_\phi$都是卷积，池化，batch norm操作，没什么特别的。concatenation似乎就是从这里来的，现在基本也都是把query feature和support feature concatenate起来。其过程可用如下公式表示
$$
r_{i,j} = g_\phi(C(f_\phi(x_i),f_\phi(x_j)))
$$
其中$f_{\phi}$，$g_{\phi}$表示embedding module和relation module，函数$C(·，·)$表示concatenation。&lt;/p>
&lt;h2 id="zero-shot-learning">ZERO-SHOT LEARNING&lt;/h2>
&lt;p>在zero-shot learning中，不再给support image，而是给semantic class embedding vector $v_c$，其过程可以用如下公式表示
$$
r_{i,j} = g_\phi(C(f_{\phi_1}(v_c),f_{\phi_2}(x_j)))
$$
&lt;img src="https://s2.loli.net/2022/08/20/zASjlHLM4GRkfJ1.png"
loading="lazy"
alt="image.png"
>&lt;/p>
&lt;p>这里的DNN是训练好的模型，如VGG、Inception等。&lt;/p>
&lt;p>和prototypical network的区别&lt;/p>
&lt;blockquote>
&lt;p>Relation Network比原型网络的方法多了可学习的层，用来判别相似度度量，而在原型网络中是直接拿特定的相似度度量公式（如cosine similarity）&lt;/p>
&lt;/blockquote></description></item></channel></rss>