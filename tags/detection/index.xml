<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Detection on Dai Fengyuan</title><link>https://SuperCarryDFY.github.io/tags/detection/</link><description>Recent content in Detection on Dai Fengyuan</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 13 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://SuperCarryDFY.github.io/tags/detection/index.xml" rel="self" type="application/rss+xml"/><item><title>Few-shot Object Detection via Feature Reweighting</title><link>https://SuperCarryDFY.github.io/p/few-shot-object-detection-via-feature-reweighting/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://SuperCarryDFY.github.io/p/few-shot-object-detection-via-feature-reweighting/</guid><description>&lt;img src="https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png" alt="Featured image of post Few-shot Object Detection via Feature Reweighting" />&lt;p>&lt;img src="https://s2.loli.net/2022/09/13/oZqPrCX2h7ixfje.png"
loading="lazy"
alt="image-20220913110345915"
>&lt;/p>
&lt;p>This paper is from ICCV 2019. It addressed detection tasks R-CNN based network. However, it simply uses a shared classifier( mlp, I think) and bbox regressor, which is put forward in R-CNN though, to get predictions. Also, it tries a new form loss function out and gets a relatively large promotion.&lt;/p>
&lt;h2 id="abstruct--introduction">ABSTRUCT &amp;amp; INTRODUCTION&lt;/h2>
&lt;p>The model is mainly consist of two modules, i.e.,&lt;/p>
&lt;ul>
&lt;li>a meta feature learner.&lt;/li>
&lt;li>a light-weight feature reweighting module.&lt;/li>
&lt;/ul>
&lt;p>The training process is corresponding to two-phase learning scheme,&lt;/p>
&lt;ul>
&lt;li>first learn meta features and good reweighting module from base classes.&lt;/li>
&lt;li>fine-tune the detection model to adapt to novel classes.&lt;/li>
&lt;/ul>
&lt;p>Though it contains two-phase training, it&amp;rsquo;s an end-to-end method.&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://s2.loli.net/2022/09/13/hZK19UkL7T3twCr.png"
loading="lazy"
alt="image-20220913110439796"
>&lt;/p>
&lt;p>&lt;strong>Reweighting Module&lt;/strong>&lt;/p>
&lt;p>This module taking the support examples as input learns to embed support information into reweighting vectors and adjusts the contribution of each meta feature of the query image accordingly for the following detection prediction module.
It is like what is used in BAM. However, in BAM it just uses global max pooling ( or global average pooling, whatever), which is not learnable, to get the prototype vectors. This paper uses a learnable layer to get prototype vectors.
After that, they apply prototype vectors to obtain the class-specific feature Fi for novel class i by F_i = F \times w_i, where \times means channel-wise multiplication.&lt;/p>
&lt;p>&lt;strong>Shared Classifier &amp;amp; BBox Regressor&lt;/strong>&lt;/p>
&lt;p>There aren&amp;rsquo;t any details about Shared Classifier &amp;amp; BBox Regressor module. I simply think this module just being MLP combines BBox Regressor.&lt;/p>
&lt;p>&lt;strong>Learning Scheme&lt;/strong>&lt;/p>
&lt;p>In the first stage, they just feed the model with abundant base images with annotations. In this way, the model can learn to coordinate the two modules in the desired way.&lt;/p>
&lt;p>In the second stage, they fine-tune the model on both base and novel classes. The training procedure is the same as the first phase, except that it takes significantly fewer iterations for the model to converge.&lt;/p>
&lt;p>After two training phases, the model can do a test without a novel class as input (it should not be deemed as a novel class, for it has seen the classes in the second stage) and reweighting module, because it remembers the prototype vectors of all class and just do inference in novel images.&lt;/p>
&lt;h2 id="loss-function">LOSS FUNCTION&lt;/h2>
&lt;p>It is intuitive to use binary cross-entropy as a detection loss function, regressing 1 if the object is the target class and 0 otherwise. However, binary cross-entropy strives to produce balanced positive and negative predictions and could not remove such false predictions.
Instead, they adopt a softmax layer to calibrate the classification scores among different classes. It can be denoted as \hat c_i = \frac{e^{c_i}}{\sum_{j=1}^Ne^{c_j}}. Then, the loss function is below：
$$
L_c= -\sum_{i=1}^{N}1(·,i)log(\hat c_i)
$$&lt;/p>
&lt;p>where 1(·, i) is an indicator function for whether the current anchor box really belongs to class i or not.
Finally, the overall loss function is
$$
L_{det} = L_c + L_bbx + L_{obj}
$$&lt;/p></description></item></channel></rss>